{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê°ì •ë¶„ì„ ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (LSTM)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‹¤ìŒ 3ê°€ì§€ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ì£¼ê°€ ë°©í–¥ì„±ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **ğŸ“° ë‰´ìŠ¤ ê°ì •ë¶„ì„ (FinBERT)**: ê¸ˆìœµ ë‰´ìŠ¤ì˜ ê°ì • ì ìˆ˜\n",
    "2. **ğŸ¦ íŠ¸ìœ„í„° ê°ì •ë¶„ì„ (VADER)**: ì†Œì…œë¯¸ë””ì–´ì˜ ì‹¤ì‹œê°„ ê°ì •\n",
    "3. **ğŸ“ˆ ì£¼ê°€ ë°ì´í„°**: AAPL 1ì‹œê°„ ê°„ê²© ì£¼ê°€ ì •ë³´\n",
    "\n",
    "## ëª©í‘œ\n",
    "- **ì…ë ¥**: ê³¼ê±° 24ì‹œê°„ì˜ ê°ì • + ì£¼ê°€ ë°ì´í„°\n",
    "- **ì¶œë ¥**: ë‹¤ìŒ ì‹œê°„ì˜ ì£¼ê°€ ë°©í–¥ì„± (ìƒìŠ¹/ìœ ì§€/í•˜ë½)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ë° ê¸°ë³¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ!\n",
      "ğŸ“Š ì‚¬ìš© í™˜ê²½: pandas 2.2.3, numpy 1.24.4\n",
      "ğŸ¤– TensorFlow 2.13.0 ì‚¬ìš© ê°€ëŠ¥\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ê¸°ë³¸ ML ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"âœ… ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ì‚¬ìš© í™˜ê²½: pandas {pd.__version__}, numpy {np.__version__}\")\n",
    "\n",
    "# TensorFlowëŠ” ì„¤ì¹˜ ë¬¸ì œë¡œ ì¸í•´ ë‚˜ì¤‘ì— ì²˜ë¦¬\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"ğŸ¤– TensorFlow {tf.__version__} ì‚¬ìš© ê°€ëŠ¥\")\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ TensorFlow ë¯¸ì„¤ì¹˜ - ëŒ€ì•ˆ ëª¨ë¸ ì‚¬ìš© ì˜ˆì •\")\n",
    "    TF_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ë°ì´í„° íŒŒì¼ í™•ì¸:\n",
      "==================================================\n",
      "âœ… ğŸ“ˆ ì£¼ê°€ ë°ì´í„°: ../stock/AAPL_1hour_data_365days.csv (1.9MB)\n",
      "âœ… ğŸ“° ë‰´ìŠ¤ ê°ì •ë¶„ì„: ../finnhub/AAPL_finnhub_processed_final.csv (3.0MB)\n",
      "âœ… ğŸ¦ íŠ¸ìœ„í„° ê°ì •ë¶„ì„: ../X_data/merged_tweets_with_sentiment.csv (1.9MB)\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ë°ì´í„° íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "stock_file = \"../stock/AAPL_1hour_data_365days.csv\"  # ì£¼ê°€ ë°ì´í„°\n",
    "news_file = \"../finnhub/AAPL_finnhub_processed_final.csv\"  # ë‰´ìŠ¤ ê°ì •ë¶„ì„\n",
    "twitter_file = \"../X_data/merged_tweets_with_sentiment.csv\"  # íŠ¸ìœ„í„° ê°ì •ë¶„ì„\n",
    "\n",
    "# íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "files_info = {\n",
    "    \"ğŸ“ˆ ì£¼ê°€ ë°ì´í„°\": stock_file,\n",
    "    \"ğŸ“° ë‰´ìŠ¤ ê°ì •ë¶„ì„\": news_file, \n",
    "    \"ğŸ¦ íŠ¸ìœ„í„° ê°ì •ë¶„ì„\": twitter_file\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ ë°ì´í„° íŒŒì¼ í™•ì¸:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_files_exist = True\n",
    "for name, path in files_info.items():\n",
    "    if os.path.exists(path):\n",
    "        size_mb = os.path.getsize(path) / (1024*1024)\n",
    "        print(f\"âœ… {name}: {path} ({size_mb:.1f}MB)\")\n",
    "    else:\n",
    "        print(f\"âŒ {name}: {path} (íŒŒì¼ ì—†ìŒ)\")\n",
    "        all_files_exist = False\n",
    "\n",
    "if all_files_exist:\n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ë°ì´í„° íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ ì¼ë¶€ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ë°ì´í„° ë¡œë”© ë° ê¸°ë³¸ íƒìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ ì£¼ê°€ ë°ì´í„° ë¡œë”©...\n",
      "âœ… ì£¼ê°€ ë°ì´í„°: 3,889ê°œ í–‰\n",
      "ğŸ“… ê¸°ê°„: 2024-06-17 08:00:00+00:00 ~ 2025-06-13 23:00:00+00:00\n",
      "ğŸ“‹ ì»¬ëŸ¼: ['Datetime', 'Close', 'High', 'Low', 'Open', 'Volume', 'Returns', 'Log_Returns', 'SMA_10', 'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'RSI', 'BB_Middle', 'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Position', 'Volatility_10', 'Volatility_20', 'Price_Change', 'Price_Change_Pct', 'HL_Spread', 'HL_Spread_Pct', 'Hour', 'DayOfWeek', 'Month', 'Quarter', 'Is_Trading_Hours', 'Is_Market_Open', 'Is_Premarket', 'Is_Aftermarket', 'Is_Extended_Hours']\n",
      "\n",
      "ì£¼ê°€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Returns</th>\n",
       "      <th>Log_Returns</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>...</th>\n",
       "      <th>HL_Spread_Pct</th>\n",
       "      <th>Hour</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Month</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Is_Trading_Hours</th>\n",
       "      <th>Is_Market_Open</th>\n",
       "      <th>Is_Premarket</th>\n",
       "      <th>Is_Aftermarket</th>\n",
       "      <th>Is_Extended_Hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-06-17 08:00:00+00:00</td>\n",
       "      <td>214.350</td>\n",
       "      <td>214.36</td>\n",
       "      <td>212.82</td>\n",
       "      <td>213.24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718451</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-17 09:00:00+00:00</td>\n",
       "      <td>213.930</td>\n",
       "      <td>214.62</td>\n",
       "      <td>213.66</td>\n",
       "      <td>214.24</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001959</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448745</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-06-17 10:00:00+00:00</td>\n",
       "      <td>213.620</td>\n",
       "      <td>214.10</td>\n",
       "      <td>213.61</td>\n",
       "      <td>213.96</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001449</td>\n",
       "      <td>-0.001450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229379</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-06-17 11:00:00+00:00</td>\n",
       "      <td>213.960</td>\n",
       "      <td>214.18</td>\n",
       "      <td>213.62</td>\n",
       "      <td>213.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261731</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-06-17 12:00:00+00:00</td>\n",
       "      <td>214.145</td>\n",
       "      <td>214.60</td>\n",
       "      <td>212.62</td>\n",
       "      <td>213.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924607</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime    Close    High     Low    Open  Volume  \\\n",
       "0 2024-06-17 08:00:00+00:00  214.350  214.36  212.82  213.24       0   \n",
       "1 2024-06-17 09:00:00+00:00  213.930  214.62  213.66  214.24       0   \n",
       "2 2024-06-17 10:00:00+00:00  213.620  214.10  213.61  213.96       0   \n",
       "3 2024-06-17 11:00:00+00:00  213.960  214.18  213.62  213.62       0   \n",
       "4 2024-06-17 12:00:00+00:00  214.145  214.60  212.62  213.94       0   \n",
       "\n",
       "    Returns  Log_Returns  SMA_10  SMA_20  ...  HL_Spread_Pct  Hour  DayOfWeek  \\\n",
       "0       NaN          NaN     NaN     NaN  ...       0.718451     8          0   \n",
       "1 -0.001959    -0.001961     NaN     NaN  ...       0.448745     9          0   \n",
       "2 -0.001449    -0.001450     NaN     NaN  ...       0.229379    10          0   \n",
       "3  0.001592     0.001590     NaN     NaN  ...       0.261731    11          0   \n",
       "4  0.000865     0.000864     NaN     NaN  ...       0.924607    12          0   \n",
       "\n",
       "   Month  Quarter  Is_Trading_Hours  Is_Market_Open  Is_Premarket  \\\n",
       "0      6        2                 0               0             1   \n",
       "1      6        2                 1               1             0   \n",
       "2      6        2                 1               1             0   \n",
       "3      6        2                 1               1             0   \n",
       "4      6        2                 1               1             0   \n",
       "\n",
       "   Is_Aftermarket  Is_Extended_Hours  \n",
       "0               0                  1  \n",
       "1               0                  0  \n",
       "2               0                  0  \n",
       "3               0                  0  \n",
       "4               0                  0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. ì£¼ê°€ ë°ì´í„° ë¡œë“œ\n",
    "print(\"ğŸ“ˆ ì£¼ê°€ ë°ì´í„° ë¡œë”©...\")\n",
    "stock_df = pd.read_csv(stock_file)\n",
    "stock_df['Datetime'] = pd.to_datetime(stock_df['Datetime'])\n",
    "stock_df = stock_df.sort_values('Datetime').reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… ì£¼ê°€ ë°ì´í„°: {len(stock_df):,}ê°œ í–‰\")\n",
    "print(f\"ğŸ“… ê¸°ê°„: {stock_df['Datetime'].min()} ~ {stock_df['Datetime'].max()}\")\n",
    "print(f\"ğŸ“‹ ì»¬ëŸ¼: {list(stock_df.columns)}\")\n",
    "print(f\"\\nì£¼ê°€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "stock_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ê°ì •ë¶„ì„ ë°ì´í„° ë¡œë”©...\n",
      "âœ… ë‰´ìŠ¤ ë°ì´í„°: 8,767ê°œ í–‰\n",
      "ğŸ“… ê¸°ê°„: 2023-03-22 18:22:51 ~ 2025-06-14 09:55:00\n",
      "ğŸ“‹ ì»¬ëŸ¼: ['Date', 'full_text', 'sentiment', 'neg', 'neu', 'pos']\n",
      "\n",
      "ğŸ“Š ë‰´ìŠ¤ ê°ì •ë¶„ì„ ê²°ê³¼ ë¶„í¬:\n",
      "sentiment\n",
      "neutral     4731\n",
      "positive    2384\n",
      "negative    1652\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ë‰´ìŠ¤ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-22 18:22:51</td>\n",
       "      <td>Stock Market Today: Stock Market News And Anal...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>8.965701e-06</td>\n",
       "      <td>0.999617</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-19 00:47:00</td>\n",
       "      <td>Chipmaker Nvidia surpasses Microsoft as most v...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>6.796851e-07</td>\n",
       "      <td>0.995314</td>\n",
       "      <td>0.004685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-06-19 01:17:26</td>\n",
       "      <td>Extended offers dampen excitement for China's ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>9.999908e-01</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-06-19 01:31:00</td>\n",
       "      <td>Qualcomm Ends $75M Lawsuit Linked To Apple's P...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.074722e-07</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-06-19 01:45:00</td>\n",
       "      <td>LONDON MARKET EARLY CALL: UK inflation data ah...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>6.354484e-07</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date                                          full_text  \\\n",
       "0 2023-03-22 18:22:51  Stock Market Today: Stock Market News And Anal...   \n",
       "1 2024-06-19 00:47:00  Chipmaker Nvidia surpasses Microsoft as most v...   \n",
       "2 2024-06-19 01:17:26  Extended offers dampen excitement for China's ...   \n",
       "3 2024-06-19 01:31:00  Qualcomm Ends $75M Lawsuit Linked To Apple's P...   \n",
       "4 2024-06-19 01:45:00  LONDON MARKET EARLY CALL: UK inflation data ah...   \n",
       "\n",
       "  sentiment           neg       neu       pos  \n",
       "0   neutral  8.965701e-06  0.999617  0.000374  \n",
       "1   neutral  6.796851e-07  0.995314  0.004685  \n",
       "2  negative  9.999908e-01  0.000005  0.000004  \n",
       "3   neutral  1.074722e-07  0.999995  0.000005  \n",
       "4   neutral  6.354484e-07  0.999995  0.000005  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. ë‰´ìŠ¤ ê°ì •ë¶„ì„ ë°ì´í„° ë¡œë“œ\n",
    "print(\"ğŸ“° ë‰´ìŠ¤ ê°ì •ë¶„ì„ ë°ì´í„° ë¡œë”©...\")\n",
    "news_df = pd.read_csv(news_file)\n",
    "news_df['Date'] = pd.to_datetime(news_df['Date'])\n",
    "news_df = news_df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… ë‰´ìŠ¤ ë°ì´í„°: {len(news_df):,}ê°œ í–‰\")\n",
    "print(f\"ğŸ“… ê¸°ê°„: {news_df['Date'].min()} ~ {news_df['Date'].max()}\")\n",
    "print(f\"ğŸ“‹ ì»¬ëŸ¼: {list(news_df.columns)}\")\n",
    "\n",
    "# ê°ì •ë¶„ì„ ê²°ê³¼ ë¶„í¬\n",
    "print(f\"\\nğŸ“Š ë‰´ìŠ¤ ê°ì •ë¶„ì„ ê²°ê³¼ ë¶„í¬:\")\n",
    "print(news_df['sentiment'].value_counts())\n",
    "print(f\"\\në‰´ìŠ¤ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "news_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦ íŠ¸ìœ„í„° ê°ì •ë¶„ì„ ë°ì´í„° ë¡œë”©...\n",
      "âœ… íŠ¸ìœ„í„° ë°ì´í„°: 8,594ê°œ í–‰\n",
      "ğŸ“… ê¸°ê°„: 2020-01-17 01:05:39 ~ 2025-06-16 04:36:37\n",
      "ğŸ“‹ ì»¬ëŸ¼: ['created_at', 'full_text', 'username', 'sentiment', 'neg', 'neu', 'pos']\n",
      "\n",
      "ğŸ“Š íŠ¸ìœ„í„° ê°ì •ë¶„ì„ ê²°ê³¼ ë¶„í¬:\n",
      "sentiment\n",
      "positive    4609\n",
      "negative    2184\n",
      "neutral     1801\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ‘¥ ì‚¬ìš©ìë³„ íŠ¸ìœ— ìˆ˜:\n",
      "username\n",
      "@tim_cook          838\n",
      "@marcorubio        796\n",
      "@BillAckman        792\n",
      "@michaelbatnick    757\n",
      "@elonmusk          719\n",
      "@RayDalio          706\n",
      "@JDVance           675\n",
      "@LizAnnSonders     672\n",
      "@CathieDWood       656\n",
      "@WhiteHouse        620\n",
      "Name: count, dtype: int64\n",
      "\n",
      "íŠ¸ìœ„í„° ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>username</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-17 01:05:39</td>\n",
       "      <td>There are countless ways to make a difference....</td>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-18 16:25:52</td>\n",
       "      <td>Glad to support the essential work of our frie...</td>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-18 21:22:26</td>\n",
       "      <td>When you add up every ambitious dreamer, every...</td>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-19 19:58:20</td>\n",
       "      <td>What an amazing welcome back to Ireland! Thank...</td>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-20 09:39:31</td>\n",
       "      <td>\"I have the audacity to believe that peoples e...</td>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           created_at                                          full_text  \\\n",
       "0 2020-01-17 01:05:39  There are countless ways to make a difference....   \n",
       "1 2020-01-18 16:25:52  Glad to support the essential work of our frie...   \n",
       "2 2020-01-18 21:22:26  When you add up every ambitious dreamer, every...   \n",
       "3 2020-01-19 19:58:20  What an amazing welcome back to Ireland! Thank...   \n",
       "4 2020-01-20 09:39:31  \"I have the audacity to believe that peoples e...   \n",
       "\n",
       "    username sentiment    neg    neu    pos  \n",
       "0  @tim_cook  positive  0.031  0.785  0.183  \n",
       "1  @tim_cook  positive  0.000  0.632  0.368  \n",
       "2  @tim_cook  positive  0.043  0.625  0.332  \n",
       "3  @tim_cook  positive  0.000  0.615  0.385  \n",
       "4  @tim_cook  positive  0.000  0.791  0.209  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. íŠ¸ìœ„í„° ê°ì •ë¶„ì„ ë°ì´í„° ë¡œë“œ\n",
    "print(\"ğŸ¦ íŠ¸ìœ„í„° ê°ì •ë¶„ì„ ë°ì´í„° ë¡œë”©...\")\n",
    "twitter_df = pd.read_csv(twitter_file)\n",
    "twitter_df['created_at'] = pd.to_datetime(twitter_df['created_at'])\n",
    "twitter_df = twitter_df.sort_values('created_at').reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… íŠ¸ìœ„í„° ë°ì´í„°: {len(twitter_df):,}ê°œ í–‰\")\n",
    "print(f\"ğŸ“… ê¸°ê°„: {twitter_df['created_at'].min()} ~ {twitter_df['created_at'].max()}\")\n",
    "print(f\"ğŸ“‹ ì»¬ëŸ¼: {list(twitter_df.columns)}\")\n",
    "\n",
    "# ê°ì •ë¶„ì„ ê²°ê³¼ ë¶„í¬\n",
    "print(f\"\\nğŸ“Š íŠ¸ìœ„í„° ê°ì •ë¶„ì„ ê²°ê³¼ ë¶„í¬:\")\n",
    "print(twitter_df['sentiment'].value_counts())\n",
    "\n",
    "# ì‚¬ìš©ìë³„ íŠ¸ìœ— ìˆ˜\n",
    "print(f\"\\nğŸ‘¥ ì‚¬ìš©ìë³„ íŠ¸ìœ— ìˆ˜:\")\n",
    "print(twitter_df['username'].value_counts().head(10))\n",
    "print(f\"\\níŠ¸ìœ„í„° ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "twitter_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4ï¸âƒ£ ì‹œê°„ ê¸°ì¤€ ë°ì´í„° ë™ê¸°í™”\n",
    "\n",
    "3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì„ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤:\n",
    "- **ì£¼ê°€**: 1ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ì´ë¯¸ ì •ë¦¬ë¨\n",
    "- **ë‰´ìŠ¤**: ë¶ˆê·œì¹™í•œ ì‹œê°„ â†’ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì§‘ê³„\n",
    "- **íŠ¸ìœ„í„°**: ì‹¤ì‹œê°„ â†’ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì§‘ê³„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ì‹œê°„ ê¸°ì¤€ ë°ì´í„° ë™ê¸°í™” ì‹œì‘...\n",
      "ğŸ“° ë‰´ìŠ¤ ë°ì´í„° ì‹œê°„ë³„ ì§‘ê³„: 3,973ê°œ ì‹œê°„ëŒ€\n",
      "ğŸ¦ íŠ¸ìœ„í„° ë°ì´í„° ì‹œê°„ë³„ ì§‘ê³„: 4,915ê°œ ì‹œê°„ëŒ€\n",
      "ğŸ“ˆ ì£¼ê°€ ë°ì´í„° ì‹œê°„ë³„ ì •ë¦¬: 3,889ê°œ ì‹œê°„ëŒ€\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ ì‹œê°„ ê¸°ì¤€ ë°ì´í„° ë™ê¸°í™” ì‹œì‘...\")\n",
    "\n",
    "# 1. ì£¼ê°€ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì§‘ê³„\n",
    "stock_df['hour'] = stock_df['Datetime'].dt.floor('H')\n",
    "\n",
    "# 2. ë‰´ìŠ¤ ë°ì´í„°ë¥¼ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì§‘ê³„\n",
    "news_df['hour'] = news_df['Date'].dt.floor('H')\n",
    "news_hourly = news_df.groupby('hour').agg({\n",
    "    'pos': 'mean',\n",
    "    'neu': 'mean', \n",
    "    'neg': 'mean',\n",
    "    'sentiment': lambda x: x.mode().iloc[0] if not x.empty else 'neutral'\n",
    "}).reset_index()\n",
    "news_hourly.columns = ['hour', 'news_pos', 'news_neu', 'news_neg', 'news_sentiment']\n",
    "\n",
    "print(f\"ğŸ“° ë‰´ìŠ¤ ë°ì´í„° ì‹œê°„ë³„ ì§‘ê³„: {len(news_hourly):,}ê°œ ì‹œê°„ëŒ€\")\n",
    "\n",
    "# 3. íŠ¸ìœ„í„° ë°ì´í„°ë¥¼ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì§‘ê³„\n",
    "twitter_df['hour'] = twitter_df['created_at'].dt.floor('H')\n",
    "twitter_hourly = twitter_df.groupby('hour').agg({\n",
    "    'pos': 'mean',\n",
    "    'neu': 'mean',\n",
    "    'neg': 'mean', \n",
    "    'sentiment': lambda x: x.mode().iloc[0] if not x.empty else 'neutral'\n",
    "}).reset_index()\n",
    "twitter_hourly.columns = ['hour', 'twitter_pos', 'twitter_neu', 'twitter_neg', 'twitter_sentiment']\n",
    "\n",
    "print(f\"ğŸ¦ íŠ¸ìœ„í„° ë°ì´í„° ì‹œê°„ë³„ ì§‘ê³„: {len(twitter_hourly):,}ê°œ ì‹œê°„ëŒ€\")\n",
    "\n",
    "# 4. ì£¼ê°€ ë°ì´í„°ë„ ì‹œê°„ë³„ë¡œ ì •ë¦¬ (ì¤‘ë³µ ì œê±°)\n",
    "stock_hourly = stock_df.drop_duplicates(subset=['hour'], keep='last').reset_index(drop=True)\n",
    "print(f\"ğŸ“ˆ ì£¼ê°€ ë°ì´í„° ì‹œê°„ë³„ ì •ë¦¬: {len(stock_hourly):,}ê°œ ì‹œê°„ëŒ€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ë°ì´í„° ë³‘í•© ì¤‘...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on datetime64[ns, UTC] and datetime64[ns] columns for key 'hour'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 5. 3ê°œ ë°ì´í„° ë³‘í•©\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ”— ë°ì´í„° ë³‘í•© ì¤‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m combined \u001b[38;5;241m=\u001b[39m stock_hourly\u001b[38;5;241m.\u001b[39mmerge(news_hourly, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m combined \u001b[38;5;241m=\u001b[39m combined\u001b[38;5;241m.\u001b[39mmerge(twitter_hourly, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ë°ì´í„° ë³‘í•© ì™„ë£Œ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(combined)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mê°œ í–‰\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[0;32m  10833\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10834\u001b[0m         right,\n\u001b[0;32m  10835\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[0;32m  10836\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[0;32m  10837\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[0;32m  10838\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[0;32m  10839\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[0;32m  10840\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[0;32m  10841\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m  10842\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[0;32m  10843\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m  10844\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[0;32m  10845\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m  10846\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[0;32m    173\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[0;32m    174\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[0;32m    175\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[0;32m    176\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[0;32m    177\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[0;32m    178\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[0;32m    179\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    180\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[0;32m    181\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1518\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lk\u001b[38;5;241m.\u001b[39mdtype, DatetimeTZDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1516\u001b[0m     rk\u001b[38;5;241m.\u001b[39mdtype, DatetimeTZDtype\n\u001b[0;32m   1517\u001b[0m ):\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lk\u001b[38;5;241m.\u001b[39mdtype, DatetimeTZDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1520\u001b[0m     rk\u001b[38;5;241m.\u001b[39mdtype, DatetimeTZDtype\n\u001b[0;32m   1521\u001b[0m ):\n\u001b[0;32m   1522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on datetime64[ns, UTC] and datetime64[ns] columns for key 'hour'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# 5. 3ê°œ ë°ì´í„° ë³‘í•©\n",
    "print(\"ğŸ”— ë°ì´í„° ë³‘í•© ì¤‘...\")\n",
    "combined = stock_hourly.merge(news_hourly, on='hour', how='left')\n",
    "combined = combined.merge(twitter_hourly, on='hour', how='left')\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë³‘í•© ì™„ë£Œ: {len(combined):,}ê°œ í–‰\")\n",
    "print(f\"ğŸ“… í†µí•© ë°ì´í„° ê¸°ê°„: {combined['hour'].min()} ~ {combined['hour'].max()}\")\n",
    "\n",
    "# 6. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (forward fill)\n",
    "sentiment_cols = ['news_pos', 'news_neu', 'news_neg', 'twitter_pos', 'twitter_neu', 'twitter_neg']\n",
    "print(f\"\\nğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „:\")\n",
    "print(combined[sentiment_cols].isnull().sum())\n",
    "\n",
    "combined[sentiment_cols] = combined[sentiment_cols].fillna(method='ffill')\n",
    "combined[sentiment_cols] = combined[sentiment_cols].fillna(0.5)  # ì´ˆê¸°ê°’\n",
    "\n",
    "# ê°ì • ë¼ë²¨ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "combined['news_sentiment'] = combined['news_sentiment'].fillna('neutral')\n",
    "combined['twitter_sentiment'] = combined['twitter_sentiment'].fillna('neutral')\n",
    "\n",
    "print(f\"\\nğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„:\")\n",
    "print(combined[sentiment_cols].isnull().sum())\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… í†µí•© ë°ì´í„°: {len(combined):,}ê°œ í–‰, {len(combined.columns)}ê°œ ì»¬ëŸ¼\")\n",
    "combined.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5ï¸âƒ£ í†µí•© íŠ¹ì„± ìƒì„± ë° Feature Engineering\n",
    "\n",
    "ê°ì • ì ìˆ˜ë¥¼ í†µí•©í•˜ê³  ì¶”ê°€ì ì¸ íŠ¹ì„±ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤:\n",
    "- **ê°ì • ì ìˆ˜ í†µí•©**: FinBERT(60%) + VADER(40%) ê°€ì¤‘ í‰ê· \n",
    "- **ê°ì • ê°•ë„**: ìµœëŒ€ê°’ - ìµœì†Œê°’ìœ¼ë¡œ ê°ì •ì˜ í™•ì‹ ë„ ì¸¡ì •\n",
    "- **ì‹œê°„ì  íŠ¹ì„±**: ë³€í™”ìœ¨, ì´ë™í‰ê· , ê¸°ìˆ ì  ì§€í‘œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ í†µí•© íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
    "\n",
    "df = combined.copy()\n",
    "\n",
    "# 1. ê°ì • ì ìˆ˜ í†µí•© (ê°€ì¤‘ í‰ê· : FinBERT 60%, VADER 40%)\n",
    "df['combined_pos'] = 0.6 * df['news_pos'] + 0.4 * df['twitter_pos']\n",
    "df['combined_neu'] = 0.6 * df['news_neu'] + 0.4 * df['twitter_neu']\n",
    "df['combined_neg'] = 0.6 * df['news_neg'] + 0.4 * df['twitter_neg']\n",
    "\n",
    "print(\"âœ… ê°ì • ì ìˆ˜ í†µí•© ì™„ë£Œ\")\n",
    "\n",
    "# 2. ê°ì • ê°•ë„ ë° ê·¹ì„±\n",
    "df['news_sentiment_intensity'] = df[['news_pos', 'news_neu', 'news_neg']].max(axis=1) - df[['news_pos', 'news_neu', 'news_neg']].min(axis=1)\n",
    "df['twitter_sentiment_intensity'] = df[['twitter_pos', 'twitter_neu', 'twitter_neg']].max(axis=1) - df[['twitter_pos', 'twitter_neu', 'twitter_neg']].min(axis=1)\n",
    "df['combined_sentiment_intensity'] = df[['combined_pos', 'combined_neu', 'combined_neg']].max(axis=1) - df[['combined_pos', 'combined_neu', 'combined_neg']].min(axis=1)\n",
    "\n",
    "print(\"âœ… ê°ì • ê°•ë„ ê³„ì‚° ì™„ë£Œ\")\n",
    "\n",
    "# 3. ê°ì • ì ìˆ˜ ë³€í™”ìœ¨ (ì‹œê°„ë³„)\n",
    "for col in ['news_pos', 'news_neu', 'news_neg', 'twitter_pos', 'twitter_neu', 'twitter_neg']:\n",
    "    df[f'{col}_change'] = df[col].pct_change()\n",
    "\n",
    "print(\"âœ… ê°ì • ì ìˆ˜ ë³€í™”ìœ¨ ê³„ì‚° ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ê°ì • ì ìˆ˜ ì´ë™í‰ê·  (3ì‹œê°„, 6ì‹œê°„)\n",
    "for window in [3, 6]:\n",
    "    for col in ['combined_pos', 'combined_neu', 'combined_neg']:\n",
    "        df[f'{col}_ma{window}'] = df[col].rolling(window).mean()\n",
    "\n",
    "print(\"âœ… ê°ì • ì ìˆ˜ ì´ë™í‰ê·  ê³„ì‚° ì™„ë£Œ\")\n",
    "\n",
    "# 5. ì£¼ê°€ íŠ¹ì„±\n",
    "df['price_change'] = df['Close'].pct_change()  # ìˆ˜ìµë¥ \n",
    "df['price_volatility'] = df['price_change'].rolling(6).std()  # ë³€ë™ì„±\n",
    "df['volume_ma'] = df['Volume'].rolling(6).mean()  # ê±°ë˜ëŸ‰ ì´ë™í‰ê· \n",
    "\n",
    "print(\"âœ… ì£¼ê°€ íŠ¹ì„± ê³„ì‚° ì™„ë£Œ\")\n",
    "\n",
    "# 6. ê¸°ìˆ ì  ì§€í‘œ\n",
    "def calculate_rsi(prices, window=14):\n",
    "    \"\"\"RSI ê³„ì‚°\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "df['sma_5'] = df['Close'].rolling(5).mean()\n",
    "df['sma_10'] = df['Close'].rolling(10).mean()\n",
    "df['rsi'] = calculate_rsi(df['Close'])\n",
    "\n",
    "print(\"âœ… ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ì‹œê°„ íŠ¹ì„±\n",
    "df['hour_of_day'] = df['hour'].dt.hour\n",
    "df['day_of_week'] = df['hour'].dt.dayofweek\n",
    "df['is_trading_hours'] = ((df['hour_of_day'] >= 9) & (df['hour_of_day'] <= 16)).astype(int)\n",
    "\n",
    "print(\"âœ… ì‹œê°„ íŠ¹ì„± ê³„ì‚° ì™„ë£Œ\")\n",
    "\n",
    "# 8. íƒ€ê²Ÿ ë¼ë²¨ ìƒì„± (ë‹¤ìŒ ì‹œê°„ì˜ ì£¼ê°€ ë°©í–¥ì„±)\n",
    "df['next_price_change'] = df['price_change'].shift(-1)  # ë‹¤ìŒ ì‹œê°„ ìˆ˜ìµë¥ \n",
    "\n",
    "# ì„ê³„ê°’ ì„¤ì •: Â±0.5%\n",
    "def classify_direction(change):\n",
    "    if pd.isna(change):\n",
    "        return 'neutral'\n",
    "    elif change > 0.005:  # +0.5%\n",
    "        return 'positive'\n",
    "    elif change < -0.005:  # -0.5%\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['target'] = df['next_price_change'].apply(classify_direction)\n",
    "\n",
    "print(\"âœ… íƒ€ê²Ÿ ë¼ë²¨ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì œê±°\n",
    "df_clean = df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nğŸ‰ íŠ¹ì„± ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ìµœì¢… ë°ì´í„°: {len(df_clean):,}ê°œ í–‰, {len(df_clean.columns)}ê°œ ì»¬ëŸ¼\")\n",
    "print(f\"\\nğŸ“ˆ íƒ€ê²Ÿ ë¶„í¬:\")\n",
    "print(df_clean['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6ï¸âƒ£ ë°ì´í„° ì‹œê°í™” ë° íƒìƒ‰ì  ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì • ì ìˆ˜ì™€ ì£¼ê°€ì˜ ì‹œê°„ì  ë³€í™” ì‹œê°í™”\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# 1. ì£¼ê°€ ë³€í™”\n",
    "recent_data = df_clean.tail(168)  # ìµœê·¼ 1ì£¼ì¼ ë°ì´í„°\n",
    "axes[0].plot(recent_data['hour'], recent_data['Close'], color='blue', alpha=0.8)\n",
    "axes[0].set_title('ğŸ“ˆ AAPL ì£¼ê°€ ë³€í™” (ìµœê·¼ 1ì£¼ì¼)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('ì£¼ê°€ ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. ë‰´ìŠ¤ vs íŠ¸ìœ„í„° ê°ì • ë¹„êµ\n",
    "axes[1].plot(recent_data['hour'], recent_data['news_pos'], label='ë‰´ìŠ¤ Positive (FinBERT)', color='green', alpha=0.7)\n",
    "axes[1].plot(recent_data['hour'], recent_data['twitter_pos'], label='íŠ¸ìœ„í„° Positive (VADER)', color='lightgreen', alpha=0.7)\n",
    "axes[1].plot(recent_data['hour'], recent_data['combined_pos'], label='í†µí•© Positive', color='darkgreen', linewidth=2)\n",
    "axes[1].set_title('ğŸ˜Š Positive ê°ì • ì ìˆ˜ ë¹„êµ', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Positive ì ìˆ˜')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. ê°ì • ê°•ë„\n",
    "axes[2].bar(recent_data['hour'], recent_data['combined_sentiment_intensity'], \n",
    "           color=['red' if x == 'negative' else 'gray' if x == 'neutral' else 'green' \n",
    "                  for x in recent_data['target']], alpha=0.7)\n",
    "axes[2].set_title('ğŸ’ª í†µí•© ê°ì • ê°•ë„ (ìƒ‰ìƒ: ë‹¤ìŒ ì‹œê°„ ì£¼ê°€ ë°©í–¥)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('ì‹œê°„')\n",
    "axes[2].set_ylabel('ê°ì • ê°•ë„')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì • ì ìˆ˜ì™€ ì£¼ê°€ ìˆ˜ìµë¥ ì˜ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "correlation_cols = [\n",
    "    'news_pos', 'news_neu', 'news_neg',\n",
    "    'twitter_pos', 'twitter_neu', 'twitter_neg',\n",
    "    'combined_pos', 'combined_neu', 'combined_neg',\n",
    "    'news_sentiment_intensity', 'twitter_sentiment_intensity', 'combined_sentiment_intensity',\n",
    "    'price_change', 'next_price_change'\n",
    "]\n",
    "\n",
    "corr_matrix = df_clean[correlation_cols].corr()\n",
    "\n",
    "# ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('ğŸ”¥ ê°ì • ì ìˆ˜ì™€ ì£¼ê°€ ìˆ˜ìµë¥  ìƒê´€ê´€ê³„', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ì£¼ìš” ìƒê´€ê´€ê³„ ì¶œë ¥\n",
    "print(\"ğŸ“Š ì£¼ê°€ ìˆ˜ìµë¥ ê³¼ì˜ ì£¼ìš” ìƒê´€ê´€ê³„:\")\n",
    "price_corr = corr_matrix['next_price_change'].abs().sort_values(ascending=False)\n",
    "for feature, corr_val in price_corr.head(10).items():\n",
    "    if feature != 'next_price_change':\n",
    "        print(f\"  {feature:30s}: {corr_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7ï¸âƒ£ LSTMìš© ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "LSTM ëª¨ë¸ì„ ìœ„í•´ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤:\n",
    "- **ì‹œí€€ìŠ¤ ê¸¸ì´**: 24ì‹œê°„ (24ê°œ ì‹œì )\n",
    "- **ì…ë ¥**: ê³¼ê±° 24ì‹œê°„ì˜ ê°ì • + ì£¼ê°€ íŠ¹ì„±\n",
    "- **ì¶œë ¥**: ë‹¤ìŒ ì‹œê°„ì˜ ì£¼ê°€ ë°©í–¥ì„± (positive/neutral/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„\n",
    "sequence_length = 24  # 24ì‹œê°„ ì‹œí€€ìŠ¤\n",
    "\n",
    "print(f\"ğŸ”„ LSTM ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„ ì¤‘... (ì‹œí€€ìŠ¤ ê¸¸ì´: {sequence_length})\")\n",
    "\n",
    "# íŠ¹ì„± ì„ íƒ (ê°ì • + ì£¼ê°€ + ê¸°ìˆ ì  ì§€í‘œ)\n",
    "feature_cols = [\n",
    "    # ê°ì • ì ìˆ˜ (ì›ë³¸)\n",
    "    'news_pos', 'news_neu', 'news_neg',\n",
    "    'twitter_pos', 'twitter_neu', 'twitter_neg',\n",
    "    # í†µí•© ê°ì • ì ìˆ˜\n",
    "    'combined_pos', 'combined_neu', 'combined_neg',\n",
    "    # ê°ì • ê°•ë„\n",
    "    'news_sentiment_intensity', 'twitter_sentiment_intensity', 'combined_sentiment_intensity',\n",
    "    # ì£¼ê°€ íŠ¹ì„±\n",
    "    'Close', 'Volume', 'price_change', 'price_volatility',\n",
    "    # ê¸°ìˆ ì  ì§€í‘œ\n",
    "    'rsi', 'sma_5', 'sma_10',\n",
    "    # ì‹œê°„ íŠ¹ì„±\n",
    "    'hour_of_day', 'day_of_week', 'is_trading_hours'\n",
    "]\n",
    "\n",
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "available_cols = [col for col in feature_cols if col in df_clean.columns]\n",
    "print(f\"ğŸ“‹ ì‚¬ìš©í•  íŠ¹ì„±: {len(available_cols)}ê°œ\")\n",
    "print(f\"  {available_cols}\")\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "X = df_clean[available_cols].values\n",
    "y = df_clean['target'].values\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° í˜•íƒœ:\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë ˆì´ë¸” ì¸ì½”ë”©\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"ğŸ“Š ë ˆì´ë¸” ë§¤í•‘: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# ë°ì´í„° ì •ê·œí™”\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ì •ê·œí™” ì™„ë£Œ\")\n",
    "\n",
    "# ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±\n",
    "X_sequences, y_sequences = [], []\n",
    "\n",
    "for i in range(len(X_scaled) - sequence_length):\n",
    "    X_sequences.append(X_scaled[i:(i + sequence_length)])\n",
    "    y_sequences.append(y_encoded[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "print(f\"\\nâœ… ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ:\")\n",
    "print(f\"  X shape: {X_sequences.shape}\")\n",
    "print(f\"  y shape: {y_sequences.shape}\")\n",
    "\n",
    "# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42, stratify=y_sequences\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° ë¶„í• :\")\n",
    "print(f\"  í›ˆë ¨ ì„¸íŠ¸: {X_train.shape[0]:,}ê°œ\")\n",
    "print(f\"  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape[0]:,}ê°œ\")\n",
    "\n",
    "# í•™ìŠµìš© ë°ì´í„° ì €ì¥ (ë‚˜ì¤‘ì— ì‚¬ìš©)\n",
    "np.save('X_train_sequences.npy', X_train)\n",
    "np.save('X_test_sequences.npy', X_test) \n",
    "np.save('y_train_sequences.npy', y_train)\n",
    "np.save('y_test_sequences.npy', y_test)\n",
    "\n",
    "print(\"ğŸ’¾ ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8ï¸âƒ£ ëª¨ë¸ êµ¬ì¶• ë° í›ˆë ¨\n",
    "\n",
    "### Option 1: LSTM ëª¨ë¸ (TensorFlow í•„ìš”)\n",
    "TensorFlowê°€ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´ LSTM ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "### Option 2: ëŒ€ì•ˆ ëª¨ë¸ (Random Forest)\n",
    "TensorFlowê°€ ì—†ë‹¤ë©´ Random Forestë¡œ ì‹œê³„ì—´ íŠ¹ì„±ì„ í‰ë©´í™”í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    print(\"ğŸ¤– TensorFlow ì‚¬ìš© ê°€ëŠ¥ - LSTM ëª¨ë¸ êµ¬ì¶•\")\n",
    "    \n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    \n",
    "    # LSTM ëª¨ë¸ êµ¬ì¶•\n",
    "    n_features = X_train.shape[2]\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    print(f\"  ì…ë ¥ í˜•íƒœ: ({sequence_length}, {n_features})\")\n",
    "    print(f\"  ì¶œë ¥ í´ë˜ìŠ¤: {n_classes}ê°œ\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´\n",
    "        LSTM(128, return_sequences=True, input_shape=(sequence_length, n_features)),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´  \n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # ì„¸ ë²ˆì§¸ LSTM ë ˆì´ì–´\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # ì™„ì „ì—°ê²° ë ˆì´ì–´\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(25, activation='relu'),\n",
    "        \n",
    "        # ì¶œë ¥ ë ˆì´ì–´ (3í´ë˜ìŠ¤ ë¶„ë¥˜)\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # ëª¨ë¸ ì»´íŒŒì¼\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',  # ì •ìˆ˜ ë¼ë²¨ìš©\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… LSTM ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "    model.summary()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ TensorFlow ë¯¸ì„¤ì¹˜ - Random Forest ëŒ€ì•ˆ ëª¨ë¸ ì‚¬ìš©\")\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    # ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ í‰ë©´í™” (flatten)\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    print(f\"í‰ë©´í™”ëœ ë°ì´í„° í˜•íƒœ:\")\n",
    "    print(f\"  X_train_flat: {X_train_flat.shape}\")\n",
    "    print(f\"  X_test_flat: {X_test_flat.shape}\")\n",
    "    \n",
    "    # Random Forest ëª¨ë¸\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Random Forest ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í›ˆë ¨\n",
    "if TF_AVAILABLE:\n",
    "    print(\"ğŸš€ LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "    \n",
    "    # ì½œë°± ì„¤ì •\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "    ]\n",
    "    \n",
    "    # í›ˆë ¨ ì‹¤í–‰\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=30,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¤„ì„\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… LSTM ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "    \n",
    "    # í›ˆë ¨ ê³¡ì„  ì‹œê°í™”\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    ax1.plot(history.history['loss'], label='Train Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title('LSTM Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax2.set_title('LSTM Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸš€ Random Forest ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "    \n",
    "    # Random Forest í›ˆë ¨\n",
    "    rf_model.fit(X_train_flat, y_train)\n",
    "    \n",
    "    print(\"âœ… Random Forest ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9ï¸âƒ£ ëª¨ë¸ í‰ê°€ ë° ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í‰ê°€\n",
    "if TF_AVAILABLE:\n",
    "    print(\"ğŸ“Š LSTM ëª¨ë¸ í‰ê°€ ì¤‘...\")\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # ì •í™•ë„\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"ğŸ¯ LSTM í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "    target_names = le.classes_\n",
    "    print(f\"\\nğŸ“‹ LSTM ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title('LSTM Model - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸ“Š Random Forest ëª¨ë¸ í‰ê°€ ì¤‘...\")\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred_rf = rf_model.predict(X_test_flat)\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test_flat)\n",
    "    \n",
    "    # ì •í™•ë„\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    print(f\"ğŸ¯ Random Forest í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy_rf:.4f} ({accuracy_rf*100:.2f}%)\")\n",
    "    \n",
    "    # ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "    target_names = le.classes_\n",
    "    print(f\"\\nğŸ“‹ Random Forest ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "    print(classification_report(y_test, y_pred_rf, target_names=target_names))\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬\n",
    "    cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', \n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title('Random Forest Model - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # íŠ¹ì„± ì¤‘ìš”ë„\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    \n",
    "    # íŠ¹ì„± ì´ë¦„ ìƒì„± (í‰ë©´í™”ëœ íŠ¹ì„±)\n",
    "    feature_names = []\n",
    "    for i in range(sequence_length):\n",
    "        for col in available_cols:\n",
    "            feature_names.append(f\"{col}_t-{sequence_length-i-1}\")\n",
    "    \n",
    "    # ìƒìœ„ 20ê°œ ì¤‘ìš” íŠ¹ì„±\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=importance_df.head(20), x='importance', y='feature')\n",
    "    plt.title('Random Forest - ìƒìœ„ 20ê°œ íŠ¹ì„± ì¤‘ìš”ë„')\n",
    "    plt.xlabel('ì¤‘ìš”ë„')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:\")\n",
    "    for i, (feature, importance) in enumerate(importance_df.head(10).values):\n",
    "        print(f\"  {i+1:2d}. {feature:40s}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ”Ÿ ë¯¸ë˜ ì£¼ê°€ ë°©í–¥ì„± ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¯¸ë˜ ì˜ˆì¸¡ (ë‹¤ìŒ 12ì‹œê°„)\n",
    "n_predictions = 12\n",
    "print(f\"ğŸ”® í–¥í›„ {n_predictions}ì‹œê°„ ì£¼ê°€ ë°©í–¥ì„± ì˜ˆì¸¡...\")\n",
    "\n",
    "# ìµœê·¼ ë°ì´í„° ì¤€ë¹„\n",
    "recent_data = df_clean.tail(sequence_length)\n",
    "X_recent = recent_data[available_cols].values\n",
    "X_recent_scaled = scaler.transform(X_recent)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    # LSTM ì˜ˆì¸¡\n",
    "    current_sequence = X_recent_scaled.copy()\n",
    "    \n",
    "    for i in range(n_predictions):\n",
    "        # ì˜ˆì¸¡\n",
    "        X_input = current_sequence.reshape(1, sequence_length, len(available_cols))\n",
    "        pred_proba = model.predict(X_input, verbose=0)\n",
    "        pred_class = np.argmax(pred_proba, axis=1)[0]\n",
    "        pred_label = le.inverse_transform([pred_class])[0]\n",
    "        \n",
    "        predictions.append({\n",
    "            'hour': i + 1,\n",
    "            'prediction': pred_label,\n",
    "            'confidence': np.max(pred_proba),\n",
    "            'probabilities': {\n",
    "                label: prob for label, prob in zip(le.classes_, pred_proba[0])\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ (ë‹¨ìˆœí™”: ë§ˆì§€ë§‰ ê°’ ë³µì‚¬)\n",
    "        current_sequence = np.roll(current_sequence, -1, axis=0)\n",
    "        current_sequence[-1] = current_sequence[-2]  # ë‹¨ìˆœ ë³µì‚¬\n",
    "    \n",
    "    model_name = \"LSTM\"\n",
    "else:\n",
    "    # Random Forest ì˜ˆì¸¡\n",
    "    X_input_flat = X_recent_scaled.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_predictions):\n",
    "        pred_proba = rf_model.predict_proba(X_input_flat)[0]\n",
    "        pred_class = rf_model.predict(X_input_flat)[0]\n",
    "        pred_label = le.inverse_transform([pred_class])[0]\n",
    "        \n",
    "        predictions.append({\n",
    "            'hour': i + 1,\n",
    "            'prediction': pred_label,\n",
    "            'confidence': np.max(pred_proba),\n",
    "            'probabilities': {\n",
    "                label: prob for label, prob in zip(le.classes_, pred_proba)\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    model_name = \"Random Forest\"\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"\\nğŸ¯ {model_name} ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "print(\"=\" * 60)\n",
    "for pred in predictions:\n",
    "    probs = pred['probabilities']\n",
    "    print(f\"ì‹œê°„ +{pred['hour']:2d}: {pred['prediction']:8s} (ì‹ ë¢°ë„: {pred['confidence']:.3f})\")\n",
    "    print(f\"         ìƒì„¸: pos={probs.get('positive', 0):.3f}, neu={probs.get('neutral', 0):.3f}, neg={probs.get('negative', 0):.3f}\")\n",
    "    print()\n",
    "\n",
    "# ì˜ˆì¸¡ ë¶„í¬\n",
    "pred_counts = {}\n",
    "for pred in predictions:\n",
    "    label = pred['prediction']\n",
    "    pred_counts[label] = pred_counts.get(label, 0) + 1\n",
    "\n",
    "print(f\"ğŸ“Š {n_predictions}ì‹œê°„ ì˜ˆì¸¡ ë¶„í¬:\")\n",
    "for label, count in pred_counts.items():\n",
    "    print(f\"  {label}: {count}ì‹œê°„ ({count/n_predictions*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ“Š ê²°ë¡  ë° ìš”ì•½\n",
    "\n",
    "### ğŸ¯ ì£¼ìš” ì„±ê³¼\n",
    "1. **ë©€í‹°ëª¨ë‹¬ ë°ì´í„° í†µí•©**: ë‰´ìŠ¤(FinBERT) + íŠ¸ìœ„í„°(VADER) + ì£¼ê°€ ë°ì´í„° ì„±ê³µì  í†µí•©\n",
    "2. **ì‹œê³„ì—´ ê°ì • ë¶„ì„**: ì‹œê°„ ê¸°ì¤€ ê°ì • ì ìˆ˜ ë™ê¸°í™” ë° íŠ¹ì„± ê³µí•™\n",
    "3. **ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•**: LSTM ë˜ëŠ” Random Forestë¥¼ í†µí•œ ì£¼ê°€ ë°©í–¥ì„± ì˜ˆì¸¡\n",
    "\n",
    "### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n",
    "- **ê°ì • ì ìˆ˜ í†µí•©**: FinBERT(ê¸ˆìœµ íŠ¹í™”) 60% + VADER(ì‹¤ì‹œê°„ì„±) 40% ê°€ì¤‘ í‰ê· \n",
    "- **ì‹œê°„ì  íŠ¹ì„±**: ê°ì • ê°•ë„, ë³€í™”ìœ¨, ì´ë™í‰ê·  ë“±ì´ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ì—­í• \n",
    "- **ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜**: ìƒìŠ¹(positive), ìœ ì§€(neutral), í•˜ë½(negative) 3ê°œ í´ë˜ìŠ¤\n",
    "\n",
    "### ğŸ”§ ê¸°ìˆ ì  íŠ¹ì§•\n",
    "- **ë°ì´í„° ì „ì²˜ë¦¬**: 1ì‹œê°„ ë‹¨ìœ„ ì‹œê³„ì—´ ë™ê¸°í™”\n",
    "- **íŠ¹ì„± ê³µí•™**: 24ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ ê°ì • ë° ê¸°ìˆ ì  ì§€í‘œ\n",
    "- **ëª¨ë¸ ìœ ì—°ì„±**: TensorFlow ìœ ë¬´ì— ë”°ë¥¸ LSTM/Random Forest ìë™ ì„ íƒ\n",
    "\n",
    "### ğŸ“ˆ í™œìš© ë°©ì•ˆ\n",
    "1. **ì‹¤ì‹œê°„ ì£¼ê°€ ì˜ˆì¸¡**: ìµœì‹  ê°ì • ë°ì´í„°ë¡œ ì§€ì†ì  ì˜ˆì¸¡ ì—…ë°ì´íŠ¸\n",
    "2. **í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬**: ê°ì • ê¸°ë°˜ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì „ëµ ìˆ˜ë¦½\n",
    "3. **ì•Œê³ ë¦¬ì¦˜ íŠ¸ë ˆì´ë”©**: ê°ì • ì ìˆ˜ë¥¼ í™œìš©í•œ ìë™ ë§¤ë§¤ ì‹œìŠ¤í…œ\n",
    "\n",
    "### âš ï¸ ì£¼ì˜ì‚¬í•­\n",
    "- ì´ ëª¨ë¸ì€ **êµìœ¡ ë° ì—°êµ¬ ëª©ì **ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤\n",
    "- **ì‹¤ì œ íˆ¬ì ê²°ì •**ì—ëŠ” ì¶”ê°€ì ì¸ ê²€ì¦ê³¼ ë¦¬ìŠ¤í¬ ê´€ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤\n",
    "- ê¸ˆìœµ ì‹œì¥ì€ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ìš”ì†Œë“¤ì´ ë§ìœ¼ë¯€ë¡œ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
