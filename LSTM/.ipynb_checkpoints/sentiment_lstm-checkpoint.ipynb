{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감정분석 기반 주가 예측 시스템 (LSTM)\n",
    "\n",
    "이 노트북은 다음 3가지 데이터를 통합하여 주가 방향성을 예측합니다:\n",
    "\n",
    "1. **📰 뉴스 감정분석 (FinBERT)**: 금융 뉴스의 감정 점수\n",
    "2. **🐦 트위터 감정분석 (VADER)**: 소셜미디어의 실시간 감정\n",
    "3. **📈 주가 데이터**: AAPL 1시간 간격 주가 정보\n",
    "\n",
    "## 목표\n",
    "- **입력**: 과거 24시간의 감정 + 주가 데이터\n",
    "- **출력**: 다음 시간의 주가 방향성 (상승/유지/하락)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ 라이브러리 Import 및 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 기본 라이브러리 로딩 완료!\n",
      "📊 사용 환경: pandas 2.2.3, numpy 1.24.4\n",
      "🤖 TensorFlow 2.13.0 사용 가능\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 기본 ML 라이브러리\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"✅ 기본 라이브러리 로딩 완료!\")\n",
    "print(f\"📊 사용 환경: pandas {pd.__version__}, numpy {np.__version__}\")\n",
    "\n",
    "# TensorFlow는 설치 문제로 인해 나중에 처리\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"🤖 TensorFlow {tf.__version__} 사용 가능\")\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ TensorFlow 미설치 - 대안 모델 사용 예정\")\n",
    "    TF_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ 데이터 파일 경로 설정 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 데이터 파일 확인:\n",
      "==================================================\n",
      "✅ 📈 주가 데이터: ../stock/AAPL_1hour_data_365days.csv (1.9MB)\n",
      "✅ 📰 뉴스 감정분석: ../finnhub/AAPL_finnhub_processed_final.csv (3.0MB)\n",
      "✅ 🐦 트위터 감정분석: ../X_data/merged_tweets_with_sentiment.csv (1.9MB)\n",
      "\n",
      "🎉 모든 데이터 파일이 준비되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 데이터 파일 경로 설정\n",
    "stock_file = \"../stock/AAPL_1hour_data_365days.csv\"  # 주가 데이터\n",
    "news_file = \"../finnhub/AAPL_finnhub_processed_final.csv\"  # 뉴스 감정분석\n",
    "twitter_file = \"../X_data/merged_tweets_with_sentiment.csv\"  # 트위터 감정분석\n",
    "\n",
    "# 파일 존재 여부 확인\n",
    "files_info = {\n",
    "    \"📈 주가 데이터\": stock_file,\n",
    "    \"📰 뉴스 감정분석\": news_file, \n",
    "    \"🐦 트위터 감정분석\": twitter_file\n",
    "}\n",
    "\n",
    "print(\"📁 데이터 파일 확인:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_files_exist = True\n",
    "for name, path in files_info.items():\n",
    "    if os.path.exists(path):\n",
    "        size_mb = os.path.getsize(path) / (1024*1024)\n",
    "        print(f\"✅ {name}: {path} ({size_mb:.1f}MB)\")\n",
    "    else:\n",
    "        print(f\"❌ {name}: {path} (파일 없음)\")\n",
    "        all_files_exist = False\n",
    "\n",
    "if all_files_exist:\n",
    "    print(\"\\n🎉 모든 데이터 파일이 준비되었습니다!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ 일부 파일이 누락되었습니다. 경로를 확인해주세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ 데이터 로딩 및 기본 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 주가 데이터 로딩...\n",
      "✅ 주가 데이터: 3,889개 행\n",
      "📅 기간: 2024-06-17 08:00:00+00:00 ~ 2025-06-13 23:00:00+00:00\n",
      "📋 컬럼: ['Datetime', 'Close', 'High', 'Low', 'Open', 'Volume', 'Returns', 'Log_Returns', 'SMA_10', 'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'RSI', 'BB_Middle', 'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Position', 'Volatility_10', 'Volatility_20', 'Price_Change', 'Price_Change_Pct', 'HL_Spread', 'HL_Spread_Pct', 'Hour', 'DayOfWeek', 'Month', 'Quarter', 'Is_Trading_Hours', 'Is_Market_Open', 'Is_Premarket', 'Is_Aftermarket', 'Is_Extended_Hours']\n",
      "\n",
      "주가 데이터 미리보기:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Returns</th>\n",
       "      <th>Log_Returns</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>...</th>\n",
       "      <th>HL_Spread_Pct</th>\n",
       "      <th>Hour</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Month</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Is_Trading_Hours</th>\n",
       "      <th>Is_Market_Open</th>\n",
       "      <th>Is_Premarket</th>\n",
       "      <th>Is_Aftermarket</th>\n",
       "      <th>Is_Extended_Hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-06-17 08:00:00+00:00</td>\n",
       "      <td>214.350</td>\n",
       "      <td>214.36</td>\n",
       "      <td>212.82</td>\n",
       "      <td>213.24</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718451</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-17 09:00:00+00:00</td>\n",
       "      <td>213.930</td>\n",
       "      <td>214.62</td>\n",
       "      <td>213.66</td>\n",
       "      <td>214.24</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001959</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448745</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-06-17 10:00:00+00:00</td>\n",
       "      <td>213.620</td>\n",
       "      <td>214.10</td>\n",
       "      <td>213.61</td>\n",
       "      <td>213.96</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001449</td>\n",
       "      <td>-0.001450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229379</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-06-17 11:00:00+00:00</td>\n",
       "      <td>213.960</td>\n",
       "      <td>214.18</td>\n",
       "      <td>213.62</td>\n",
       "      <td>213.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261731</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-06-17 12:00:00+00:00</td>\n",
       "      <td>214.145</td>\n",
       "      <td>214.60</td>\n",
       "      <td>212.62</td>\n",
       "      <td>213.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924607</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime    Close    High     Low    Open  Volume  \\\n",
       "0 2024-06-17 08:00:00+00:00  214.350  214.36  212.82  213.24       0   \n",
       "1 2024-06-17 09:00:00+00:00  213.930  214.62  213.66  214.24       0   \n",
       "2 2024-06-17 10:00:00+00:00  213.620  214.10  213.61  213.96       0   \n",
       "3 2024-06-17 11:00:00+00:00  213.960  214.18  213.62  213.62       0   \n",
       "4 2024-06-17 12:00:00+00:00  214.145  214.60  212.62  213.94       0   \n",
       "\n",
       "    Returns  Log_Returns  SMA_10  SMA_20  ...  HL_Spread_Pct  Hour  DayOfWeek  \\\n",
       "0       NaN          NaN     NaN     NaN  ...       0.718451     8          0   \n",
       "1 -0.001959    -0.001961     NaN     NaN  ...       0.448745     9          0   \n",
       "2 -0.001449    -0.001450     NaN     NaN  ...       0.229379    10          0   \n",
       "3  0.001592     0.001590     NaN     NaN  ...       0.261731    11          0   \n",
       "4  0.000865     0.000864     NaN     NaN  ...       0.924607    12          0   \n",
       "\n",
       "   Month  Quarter  Is_Trading_Hours  Is_Market_Open  Is_Premarket  \\\n",
       "0      6        2                 0               0             1   \n",
       "1      6        2                 1               1             0   \n",
       "2      6        2                 1               1             0   \n",
       "3      6        2                 1               1             0   \n",
       "4      6        2                 1               1             0   \n",
       "\n",
       "   Is_Aftermarket  Is_Extended_Hours  \n",
       "0               0                  1  \n",
       "1               0                  0  \n",
       "2               0                  0  \n",
       "3               0                  0  \n",
       "4               0                  0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 주가 데이터 로드\n",
    "print(\"📈 주가 데이터 로딩...\")\n",
    "stock_df = pd.read_csv(stock_file)\n",
    "stock_df['Datetime'] = pd.to_datetime(stock_df['Datetime'])\n",
    "stock_df = stock_df.sort_values('Datetime').reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ 주가 데이터: {len(stock_df):,}개 행\")\n",
    "print(f\"📅 기간: {stock_df['Datetime'].min()} ~ {stock_df['Datetime'].max()}\")\n",
    "print(f\"📋 컬럼: {list(stock_df.columns)}\")\n",
    "print(f\"\\n주가 데이터 미리보기:\")\n",
    "stock_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📰 뉴스 감정분석 데이터 로딩...\n",
      "✅ 뉴스 데이터: 8,767개 행\n",
      "📅 기간: 2023-03-22 18:22:51 ~ 2025-06-14 09:55:00\n",
      "📋 컬럼: ['Date', 'full_text', 'sentiment', 'neg', 'neu', 'pos']\n",
      "\n",
      "📊 뉴스 감정분석 결과 분포:\n",
      "sentiment\n",
      "neutral     4731\n",
      "positive    2384\n",
      "negative    1652\n",
      "Name: count, dtype: int64\n",
      "\n",
      "뉴스 데이터 미리보기:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-22 18:22:51</td>\n",
       "      <td>Stock Market Today: Stock Market News And Anal...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>8.965701e-06</td>\n",
       "      <td>0.999617</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-19 00:47:00</td>\n",
       "      <td>Chipmaker Nvidia surpasses Microsoft as most v...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>6.796851e-07</td>\n",
       "      <td>0.995314</td>\n",
       "      <td>0.004685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-06-19 01:17:26</td>\n",
       "      <td>Extended offers dampen excitement for China's ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>9.999908e-01</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-06-19 01:31:00</td>\n",
       "      <td>Qualcomm Ends $75M Lawsuit Linked To Apple's P...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.074722e-07</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-06-19 01:45:00</td>\n",
       "      <td>LONDON MARKET EARLY CALL: UK inflation data ah...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>6.354484e-07</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date                                          full_text  \\\n",
       "0 2023-03-22 18:22:51  Stock Market Today: Stock Market News And Anal...   \n",
       "1 2024-06-19 00:47:00  Chipmaker Nvidia surpasses Microsoft as most v...   \n",
       "2 2024-06-19 01:17:26  Extended offers dampen excitement for China's ...   \n",
       "3 2024-06-19 01:31:00  Qualcomm Ends $75M Lawsuit Linked To Apple's P...   \n",
       "4 2024-06-19 01:45:00  LONDON MARKET EARLY CALL: UK inflation data ah...   \n",
       "\n",
       "  sentiment           neg       neu       pos  \n",
       "0   neutral  8.965701e-06  0.999617  0.000374  \n",
       "1   neutral  6.796851e-07  0.995314  0.004685  \n",
       "2  negative  9.999908e-01  0.000005  0.000004  \n",
       "3   neutral  1.074722e-07  0.999995  0.000005  \n",
       "4   neutral  6.354484e-07  0.999995  0.000005  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 뉴스 감정분석 데이터 로드\n",
    "print(\"📰 뉴스 감정분석 데이터 로딩...\")\n",
    "news_df = pd.read_csv(news_file)\n",
    "news_df['Date'] = pd.to_datetime(news_df['Date'])\n",
    "news_df = news_df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ 뉴스 데이터: {len(news_df):,}개 행\")\n",
    "print(f\"📅 기간: {news_df['Date'].min()} ~ {news_df['Date'].max()}\")\n",
    "print(f\"📋 컬럼: {list(news_df.columns)}\")\n",
    "\n",
    "# 감정분석 결과 분포\n",
    "print(f\"\\n📊 뉴스 감정분석 결과 분포:\")\n",
    "print(news_df['sentiment'].value_counts())\n",
    "print(f\"\\n뉴스 데이터 미리보기:\")\n",
    "news_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐦 트위터 감정분석 데이터 로딩...\n",
      "✅ 트위터 데이터: 8,594개 행\n",
      "📅 기간: 2020-01-17 01:05:39 ~ 2025-06-16 04:36:37\n",
      "📋 컬럼: ['created_at', 'full_text', 'username', 'sentiment', 'neg', 'neu', 'pos']\n",
      "\n",
      "📊 트위터 감정분석 결과 분포:\n",
      "sentiment\n",
      "positive    4609\n",
      "negative    2184\n",
      "neutral     1801\n",
      "Name: count, dtype: int64\n",
      "\n",
      "👥 사용자별 트윗 수:\n",
      "username\n",
      "@tim_cook          838\n",
      "@marcorubio        796\n",
      "@BillAckman        792\n",
      "@michaelbatnick    757\n",
      "@elonmusk          719\n",
      "@RayDalio          706\n",
      "@JDVance           675\n",
      "@LizAnnSonders     672\n",
      "@CathieDWood       656\n",
      "@WhiteHouse        620\n",
      "Name: count, dtype: int64\n",
      "\n",
      "트위터 데이터 미리보기:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>username</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-17 01:05:39</td>\n",
       "      <td>There are countless ways to make a difference....</td>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-18 16:25:52</td>\n",
       "      <td>Glad to support the essential work of our frie...</td>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-18 21:22:26</td>\n",
       "      <td>When you add up every ambitious dreamer, every...</td>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-19 19:58:20</td>\n",
       "      <td>What an amazing welcome back to Ireland! Thank...</td>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-20 09:39:31</td>\n",
       "      <td>\"I have the audacity to believe that peoples e...</td>\n",
       "      <td>@tim_cook</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           created_at                                          full_text  \\\n",
       "0 2020-01-17 01:05:39  There are countless ways to make a difference....   \n",
       "1 2020-01-18 16:25:52  Glad to support the essential work of our frie...   \n",
       "2 2020-01-18 21:22:26  When you add up every ambitious dreamer, every...   \n",
       "3 2020-01-19 19:58:20  What an amazing welcome back to Ireland! Thank...   \n",
       "4 2020-01-20 09:39:31  \"I have the audacity to believe that peoples e...   \n",
       "\n",
       "    username sentiment    neg    neu    pos  \n",
       "0  @tim_cook  positive  0.031  0.785  0.183  \n",
       "1  @tim_cook  positive  0.000  0.632  0.368  \n",
       "2  @tim_cook  positive  0.043  0.625  0.332  \n",
       "3  @tim_cook  positive  0.000  0.615  0.385  \n",
       "4  @tim_cook  positive  0.000  0.791  0.209  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 트위터 감정분석 데이터 로드\n",
    "print(\"🐦 트위터 감정분석 데이터 로딩...\")\n",
    "twitter_df = pd.read_csv(twitter_file)\n",
    "twitter_df['created_at'] = pd.to_datetime(twitter_df['created_at'])\n",
    "twitter_df = twitter_df.sort_values('created_at').reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ 트위터 데이터: {len(twitter_df):,}개 행\")\n",
    "print(f\"📅 기간: {twitter_df['created_at'].min()} ~ {twitter_df['created_at'].max()}\")\n",
    "print(f\"📋 컬럼: {list(twitter_df.columns)}\")\n",
    "\n",
    "# 감정분석 결과 분포\n",
    "print(f\"\\n📊 트위터 감정분석 결과 분포:\")\n",
    "print(twitter_df['sentiment'].value_counts())\n",
    "\n",
    "# 사용자별 트윗 수\n",
    "print(f\"\\n👥 사용자별 트윗 수:\")\n",
    "print(twitter_df['username'].value_counts().head(10))\n",
    "print(f\"\\n트위터 데이터 미리보기:\")\n",
    "twitter_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4️⃣ 시간 기준 데이터 동기화\n",
    "\n",
    "3개의 서로 다른 데이터셋을 1시간 단위로 동기화합니다:\n",
    "- **주가**: 1시간 간격으로 이미 정리됨\n",
    "- **뉴스**: 불규칙한 시간 → 1시간 단위로 집계\n",
    "- **트위터**: 실시간 → 1시간 단위로 집계\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 시간 기준 데이터 동기화 시작...\n",
      "📰 뉴스 데이터 시간별 집계: 3,973개 시간대\n",
      "🐦 트위터 데이터 시간별 집계: 4,915개 시간대\n",
      "📈 주가 데이터 시간별 정리: 3,889개 시간대\n"
     ]
    }
   ],
   "source": [
    "print(\"🔄 시간 기준 데이터 동기화 시작...\")\n",
    "\n",
    "# 1. 주가 데이터를 기준으로 1시간 단위로 집계\n",
    "stock_df['hour'] = stock_df['Datetime'].dt.floor('H')\n",
    "\n",
    "# 2. 뉴스 데이터를 1시간 단위로 집계\n",
    "news_df['hour'] = news_df['Date'].dt.floor('H')\n",
    "news_hourly = news_df.groupby('hour').agg({\n",
    "    'pos': 'mean',\n",
    "    'neu': 'mean', \n",
    "    'neg': 'mean',\n",
    "    'sentiment': lambda x: x.mode().iloc[0] if not x.empty else 'neutral'\n",
    "}).reset_index()\n",
    "news_hourly.columns = ['hour', 'news_pos', 'news_neu', 'news_neg', 'news_sentiment']\n",
    "\n",
    "print(f\"📰 뉴스 데이터 시간별 집계: {len(news_hourly):,}개 시간대\")\n",
    "\n",
    "# 3. 트위터 데이터를 1시간 단위로 집계\n",
    "twitter_df['hour'] = twitter_df['created_at'].dt.floor('H')\n",
    "twitter_hourly = twitter_df.groupby('hour').agg({\n",
    "    'pos': 'mean',\n",
    "    'neu': 'mean',\n",
    "    'neg': 'mean', \n",
    "    'sentiment': lambda x: x.mode().iloc[0] if not x.empty else 'neutral'\n",
    "}).reset_index()\n",
    "twitter_hourly.columns = ['hour', 'twitter_pos', 'twitter_neu', 'twitter_neg', 'twitter_sentiment']\n",
    "\n",
    "print(f\"🐦 트위터 데이터 시간별 집계: {len(twitter_hourly):,}개 시간대\")\n",
    "\n",
    "# 4. 주가 데이터도 시간별로 정리 (중복 제거)\n",
    "stock_hourly = stock_df.drop_duplicates(subset=['hour'], keep='last').reset_index(drop=True)\n",
    "print(f\"📈 주가 데이터 시간별 정리: {len(stock_hourly):,}개 시간대\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 데이터 병합 중...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on datetime64[ns, UTC] and datetime64[ns] columns for key 'hour'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 5. 3개 데이터 병합\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔗 데이터 병합 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m combined \u001b[38;5;241m=\u001b[39m stock_hourly\u001b[38;5;241m.\u001b[39mmerge(news_hourly, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m combined \u001b[38;5;241m=\u001b[39m combined\u001b[38;5;241m.\u001b[39mmerge(twitter_hourly, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 데이터 병합 완료: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(combined)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m개 행\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[0;32m  10833\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10834\u001b[0m         right,\n\u001b[0;32m  10835\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[0;32m  10836\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[0;32m  10837\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[0;32m  10838\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[0;32m  10839\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[0;32m  10840\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[0;32m  10841\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m  10842\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[0;32m  10843\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m  10844\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[0;32m  10845\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m  10846\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[0;32m    173\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[0;32m    174\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[0;32m    175\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[0;32m    176\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[0;32m    177\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[0;32m    178\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[0;32m    179\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    180\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[0;32m    181\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1518\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lk\u001b[38;5;241m.\u001b[39mdtype, DatetimeTZDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1516\u001b[0m     rk\u001b[38;5;241m.\u001b[39mdtype, DatetimeTZDtype\n\u001b[0;32m   1517\u001b[0m ):\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lk\u001b[38;5;241m.\u001b[39mdtype, DatetimeTZDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1520\u001b[0m     rk\u001b[38;5;241m.\u001b[39mdtype, DatetimeTZDtype\n\u001b[0;32m   1521\u001b[0m ):\n\u001b[0;32m   1522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on datetime64[ns, UTC] and datetime64[ns] columns for key 'hour'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# 5. 3개 데이터 병합\n",
    "print(\"🔗 데이터 병합 중...\")\n",
    "combined = stock_hourly.merge(news_hourly, on='hour', how='left')\n",
    "combined = combined.merge(twitter_hourly, on='hour', how='left')\n",
    "\n",
    "print(f\"✅ 데이터 병합 완료: {len(combined):,}개 행\")\n",
    "print(f\"📅 통합 데이터 기간: {combined['hour'].min()} ~ {combined['hour'].max()}\")\n",
    "\n",
    "# 6. 결측치 처리 (forward fill)\n",
    "sentiment_cols = ['news_pos', 'news_neu', 'news_neg', 'twitter_pos', 'twitter_neu', 'twitter_neg']\n",
    "print(f\"\\n🔧 결측치 처리 전:\")\n",
    "print(combined[sentiment_cols].isnull().sum())\n",
    "\n",
    "combined[sentiment_cols] = combined[sentiment_cols].fillna(method='ffill')\n",
    "combined[sentiment_cols] = combined[sentiment_cols].fillna(0.5)  # 초기값\n",
    "\n",
    "# 감정 라벨 결측치 처리\n",
    "combined['news_sentiment'] = combined['news_sentiment'].fillna('neutral')\n",
    "combined['twitter_sentiment'] = combined['twitter_sentiment'].fillna('neutral')\n",
    "\n",
    "print(f\"\\n🔧 결측치 처리 후:\")\n",
    "print(combined[sentiment_cols].isnull().sum())\n",
    "\n",
    "print(f\"\\n✅ 최종 통합 데이터: {len(combined):,}개 행, {len(combined.columns)}개 컬럼\")\n",
    "combined.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5️⃣ 통합 특성 생성 및 Feature Engineering\n",
    "\n",
    "감정 점수를 통합하고 추가적인 특성들을 생성합니다:\n",
    "- **감정 점수 통합**: FinBERT(60%) + VADER(40%) 가중 평균\n",
    "- **감정 강도**: 최대값 - 최소값으로 감정의 확신도 측정\n",
    "- **시간적 특성**: 변화율, 이동평균, 기술적 지표\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 통합 특성 생성 중...\")\n",
    "\n",
    "df = combined.copy()\n",
    "\n",
    "# 1. 감정 점수 통합 (가중 평균: FinBERT 60%, VADER 40%)\n",
    "df['combined_pos'] = 0.6 * df['news_pos'] + 0.4 * df['twitter_pos']\n",
    "df['combined_neu'] = 0.6 * df['news_neu'] + 0.4 * df['twitter_neu']\n",
    "df['combined_neg'] = 0.6 * df['news_neg'] + 0.4 * df['twitter_neg']\n",
    "\n",
    "print(\"✅ 감정 점수 통합 완료\")\n",
    "\n",
    "# 2. 감정 강도 및 극성\n",
    "df['news_sentiment_intensity'] = df[['news_pos', 'news_neu', 'news_neg']].max(axis=1) - df[['news_pos', 'news_neu', 'news_neg']].min(axis=1)\n",
    "df['twitter_sentiment_intensity'] = df[['twitter_pos', 'twitter_neu', 'twitter_neg']].max(axis=1) - df[['twitter_pos', 'twitter_neu', 'twitter_neg']].min(axis=1)\n",
    "df['combined_sentiment_intensity'] = df[['combined_pos', 'combined_neu', 'combined_neg']].max(axis=1) - df[['combined_pos', 'combined_neu', 'combined_neg']].min(axis=1)\n",
    "\n",
    "print(\"✅ 감정 강도 계산 완료\")\n",
    "\n",
    "# 3. 감정 점수 변화율 (시간별)\n",
    "for col in ['news_pos', 'news_neu', 'news_neg', 'twitter_pos', 'twitter_neu', 'twitter_neg']:\n",
    "    df[f'{col}_change'] = df[col].pct_change()\n",
    "\n",
    "print(\"✅ 감정 점수 변화율 계산 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 감정 점수 이동평균 (3시간, 6시간)\n",
    "for window in [3, 6]:\n",
    "    for col in ['combined_pos', 'combined_neu', 'combined_neg']:\n",
    "        df[f'{col}_ma{window}'] = df[col].rolling(window).mean()\n",
    "\n",
    "print(\"✅ 감정 점수 이동평균 계산 완료\")\n",
    "\n",
    "# 5. 주가 특성\n",
    "df['price_change'] = df['Close'].pct_change()  # 수익률\n",
    "df['price_volatility'] = df['price_change'].rolling(6).std()  # 변동성\n",
    "df['volume_ma'] = df['Volume'].rolling(6).mean()  # 거래량 이동평균\n",
    "\n",
    "print(\"✅ 주가 특성 계산 완료\")\n",
    "\n",
    "# 6. 기술적 지표\n",
    "def calculate_rsi(prices, window=14):\n",
    "    \"\"\"RSI 계산\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "df['sma_5'] = df['Close'].rolling(5).mean()\n",
    "df['sma_10'] = df['Close'].rolling(10).mean()\n",
    "df['rsi'] = calculate_rsi(df['Close'])\n",
    "\n",
    "print(\"✅ 기술적 지표 계산 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 시간 특성\n",
    "df['hour_of_day'] = df['hour'].dt.hour\n",
    "df['day_of_week'] = df['hour'].dt.dayofweek\n",
    "df['is_trading_hours'] = ((df['hour_of_day'] >= 9) & (df['hour_of_day'] <= 16)).astype(int)\n",
    "\n",
    "print(\"✅ 시간 특성 계산 완료\")\n",
    "\n",
    "# 8. 타겟 라벨 생성 (다음 시간의 주가 방향성)\n",
    "df['next_price_change'] = df['price_change'].shift(-1)  # 다음 시간 수익률\n",
    "\n",
    "# 임계값 설정: ±0.5%\n",
    "def classify_direction(change):\n",
    "    if pd.isna(change):\n",
    "        return 'neutral'\n",
    "    elif change > 0.005:  # +0.5%\n",
    "        return 'positive'\n",
    "    elif change < -0.005:  # -0.5%\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['target'] = df['next_price_change'].apply(classify_direction)\n",
    "\n",
    "print(\"✅ 타겟 라벨 생성 완료\")\n",
    "\n",
    "# 결측치 제거\n",
    "df_clean = df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n🎉 특성 생성 완료!\")\n",
    "print(f\"📊 최종 데이터: {len(df_clean):,}개 행, {len(df_clean.columns)}개 컬럼\")\n",
    "print(f\"\\n📈 타겟 분포:\")\n",
    "print(df_clean['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6️⃣ 데이터 시각화 및 탐색적 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 점수와 주가의 시간적 변화 시각화\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# 1. 주가 변화\n",
    "recent_data = df_clean.tail(168)  # 최근 1주일 데이터\n",
    "axes[0].plot(recent_data['hour'], recent_data['Close'], color='blue', alpha=0.8)\n",
    "axes[0].set_title('📈 AAPL 주가 변화 (최근 1주일)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('주가 ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. 뉴스 vs 트위터 감정 비교\n",
    "axes[1].plot(recent_data['hour'], recent_data['news_pos'], label='뉴스 Positive (FinBERT)', color='green', alpha=0.7)\n",
    "axes[1].plot(recent_data['hour'], recent_data['twitter_pos'], label='트위터 Positive (VADER)', color='lightgreen', alpha=0.7)\n",
    "axes[1].plot(recent_data['hour'], recent_data['combined_pos'], label='통합 Positive', color='darkgreen', linewidth=2)\n",
    "axes[1].set_title('😊 Positive 감정 점수 비교', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Positive 점수')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. 감정 강도\n",
    "axes[2].bar(recent_data['hour'], recent_data['combined_sentiment_intensity'], \n",
    "           color=['red' if x == 'negative' else 'gray' if x == 'neutral' else 'green' \n",
    "                  for x in recent_data['target']], alpha=0.7)\n",
    "axes[2].set_title('💪 통합 감정 강도 (색상: 다음 시간 주가 방향)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('시간')\n",
    "axes[2].set_ylabel('감정 강도')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 점수와 주가 수익률의 상관관계 분석\n",
    "correlation_cols = [\n",
    "    'news_pos', 'news_neu', 'news_neg',\n",
    "    'twitter_pos', 'twitter_neu', 'twitter_neg',\n",
    "    'combined_pos', 'combined_neu', 'combined_neg',\n",
    "    'news_sentiment_intensity', 'twitter_sentiment_intensity', 'combined_sentiment_intensity',\n",
    "    'price_change', 'next_price_change'\n",
    "]\n",
    "\n",
    "corr_matrix = df_clean[correlation_cols].corr()\n",
    "\n",
    "# 상관관계 히트맵\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('🔥 감정 점수와 주가 수익률 상관관계', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 주요 상관관계 출력\n",
    "print(\"📊 주가 수익률과의 주요 상관관계:\")\n",
    "price_corr = corr_matrix['next_price_change'].abs().sort_values(ascending=False)\n",
    "for feature, corr_val in price_corr.head(10).items():\n",
    "    if feature != 'next_price_change':\n",
    "        print(f\"  {feature:30s}: {corr_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7️⃣ LSTM용 시퀀스 데이터 준비\n",
    "\n",
    "LSTM 모델을 위해 시계열 데이터를 시퀀스 형태로 변환합니다:\n",
    "- **시퀀스 길이**: 24시간 (24개 시점)\n",
    "- **입력**: 과거 24시간의 감정 + 주가 특성\n",
    "- **출력**: 다음 시간의 주가 방향성 (positive/neutral/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 시퀀스 데이터 준비\n",
    "sequence_length = 24  # 24시간 시퀀스\n",
    "\n",
    "print(f\"🔄 LSTM 시퀀스 데이터 준비 중... (시퀀스 길이: {sequence_length})\")\n",
    "\n",
    "# 특성 선택 (감정 + 주가 + 기술적 지표)\n",
    "feature_cols = [\n",
    "    # 감정 점수 (원본)\n",
    "    'news_pos', 'news_neu', 'news_neg',\n",
    "    'twitter_pos', 'twitter_neu', 'twitter_neg',\n",
    "    # 통합 감정 점수\n",
    "    'combined_pos', 'combined_neu', 'combined_neg',\n",
    "    # 감정 강도\n",
    "    'news_sentiment_intensity', 'twitter_sentiment_intensity', 'combined_sentiment_intensity',\n",
    "    # 주가 특성\n",
    "    'Close', 'Volume', 'price_change', 'price_volatility',\n",
    "    # 기술적 지표\n",
    "    'rsi', 'sma_5', 'sma_10',\n",
    "    # 시간 특성\n",
    "    'hour_of_day', 'day_of_week', 'is_trading_hours'\n",
    "]\n",
    "\n",
    "# 사용 가능한 컬럼만 선택\n",
    "available_cols = [col for col in feature_cols if col in df_clean.columns]\n",
    "print(f\"📋 사용할 특성: {len(available_cols)}개\")\n",
    "print(f\"  {available_cols}\")\n",
    "\n",
    "# 데이터 준비\n",
    "X = df_clean[available_cols].values\n",
    "y = df_clean['target'].values\n",
    "\n",
    "print(f\"\\n📊 데이터 형태:\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 인코딩\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"📊 레이블 매핑: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"✅ 데이터 정규화 완료\")\n",
    "\n",
    "# 시퀀스 데이터 생성\n",
    "X_sequences, y_sequences = [], []\n",
    "\n",
    "for i in range(len(X_scaled) - sequence_length):\n",
    "    X_sequences.append(X_scaled[i:(i + sequence_length)])\n",
    "    y_sequences.append(y_encoded[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "print(f\"\\n✅ 시퀀스 데이터 생성 완료:\")\n",
    "print(f\"  X shape: {X_sequences.shape}\")\n",
    "print(f\"  y shape: {y_sequences.shape}\")\n",
    "\n",
    "# 훈련/테스트 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42, stratify=y_sequences\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 데이터 분할:\")\n",
    "print(f\"  훈련 세트: {X_train.shape[0]:,}개\")\n",
    "print(f\"  테스트 세트: {X_test.shape[0]:,}개\")\n",
    "\n",
    "# 학습용 데이터 저장 (나중에 사용)\n",
    "np.save('X_train_sequences.npy', X_train)\n",
    "np.save('X_test_sequences.npy', X_test) \n",
    "np.save('y_train_sequences.npy', y_train)\n",
    "np.save('y_test_sequences.npy', y_test)\n",
    "\n",
    "print(\"💾 시퀀스 데이터 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8️⃣ 모델 구축 및 훈련\n",
    "\n",
    "### Option 1: LSTM 모델 (TensorFlow 필요)\n",
    "TensorFlow가 설치되어 있다면 LSTM 모델을 사용합니다.\n",
    "\n",
    "### Option 2: 대안 모델 (Random Forest)\n",
    "TensorFlow가 없다면 Random Forest로 시계열 특성을 평면화하여 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    print(\"🤖 TensorFlow 사용 가능 - LSTM 모델 구축\")\n",
    "    \n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    \n",
    "    # LSTM 모델 구축\n",
    "    n_features = X_train.shape[2]\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    print(f\"  입력 형태: ({sequence_length}, {n_features})\")\n",
    "    print(f\"  출력 클래스: {n_classes}개\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # 첫 번째 LSTM 레이어\n",
    "        LSTM(128, return_sequences=True, input_shape=(sequence_length, n_features)),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # 두 번째 LSTM 레이어  \n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # 세 번째 LSTM 레이어\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # 완전연결 레이어\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(25, activation='relu'),\n",
    "        \n",
    "        # 출력 레이어 (3클래스 분류)\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',  # 정수 라벨용\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"✅ LSTM 모델 구축 완료!\")\n",
    "    model.summary()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ TensorFlow 미설치 - Random Forest 대안 모델 사용\")\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    # 시퀀스 데이터를 평면화 (flatten)\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    print(f\"평면화된 데이터 형태:\")\n",
    "    print(f\"  X_train_flat: {X_train_flat.shape}\")\n",
    "    print(f\"  X_test_flat: {X_test_flat.shape}\")\n",
    "    \n",
    "    # Random Forest 모델\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Random Forest 모델 준비 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "if TF_AVAILABLE:\n",
    "    print(\"🚀 LSTM 모델 훈련 시작...\")\n",
    "    \n",
    "    # 콜백 설정\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "    ]\n",
    "    \n",
    "    # 훈련 실행\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=30,  # 빠른 테스트를 위해 줄임\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"✅ LSTM 모델 훈련 완료!\")\n",
    "    \n",
    "    # 훈련 곡선 시각화\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    ax1.plot(history.history['loss'], label='Train Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title('LSTM Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax2.set_title('LSTM Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"🚀 Random Forest 모델 훈련 시작...\")\n",
    "    \n",
    "    # Random Forest 훈련\n",
    "    rf_model.fit(X_train_flat, y_train)\n",
    "    \n",
    "    print(\"✅ Random Forest 모델 훈련 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9️⃣ 모델 평가 및 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "if TF_AVAILABLE:\n",
    "    print(\"📊 LSTM 모델 평가 중...\")\n",
    "    \n",
    "    # 예측\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # 정확도\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"🎯 LSTM 테스트 정확도: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # 분류 리포트\n",
    "    target_names = le.classes_\n",
    "    print(f\"\\n📋 LSTM 분류 리포트:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # 혼동 행렬\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title('LSTM Model - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"📊 Random Forest 모델 평가 중...\")\n",
    "    \n",
    "    # 예측\n",
    "    y_pred_rf = rf_model.predict(X_test_flat)\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test_flat)\n",
    "    \n",
    "    # 정확도\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    print(f\"🎯 Random Forest 테스트 정확도: {accuracy_rf:.4f} ({accuracy_rf*100:.2f}%)\")\n",
    "    \n",
    "    # 분류 리포트\n",
    "    target_names = le.classes_\n",
    "    print(f\"\\n📋 Random Forest 분류 리포트:\")\n",
    "    print(classification_report(y_test, y_pred_rf, target_names=target_names))\n",
    "    \n",
    "    # 혼동 행렬\n",
    "    cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', \n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title('Random Forest Model - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # 특성 중요도\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    \n",
    "    # 특성 이름 생성 (평면화된 특성)\n",
    "    feature_names = []\n",
    "    for i in range(sequence_length):\n",
    "        for col in available_cols:\n",
    "            feature_names.append(f\"{col}_t-{sequence_length-i-1}\")\n",
    "    \n",
    "    # 상위 20개 중요 특성\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=importance_df.head(20), x='importance', y='feature')\n",
    "    plt.title('Random Forest - 상위 20개 특성 중요도')\n",
    "    plt.xlabel('중요도')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n📈 상위 10개 중요 특성:\")\n",
    "    for i, (feature, importance) in enumerate(importance_df.head(10).values):\n",
    "        print(f\"  {i+1:2d}. {feature:40s}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🔟 미래 주가 방향성 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미래 예측 (다음 12시간)\n",
    "n_predictions = 12\n",
    "print(f\"🔮 향후 {n_predictions}시간 주가 방향성 예측...\")\n",
    "\n",
    "# 최근 데이터 준비\n",
    "recent_data = df_clean.tail(sequence_length)\n",
    "X_recent = recent_data[available_cols].values\n",
    "X_recent_scaled = scaler.transform(X_recent)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    # LSTM 예측\n",
    "    current_sequence = X_recent_scaled.copy()\n",
    "    \n",
    "    for i in range(n_predictions):\n",
    "        # 예측\n",
    "        X_input = current_sequence.reshape(1, sequence_length, len(available_cols))\n",
    "        pred_proba = model.predict(X_input, verbose=0)\n",
    "        pred_class = np.argmax(pred_proba, axis=1)[0]\n",
    "        pred_label = le.inverse_transform([pred_class])[0]\n",
    "        \n",
    "        predictions.append({\n",
    "            'hour': i + 1,\n",
    "            'prediction': pred_label,\n",
    "            'confidence': np.max(pred_proba),\n",
    "            'probabilities': {\n",
    "                label: prob for label, prob in zip(le.classes_, pred_proba[0])\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # 시퀀스 업데이트 (단순화: 마지막 값 복사)\n",
    "        current_sequence = np.roll(current_sequence, -1, axis=0)\n",
    "        current_sequence[-1] = current_sequence[-2]  # 단순 복사\n",
    "    \n",
    "    model_name = \"LSTM\"\n",
    "else:\n",
    "    # Random Forest 예측\n",
    "    X_input_flat = X_recent_scaled.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_predictions):\n",
    "        pred_proba = rf_model.predict_proba(X_input_flat)[0]\n",
    "        pred_class = rf_model.predict(X_input_flat)[0]\n",
    "        pred_label = le.inverse_transform([pred_class])[0]\n",
    "        \n",
    "        predictions.append({\n",
    "            'hour': i + 1,\n",
    "            'prediction': pred_label,\n",
    "            'confidence': np.max(pred_proba),\n",
    "            'probabilities': {\n",
    "                label: prob for label, prob in zip(le.classes_, pred_proba)\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    model_name = \"Random Forest\"\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n🎯 {model_name} 예측 결과:\")\n",
    "print(\"=\" * 60)\n",
    "for pred in predictions:\n",
    "    probs = pred['probabilities']\n",
    "    print(f\"시간 +{pred['hour']:2d}: {pred['prediction']:8s} (신뢰도: {pred['confidence']:.3f})\")\n",
    "    print(f\"         상세: pos={probs.get('positive', 0):.3f}, neu={probs.get('neutral', 0):.3f}, neg={probs.get('negative', 0):.3f}\")\n",
    "    print()\n",
    "\n",
    "# 예측 분포\n",
    "pred_counts = {}\n",
    "for pred in predictions:\n",
    "    label = pred['prediction']\n",
    "    pred_counts[label] = pred_counts.get(label, 0) + 1\n",
    "\n",
    "print(f\"📊 {n_predictions}시간 예측 분포:\")\n",
    "for label, count in pred_counts.items():\n",
    "    print(f\"  {label}: {count}시간 ({count/n_predictions*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 📊 결론 및 요약\n",
    "\n",
    "### 🎯 주요 성과\n",
    "1. **멀티모달 데이터 통합**: 뉴스(FinBERT) + 트위터(VADER) + 주가 데이터 성공적 통합\n",
    "2. **시계열 감정 분석**: 시간 기준 감정 점수 동기화 및 특성 공학\n",
    "3. **예측 모델 구축**: LSTM 또는 Random Forest를 통한 주가 방향성 예측\n",
    "\n",
    "### 💡 핵심 인사이트\n",
    "- **감정 점수 통합**: FinBERT(금융 특화) 60% + VADER(실시간성) 40% 가중 평균\n",
    "- **시간적 특성**: 감정 강도, 변화율, 이동평균 등이 예측에 중요한 역할\n",
    "- **다중 클래스 분류**: 상승(positive), 유지(neutral), 하락(negative) 3개 클래스\n",
    "\n",
    "### 🔧 기술적 특징\n",
    "- **데이터 전처리**: 1시간 단위 시계열 동기화\n",
    "- **특성 공학**: 24개 이상의 다양한 감정 및 기술적 지표\n",
    "- **모델 유연성**: TensorFlow 유무에 따른 LSTM/Random Forest 자동 선택\n",
    "\n",
    "### 📈 활용 방안\n",
    "1. **실시간 주가 예측**: 최신 감정 데이터로 지속적 예측 업데이트\n",
    "2. **포트폴리오 관리**: 감정 기반 리스크 관리 전략 수립\n",
    "3. **알고리즘 트레이딩**: 감정 점수를 활용한 자동 매매 시스템\n",
    "\n",
    "### ⚠️ 주의사항\n",
    "- 이 모델은 **교육 및 연구 목적**으로 제작되었습니다\n",
    "- **실제 투자 결정**에는 추가적인 검증과 리스크 관리가 필요합니다\n",
    "- 금융 시장은 예측 불가능한 요소들이 많으므로 신중한 접근이 필요합니다\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
