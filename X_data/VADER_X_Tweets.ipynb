{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 4. VADERë¥¼ ì´ìš©í•œ X(Twitter) íŠ¸ìœ— ê°ì •ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import\n",
    "ê°ì •ë¶„ì„ê³¼ ë°ì´í„° ì²˜ë¦¬ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„ì„ ë° ì¡°ì‘ì„ ìœ„í•œ pandas ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "\n",
    "# VADER ê°ì •ë¶„ì„ ëª¨ë¸ì„ ìœ„í•œ NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ re ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-2. VADER ê°ì •ë¶„ì„ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ\n",
    "VADER ê°ì •ë¶„ì„ì— í•„ìš”í•œ ì–´íœ˜ ì‚¬ì „ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\82102\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VADER ê°ì •ë¶„ì„ì— í•„ìš”í•œ ì–´íœ˜ ì‚¬ì „ì„ ë‹¤ìš´ë¡œë“œ\n",
    "# vader_lexicon: ê°ì • ì ìˆ˜ê°€ ë§¤í•‘ëœ ë‹¨ì–´ ì‚¬ì „ ë°ì´í„°\n",
    "# ìµœì´ˆ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ë©´ ë¡œì»¬ì— ì €ì¥ë¨\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-3. ë‹¤ì¤‘ ì‚¬ìš©ì íŠ¸ìœ— ë°ì´í„° ë³‘í•©\n",
    "X_data ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  `user_*.csv` íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•˜ì—¬ í†µí•© ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°œê²¬ëœ ëª¨ë“  CSV íŒŒì¼: ['user_@Ajay_Bagga_tweets.csv', 'user_@BillAckman_tweets.csv', 'user_@CathieDWood_tweets.csv', 'user_@elonmusk_tweets.csv', 'user_@JDVance_tweets.csv', 'user_@LizAnnSonders_tweets.csv', 'user_@marcorubio_tweets.csv', 'user_@michaelbatnick_tweets.csv', 'user_@RayDalio_tweets.csv', 'user_@SecScottBessent_tweets.csv', 'user_@sundarpichai_tweets.csv', 'user_@tim_cook_tweets.csv', 'user_@WhiteHouse_tweets.csv']\n",
      "ì´ 13ê°œì˜ íŒŒì¼ì„ ë³‘í•©í•©ë‹ˆë‹¤.\n",
      "user_@Ajay_Bagga_tweets.csv ì½ê¸° ì™„ë£Œ - 530ê°œ í–‰\n",
      "user_@BillAckman_tweets.csv ì½ê¸° ì™„ë£Œ - 800ê°œ í–‰\n",
      "user_@CathieDWood_tweets.csv ì½ê¸° ì™„ë£Œ - 670ê°œ í–‰\n",
      "user_@elonmusk_tweets.csv ì½ê¸° ì™„ë£Œ - 764ê°œ í–‰\n",
      "user_@JDVance_tweets.csv ì½ê¸° ì™„ë£Œ - 701ê°œ í–‰\n",
      "user_@LizAnnSonders_tweets.csv ì½ê¸° ì™„ë£Œ - 673ê°œ í–‰\n",
      "user_@marcorubio_tweets.csv ì½ê¸° ì™„ë£Œ - 806ê°œ í–‰\n",
      "user_@michaelbatnick_tweets.csv ì½ê¸° ì™„ë£Œ - 760ê°œ í–‰\n",
      "user_@RayDalio_tweets.csv ì½ê¸° ì™„ë£Œ - 725ê°œ í–‰\n",
      "user_@SecScottBessent_tweets.csv ì½ê¸° ì™„ë£Œ - 255ê°œ í–‰\n",
      "user_@sundarpichai_tweets.csv ì½ê¸° ì™„ë£Œ - 585ê°œ í–‰\n",
      "user_@tim_cook_tweets.csv ì½ê¸° ì™„ë£Œ - 838ê°œ í–‰\n",
      "user_@WhiteHouse_tweets.csv ì½ê¸° ì™„ë£Œ - 651ê°œ í–‰\n",
      "ğŸ‰ ì´ 8758ê°œ í–‰ì´ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ğŸ“Š í¬í•¨ëœ ì‚¬ìš©ì ìˆ˜: 13\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…ì— í•„ìš”í•œ ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import glob  # íŒŒì¼ íŒ¨í„´ ë§¤ì¹­ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os    # ìš´ì˜ì²´ì œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from datetime import datetime  # ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "# glob íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ \"user_\"ë¡œ ì‹œì‘í•˜ê³  \".csv\"ë¡œ ëë‚˜ëŠ” ëª¨ë“  íŒŒì¼ ì°¾ê¸°\n",
    "csv_files = glob.glob(\"user_*.csv\")\n",
    "print(f\"ë°œê²¬ëœ ëª¨ë“  CSV íŒŒì¼: {csv_files}\")\n",
    "print(f\"ì´ {len(csv_files)}ê°œì˜ íŒŒì¼ì„ ë³‘í•©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ê° CSV íŒŒì¼ì˜ ë°ì´í„°í”„ë ˆì„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "dfs = []\n",
    "\n",
    "# ë°œê²¬ëœ ëª¨ë“  CSV íŒŒì¼ì„ ìˆœíšŒí•˜ë©° ë°ì´í„° ë¡œë“œ\n",
    "for file in csv_files:\n",
    "    # íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "    if os.path.exists(file):\n",
    "        # CSV íŒŒì¼ì„ pandas ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì½ê¸°\n",
    "        temp_df = pd.read_csv(file)\n",
    "        \n",
    "        # íŒŒì¼ëª…ì—ì„œ ì‚¬ìš©ìëª… ì¶”ì¶œ (ì˜ˆ: \"user_@elonmusk_tweets.csv\" â†’ \"@elonmusk\")\n",
    "        username = file.replace(\"user_\", \"\").replace(\"_tweets.csv\", \"\")\n",
    "        \n",
    "        # ê° í–‰ì— í•´ë‹¹ ì‚¬ìš©ìëª…ì„ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€\n",
    "        temp_df['username'] = username\n",
    "        \n",
    "        # ë¦¬ìŠ¤íŠ¸ì— ë°ì´í„°í”„ë ˆì„ ì¶”ê°€\n",
    "        dfs.append(temp_df)\n",
    "        print(f\"{file} ì½ê¸° ì™„ë£Œ - {len(temp_df)}ê°œ í–‰\")\n",
    "\n",
    "# ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ í•˜ë‚˜ë¡œ ë³‘í•©\n",
    "if dfs:\n",
    "    # concat í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¸ë¡œë¡œ ë³‘í•©í•˜ê³  ì¸ë±ìŠ¤ ë¦¬ì…‹\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"ğŸ‰ ì´ {len(df)}ê°œ í–‰ì´ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"ğŸ“Š í¬í•¨ëœ ì‚¬ìš©ì ìˆ˜: {len(df['username'].unique())}\")\n",
    "else:\n",
    "    print(\"ì½ì„ ìˆ˜ ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-4. URL ì œê±° í•¨ìˆ˜ ì •ì˜\n",
    "íŠ¸ìœ— í…ìŠ¤íŠ¸ì—ì„œ URLì„ ì œê±°í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. URLì€ ê°ì •ë¶„ì„ì— ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ì—ì„œ URLì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        str: URLì´ ì œê±°ëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ HTTP/HTTPS URL íŒ¨í„´ì„ ì°¾ì•„ì„œ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´\n",
    "    # r'https?://\\S+' íŒ¨í„´ ì„¤ëª…:\n",
    "    # - https? : http ë˜ëŠ” https (? ëŠ” sê°€ ìˆê±°ë‚˜ ì—†ê±°ë‚˜)\n",
    "    # - :// : í”„ë¡œí† ì½œ êµ¬ë¶„ì\n",
    "    # - \\S+ : ê³µë°±ì´ ì•„ë‹Œ ë¬¸ìê°€ 1ê°œ ì´ìƒ ì—°ì† (URLì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„)\n",
    "    cleaned_text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # ì•ë’¤ ê³µë°± ì œê±°í•˜ì—¬ ë°˜í™˜\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-5. íŠ¸ìœ— í…ìŠ¤íŠ¸ì—ì„œ URL ì œê±° ì ìš©\n",
    "ì •ì˜í•œ URL ì œê±° í•¨ìˆ˜ë¥¼ ëª¨ë“  íŠ¸ìœ— í…ìŠ¤íŠ¸ì— ì ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì •ì œí•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasì˜ apply() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  íŠ¸ìœ— í…ìŠ¤íŠ¸ì— URL ì œê±° í•¨ìˆ˜ ì ìš©\n",
    "# apply()ëŠ” ì‹œë¦¬ì¦ˆì˜ ê° ìš”ì†Œì— í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì‹œë¦¬ì¦ˆë¥¼ ë°˜í™˜\n",
    "df['full_text'] = df['full_text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-6. ë¹ˆ ë¬¸ìì—´ì„ NaNìœ¼ë¡œ ë³€í™˜\n",
    "URL ì œê±° í›„ ë¹ˆ ë¬¸ìì—´ì´ ëœ í…ìŠ¤íŠ¸ë“¤ì„ pandasì˜ NaN(ê²°ì¸¡ê°’)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_36128\\1257741985.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['full_text'].replace('', pd.NA, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# ë¹ˆ ë¬¸ìì—´('')ì„ pandasì˜ NA(Not Available) ê°’ìœ¼ë¡œ ë³€í™˜\n",
    "# replace() í•¨ìˆ˜: ì²« ë²ˆì§¸ ì¸ìë¥¼ ë‘ ë²ˆì§¸ ì¸ìë¡œ ëŒ€ì²´\n",
    "# inplace=True: ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì„ ì§ì ‘ ìˆ˜ì • (ìƒˆë¡œìš´ ê°ì²´ ìƒì„± ì•ˆ í•¨)\n",
    "# pd.NA: pandas 2.0+ì—ì„œ ê¶Œì¥í•˜ëŠ” ê²°ì¸¡ê°’ í‘œí˜„\n",
    "df['full_text'].replace('', pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-7. ê²°ì¸¡ê°’(NaN) ì œê±°\n",
    "ê°ì •ë¶„ì„ì´ ë¶ˆê°€ëŠ¥í•œ ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ê²°ì¸¡ê°’ì„ ê°€ì§„ í–‰ë“¤ì„ ë°ì´í„°ì…‹ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ê°’ì„ ê°€ì§„ í–‰ë“¤ì„ ì œê±°\n",
    "# subset=['full_text']: full_text ì»¬ëŸ¼ì—ì„œ NaN ê°’ì„ ê°€ì§„ í–‰ë“¤ë§Œ ì œê±°\n",
    "# inplace=True: ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì„ ì§ì ‘ ìˆ˜ì •\n",
    "# ê°ì •ë¶„ì„ì„ ìœ„í•´ì„œëŠ” í…ìŠ¤íŠ¸ ë‚´ìš©ì´ í•„ìˆ˜ì´ë¯€ë¡œ ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì œê±° í•„ìš”\n",
    "df.dropna(subset=['full_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-8. ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ë°ì´í„° ì •ë ¬\n",
    "íŠ¸ìœ„í„° API í˜•ì‹ì˜ ë‚ ì§œë¥¼ í‘œì¤€ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì¤‘...\n",
      "ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ì •ë ¬ ì™„ë£Œ!\n",
      "ë‚ ì§œ ë²”ìœ„: 2020-01-17 01:05:39 ~ 2025-06-16 04:36:37\n",
      "\n",
      "ê²°ê³¼ í™•ì¸:\n",
      "           created_at                                          full_text  \\\n",
      "0 2025-06-16 04:36:37  1. Iran produces around 3.3mn barrels per day ...   \n",
      "1 2025-06-16 03:40:49             Today at Apple. #F1TheMovie #Severance   \n",
      "2 2025-06-16 03:04:38  Two countries, separated by 700 kms from each ...   \n",
      "3 2025-06-16 03:01:08  BREAKING: Iranian opposition Telegram channels...   \n",
      "4 2025-06-16 02:50:54  26 now.   Note the swing due east at the edge ...   \n",
      "\n",
      "      username  \n",
      "0  @Ajay_Bagga  \n",
      "1    @tim_cook  \n",
      "2  @Ajay_Bagga  \n",
      "3  @BillAckman  \n",
      "4  @BillAckman  \n"
     ]
    }
   ],
   "source": [
    "def convert_date_format(date_str):\n",
    "\n",
    "    try:\n",
    "        # íŠ¸ìœ„í„° API ë‚ ì§œ í˜•ì‹: \"Mon Jun 16 02:50:54 +0000 2025\"\n",
    "        # pd.to_datetimeìœ¼ë¡œ íŒŒì‹±í•˜ê³  strftimeìœ¼ë¡œ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "        # format ë§¤ê°œë³€ìˆ˜ ì„¤ëª…:\n",
    "        # %a: ì¶•ì•½ëœ ìš”ì¼ëª… (Mon, Tue, ...)\n",
    "        # %b: ì¶•ì•½ëœ ì›”ëª… (Jan, Feb, ...)  \n",
    "        # %d: ì¼ (01-31)\n",
    "        # %H:%M:%S: ì‹œ:ë¶„:ì´ˆ\n",
    "        # %z: íƒ€ì„ì¡´ ì˜¤í”„ì…‹ (+0000)\n",
    "        # %Y: 4ìë¦¬ ì—°ë„\n",
    "        dt = pd.to_datetime(date_str, format='%a %b %d %H:%M:%S %z %Y')\n",
    "        \n",
    "        # íƒ€ì„ì¡´ ì œê±°í•˜ê³  \"YYYY-MM-DD HH:MM:SS\" í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "        return date_str\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
    "print(\"ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì¤‘...\")\n",
    "\n",
    "# ëª¨ë“  íŠ¸ìœ—ì˜ created_at ì»¬ëŸ¼ì— ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜ ì ìš©\n",
    "df['created_at'] = df['created_at'].apply(convert_date_format)\n",
    "\n",
    "# ë¬¸ìì—´ì„ pandas datetime ê°ì²´ë¡œ ë³€í™˜ (ì •ë ¬ ë° ì‹œê°„ ì—°ì‚°ì„ ìœ„í•´)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# ìµœì‹  íŠ¸ìœ—ì´ ë§¨ ìœ„ì— ì˜¤ë„ë¡ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "# ascending=False: ë‚´ë¦¼ì°¨ìˆœ (í° ê°’ë¶€í„° ì‘ì€ ê°’ ìˆœ)\n",
    "# reset_index(drop=True): ì •ë ¬ í›„ ì¸ë±ìŠ¤ë¥¼ 0ë¶€í„° ë‹¤ì‹œ ì‹œì‘\n",
    "df = df.sort_values(by='created_at', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ì •ë ¬ ì™„ë£Œ!\")\n",
    "print(f\"ë‚ ì§œ ë²”ìœ„: {df['created_at'].min()} ~ {df['created_at'].max()}\")\n",
    "print(\"\\nê²°ê³¼ í™•ì¸:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-9. VADER ê°ì •ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "VADER(Valence Aware Dictionary and sEntiment Reasoner) ê°ì •ë¶„ì„ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER ê°ì •ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "# SentimentIntensityAnalyzer: VADER ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•œ í´ë˜ìŠ¤\n",
    "# - ì†Œì…œ ë¯¸ë””ì–´ í…ìŠ¤íŠ¸ì— íŠ¹í™”ëœ ê°ì •ë¶„ì„ ë„êµ¬\n",
    "# - ì´ëª¨í‹°ì½˜, ëŒ€ë¬¸ì, êµ¬ë‘ì , ë‹¨ì–´ ì¡°í•© ë“±ì„ ê³ ë ¤í•˜ì—¬ ê°ì • ì ìˆ˜ ê³„ì‚°\n",
    "# - positive, negative, neutral, compound ì ìˆ˜ë¥¼ ë°˜í™˜\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-10. ê°ì •ë¶„ì„ í•¨ìˆ˜ ì •ì˜\n",
    "í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ positive/negative/neutralë¡œ ë¶„ë¥˜í•˜ê³  ê° ê°ì • ì ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    # VADER ê°ì •ë¶„ì„ê¸°ë¡œ í…ìŠ¤íŠ¸ ë¶„ì„\n",
    "    # polarity_scores() ë°˜í™˜ê°’:\n",
    "    # - 'neg': negative ê°ì • ì ìˆ˜ (0~1)\n",
    "    # - 'neu': neutral ê°ì • ì ìˆ˜ (0~1)  \n",
    "    # - 'pos': positive ê°ì • ì ìˆ˜ (0~1)\n",
    "    # - 'compound': ë³µí•© ì ìˆ˜ (-1~1, ì „ì²´ì ì¸ ê°ì • ê°•ë„)\n",
    "    scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # compound ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°ì • ë¶„ë¥˜\n",
    "    compound = scores['compound']\n",
    "    \n",
    "    # VADER ê¶Œì¥ ì„ê³„ê°’ì„ ì‚¬ìš©í•œ ê°ì • ë¶„ë¥˜\n",
    "    if compound >= 0.05:\n",
    "        # compound >= 0.05: ê¸ì •ì  ê°ì •\n",
    "        sentiment = 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        # compound <= -0.05: ë¶€ì •ì  ê°ì •\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        # -0.05 < compound < 0.05: ì¤‘ë¦½ì  ê°ì •\n",
    "        sentiment = 'neutral'\n",
    "    \n",
    "    # pandas Seriesë¡œ ë°˜í™˜ (DataFrameì˜ ìƒˆ ì»¬ëŸ¼ë“¤ë¡œ í• ë‹¹í•˜ê¸° ìœ„í•¨)\n",
    "    return pd.Series([sentiment, scores['neg'], scores['neu'], scores['pos']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-11. ëª¨ë“  íŠ¸ìœ—ì— ê°ì •ë¶„ì„ ì‹¤í–‰\n",
    "ì •ì˜í•œ ê°ì •ë¶„ì„ í•¨ìˆ˜ë¥¼ ëª¨ë“  íŠ¸ìœ— í…ìŠ¤íŠ¸ì— ì ìš©í•˜ì—¬ ê°ì • ì»¬ëŸ¼ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasì˜ apply() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  íŠ¸ìœ—ì— ê°ì •ë¶„ì„ í•¨ìˆ˜ ì ìš©\n",
    "# analyze_sentiment() í•¨ìˆ˜ê°€ pd.Seriesë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ \n",
    "# ì—¬ëŸ¬ ì»¬ëŸ¼ì— ë™ì‹œì— í• ë‹¹ ê°€ëŠ¥\n",
    "# - sentiment: ê°ì • ë¶„ë¥˜ ë¼ë²¨ ('positive', 'negative', 'neutral')\n",
    "# - neg: ë¶€ì • ê°ì • ì ìˆ˜ (0~1)\n",
    "# - neu: ì¤‘ë¦½ ê°ì • ì ìˆ˜ (0~1)\n",
    "# - pos: ê¸ì • ê°ì • ì ìˆ˜ (0~1)\n",
    "df[['sentiment', 'neg', 'neu', 'pos']] = df['full_text'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-12. ìµœì¢… ë°ì´í„° ì»¬ëŸ¼ ì„ íƒ\n",
    "ë¶„ì„ì— í•„ìš”í•œ í•µì‹¬ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒí•˜ì—¬ ìµœì¢… ë°ì´í„°ì…‹ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ë¶„ì„ ê²°ê³¼ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ\n",
    "# - created_at: íŠ¸ìœ— ì‘ì„± ë‚ ì§œ/ì‹œê°„\n",
    "# - full_text: ì „ì²˜ë¦¬ëœ íŠ¸ìœ— í…ìŠ¤íŠ¸ (URL ì œê±°ë¨)\n",
    "# - username: íŠ¸ìœ— ì‘ì„±ì (@ì‚¬ìš©ìëª…)\n",
    "# - sentiment: ê°ì • ë¶„ë¥˜ ê²°ê³¼ (positive/negative/neutral)\n",
    "# - neg: ë¶€ì • ê°ì • ì ìˆ˜\n",
    "# - neu: ì¤‘ë¦½ ê°ì • ì ìˆ˜  \n",
    "# - pos: ê¸ì • ê°ì • ì ìˆ˜\n",
    "df = df[['created_at', 'full_text', 'username', 'sentiment', 'neg', 'neu', 'pos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-13. ìµœì¢… ê²°ê³¼ ì €ì¥ ë° ìš”ì•½\n",
    "ê°ì •ë¶„ì„ì´ ì™„ë£Œëœ íŠ¸ìœ— ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²°ê³¼ê°€ merged_tweets_with_sentiment.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ì´ 8594ê°œì˜ íŠ¸ìœ—ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "í¬í•¨ëœ ì‚¬ìš©ì: ['@Ajay_Bagga', '@BillAckman', '@CathieDWood', '@JDVance', '@LizAnnSonders', '@RayDalio', '@SecScottBessent', '@WhiteHouse', '@elonmusk', '@marcorubio', '@michaelbatnick', '@sundarpichai', '@tim_cook']\n",
      "ë‚ ì§œ ë²”ìœ„: 2020-01-17 01:05:39 ~ 2025-06-16 04:36:37\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ íŒŒì¼ ì €ì¥\n",
    "output_filename = \"merged_tweets_with_sentiment.csv\"\n",
    "\n",
    "# ê°ì •ë¶„ì„ì´ ì™„ë£Œëœ ë°ì´í„°í”„ë ˆì„ì„ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "# index=False: í–‰ ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì— í¬í•¨í•˜ì§€ ì•ŠìŒ\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "# ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì •ë³´ ì¶œë ¥\n",
    "print(f\"âœ… ê²°ê³¼ê°€ {output_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"ğŸ“Š ì´ {len(df)}ê°œì˜ íŠ¸ìœ—ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ê°ì •ë¶„ì„ì— í¬í•¨ëœ ì‚¬ìš©ì ëª©ë¡ (ì•ŒíŒŒë²³ ìˆœ ì •ë ¬)\n",
    "print(f\"ğŸ‘¥ í¬í•¨ëœ ì‚¬ìš©ì: {sorted(df['username'].unique())}\")\n",
    "\n",
    "# íŠ¸ìœ— ë°ì´í„°ì˜ ì‹œê°„ ë²”ìœ„\n",
    "print(f\"ğŸ“… ë‚ ì§œ ë²”ìœ„: {df['created_at'].min()} ~ {df['created_at'].max()}\")\n",
    "\n",
    "# ê°ì • ë¶„ë¥˜ë³„ íŠ¸ìœ— ìˆ˜ í†µê³„\n",
    "print(f\"\\nğŸ“ˆ ê°ì • ë¶„ë¥˜ë³„ í†µê³„:\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  - {sentiment}: {count}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "# ì‚¬ìš©ìë³„ íŠ¸ìœ— ìˆ˜ í†µê³„ (ìƒìœ„ 5ëª…)\n",
    "print(f\"\\nğŸ† ì‚¬ìš©ìë³„ íŠ¸ìœ— ìˆ˜ (ìƒìœ„ 5ëª…):\")\n",
    "user_counts = df['username'].value_counts().head(5)\n",
    "for username, count in user_counts.items():\n",
    "    print(f\"  - {username}: {count}ê°œ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
