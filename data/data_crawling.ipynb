{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Finnhub API ê¸°ë°˜ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import\n",
    "í•„ìš”í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ì›¹ API í˜¸ì¶œ, ë°ì´í„° ì²˜ë¦¬, ë‚ ì§œ ì²˜ë¦¬, í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import requests           # HTTP ìš”ì²­ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Finnhub API í˜¸ì¶œìš©)\n",
    "import pandas as pd       # ë°ì´í„° ë¶„ì„ ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (DataFrame ì‚¬ìš©)\n",
    "import datetime           # ë‚ ì§œ ë° ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import time               # ì‹œê°„ ì§€ì—° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (API í˜¸ì¶œ ì œí•œ ê´€ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. í™˜ê²½ ì„¤ì • ë¡œë“œ\n",
    "Finnhub API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINHUB_API_KEY = (\"d16f6mpr01qvtdbi7280d16f6mpr01qvtdbi728g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. API í˜¸ì¶œ ì œí•œ ì„¤ì •\n",
    "Finnhub APIëŠ” ë¶„ë‹¹ í˜¸ì¶œ íšŸìˆ˜ì— ì œí•œì´ ìˆìŠµë‹ˆë‹¤. API í˜¸ì¶œ ê°„ê²©ì„ ì¡°ì ˆí•˜ì—¬ ì œí•œì„ ìœ„ë°˜í•˜ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API í˜¸ì¶œ ì œí•œ ì„¤ì •\n",
    "# Finnhub Free TierëŠ” ë¶„ë‹¹ 60íšŒ í˜¸ì¶œ ì œí•œì´ ìˆìŒ\n",
    "API_CALLS_PER_MINUTE = 60\n",
    "\n",
    "# ê° API í˜¸ì¶œ ê°„ì˜ ìµœì†Œ ëŒ€ê¸° ì‹œê°„ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)\n",
    "# 60ì´ˆ / 60íšŒ = 1ì´ˆ ê°„ê²©ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ ì œí•œ ë‚´ì—ì„œ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ\n",
    "DELAY_BETWEEN_CALLS = 60.0 / API_CALLS_PER_MINUTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. ì•ˆì „í•œ datetime ë³€í™˜ í•¨ìˆ˜\n",
    "Finnhub APIì—ì„œ ë°›ì€ timestampë¥¼ pandas datetimeìœ¼ë¡œ ë³€í™˜í•  ë•Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•ˆì „í•œ datetime ë³€í™˜ í•¨ìˆ˜\n",
    "def safe_datetime_conversion(timestamp):\n",
    "\n",
    "    # ë¹ˆ ê°’ì´ë‚˜ 0ê°’ ì²´í¬\n",
    "    if not timestamp or timestamp == 0:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # ìœ ë‹‰ìŠ¤ íƒ€ì„ìŠ¤íƒ¬í”„ ë²”ìœ„ í™•ì¸ (1970-01-01 ì´í›„ë§Œ í—ˆìš©)\n",
    "        # ìŒìˆ˜ê°’ì€ 1970ë…„ ì´ì „ì„ ì˜ë¯¸í•˜ë¯€ë¡œ ì œì™¸\n",
    "        if timestamp < 0:\n",
    "            return None\n",
    "            \n",
    "        # pandasì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìµœëŒ€ timestamp í™•ì¸\n",
    "        # 2262-04-11 ì´í›„ëŠ” pandasì—ì„œ overflow ë°œìƒ\n",
    "        if timestamp > 9223372036:  # ì•½ 2262-04-11 23:47:16\n",
    "            return None\n",
    "            \n",
    "        # Unix timestampë¥¼ pandas datetimeìœ¼ë¡œ ë³€í™˜\n",
    "        # unit='s'ëŠ” ì´ˆ ë‹¨ìœ„ì„ì„ ì˜ë¯¸\n",
    "        return pd.to_datetime(timestamp, unit='s')\n",
    "        \n",
    "    except (ValueError, OutOfBoundsDatetime, OverflowError):\n",
    "        # pandasì—ì„œ ë°œìƒí•˜ëŠ” ë‚ ì§œ ë²”ìœ„ ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "        return None\n",
    "    except Exception:\n",
    "        # ê¸°íƒ€ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5. Finnhub ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ (ë©”ì¸ í•¨ìˆ˜)\n",
    "íŠ¹ì • ê¸°ì—…ì˜ ë‰´ìŠ¤ë¥¼ ëŒ€ëŸ‰ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ì…ë‹ˆë‹¤. ë‚ ì§œ êµ¬ê°„ì„ ë‚˜ëˆ„ì–´ ì—¬ëŸ¬ ë²ˆ API í˜¸ì¶œí•˜ì—¬ ìµœëŒ€í•œ ë§ì€ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finnhub ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ (ëŒ€ëŸ‰ ìˆ˜ì§‘ ìµœì í™”)\n",
    "def fetch_finnhub_news_extended(symbol: str = \"GOOGL\", start_date: str = \"2025-06-14\", days_per_request: int = 30) -> pd.DataFrame:\n",
    "\n",
    "    # API í‚¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ DataFrame ë°˜í™˜\n",
    "    if not FINHUB_API_KEY:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Finnhub Company News API ì—”ë“œí¬ì¸íŠ¸\n",
    "    url = \"https://finnhub.io/api/v1/company-news\"\n",
    "    \n",
    "    try:\n",
    "        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì •\n",
    "        today = datetime.date.today()\n",
    "        \n",
    "        # 3ë…„ ì „ë¶€í„° í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ ì‹œë„ (Free Tier ì œí•œ í…ŒìŠ¤íŠ¸)\n",
    "        three_years_ago = today - datetime.timedelta(days=1095)  # 1095ì¼ = ì•½ 3ë…„\n",
    "        actual_start = three_years_ago  # ì‹¤ì œ ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ\n",
    "        actual_end = today             # ì‹¤ì œ ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ\n",
    "        \n",
    "    except ValueError:\n",
    "        # ë‚ ì§œ ì²˜ë¦¬ ì˜¤ë¥˜ì‹œ ë¹ˆ DataFrame ë°˜í™˜\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # ëª¨ë“  ìˆ˜ì§‘ëœ ê¸°ì‚¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    all_articles = []\n",
    "    # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë‚ ì§œ\n",
    "    current_date = actual_start\n",
    "    # API í˜¸ì¶œ íšŸìˆ˜ ì¹´ìš´í„°\n",
    "    request_count = 0\n",
    "    \n",
    "    # ì‹œì‘ ë‚ ì§œë¶€í„° ì¢…ë£Œ ë‚ ì§œê¹Œì§€ ë°˜ë³µ ì²˜ë¦¬\n",
    "    while current_date < actual_end:\n",
    "        # ê° ìš”ì²­ì˜ ì¢…ë£Œ ë‚ ì§œ ê³„ì‚° (ì§€ì •ëœ ì¼ìˆ˜ë§Œí¼ ë˜ëŠ” ì „ì²´ ì¢…ë£Œ ë‚ ì§œê¹Œì§€)\n",
    "        period_end = min(current_date + datetime.timedelta(days=days_per_request), actual_end)\n",
    "        \n",
    "        # API ìš”ì²­ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "        params = {\n",
    "            \"symbol\": symbol,                           # ì£¼ì‹ ì‹¬ë³¼\n",
    "            \"from\": current_date.isoformat(),           # ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)\n",
    "            \"to\": period_end.isoformat(),               # ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)\n",
    "            \"token\": FINHUB_API_KEY                     # API ì¸ì¦ í† í°\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            request_count += 1  # API í˜¸ì¶œ íšŸìˆ˜ ì¦ê°€\n",
    "            \n",
    "            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ë”œë ˆì´ (ë¶„ë‹¹ 60íšŒ ì œí•œ ì¤€ìˆ˜)\n",
    "            time.sleep(DELAY_BETWEEN_CALLS)\n",
    "            \n",
    "            # HTTP GET ìš”ì²­ìœ¼ë¡œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "            res = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            # HTTP 429 (Too Many Requests) ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "            if res.status_code == 429:\n",
    "                time.sleep(10)  # 10ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„\n",
    "                res = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            # HTTP 403 (Forbidden) ì˜¤ë¥˜ì‹œ ìˆ˜ì§‘ ì¤‘ë‹¨ (API ì œí•œ ë„ë‹¬)\n",
    "            if res.status_code == 403:\n",
    "                break\n",
    "            \n",
    "            # ê¸°íƒ€ HTTP ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "            if res.status_code != 200:\n",
    "                if res.status_code != 429:\n",
    "                    # 429ê°€ ì•„ë‹Œ ì˜¤ë¥˜ëŠ” ë‹¤ìŒ ê¸°ê°„ìœ¼ë¡œ ê±´ë„ˆë›°ê¸°\n",
    "                    current_date = period_end + datetime.timedelta(days=1)\n",
    "                    continue\n",
    "                else:\n",
    "                    # 429 ì˜¤ë¥˜ëŠ” 30ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„\n",
    "                    time.sleep(30)\n",
    "                    continue\n",
    "\n",
    "            # JSON ì‘ë‹µ ë°ì´í„° íŒŒì‹±\n",
    "            data = res.json()\n",
    "            \n",
    "            # ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° (ì˜¤ë¥˜ ì‘ë‹µ ë“±) ì²˜ë¦¬\n",
    "            if not isinstance(data, list):\n",
    "                if isinstance(data, dict) and 'error' in data:\n",
    "                    # API ì œí•œ ê´€ë ¨ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸\n",
    "                    if 'limit' in data['error'].lower():\n",
    "                        break  # ì œí•œ ë„ë‹¬ì‹œ ìˆ˜ì§‘ ì¤‘ë‹¨\n",
    "                # ë‹¤ìŒ ê¸°ê°„ìœ¼ë¡œ ì´ë™\n",
    "                current_date = period_end + datetime.timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            # í•´ë‹¹ ê¸°ê°„ì˜ ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬\n",
    "            period_articles = []\n",
    "            for item in data:\n",
    "                # APIì—ì„œ ë°›ì€ timestampë¥¼ ì•ˆì „í•˜ê²Œ datetimeìœ¼ë¡œ ë³€í™˜\n",
    "                pub_date = safe_datetime_conversion(item.get(\"datetime\"))\n",
    "                \n",
    "                # ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬ì„±\n",
    "                article = {\n",
    "                    \"id\": item.get(\"id\"),                           # ê¸°ì‚¬ ê³ ìœ  ID\n",
    "                    \"title\": item.get(\"headline\", \"\"),             # ê¸°ì‚¬ ì œëª©\n",
    "                    \"summary\": item.get(\"summary\", \"\"),            # ê¸°ì‚¬ ìš”ì•½\n",
    "                    \"link\": item.get(\"url\", \"\"),                   # ê¸°ì‚¬ URL\n",
    "                    \"publisher\": item.get(\"publisher\", \"\"),        # ë°œí–‰ì‚¬\n",
    "                    \"category\": item.get(\"category\", \"\"),          # ì¹´í…Œê³ ë¦¬\n",
    "                    \"pubDate\": pub_date,                           # ë°œí–‰ ë‚ ì§œ\n",
    "                    \"image\": item.get(\"image\", \"\"),                # ì´ë¯¸ì§€ URL\n",
    "                    \"related\": item.get(\"related\", \"\"),            # ê´€ë ¨ ì •ë³´\n",
    "                    \"source\": item.get(\"source\", \"\"),              # ì†ŒìŠ¤\n",
    "                    \"collection_period\": f\"{current_date.isoformat()}_{period_end.isoformat()}\"  # ìˆ˜ì§‘ ê¸°ê°„ ì •ë³´\n",
    "                }\n",
    "                period_articles.append(article)\n",
    "            \n",
    "            # í˜„ì¬ ê¸°ê°„ì˜ ê¸°ì‚¬ë“¤ì„ ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "            all_articles.extend(period_articles)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # ì˜ˆì™¸ ë°œìƒì‹œ API ì œí•œ ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸\n",
    "            if \"limit\" in str(e).lower() or \"403\" in str(e):\n",
    "                break  # API ì œí•œ ê´€ë ¨ ì˜¤ë¥˜ì‹œ ìˆ˜ì§‘ ì¤‘ë‹¨\n",
    "        \n",
    "        # ë‹¤ìŒ ê¸°ê°„ìœ¼ë¡œ ì´ë™ (í•˜ë£¨ ê°„ê²©ìœ¼ë¡œ ê²¹ì¹˜ì§€ ì•Šê²Œ)\n",
    "        current_date = period_end + datetime.timedelta(days=1)\n",
    "        \n",
    "        # API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ì¶”ê°€ ëŒ€ê¸°\n",
    "        # 10íšŒ í˜¸ì¶œë§ˆë‹¤ 2ì´ˆ ì¶”ê°€ ëŒ€ê¸°ë¡œ ì•ˆì „ì„± í™•ë³´\n",
    "        if request_count % 10 == 0:\n",
    "            time.sleep(2)\n",
    "\n",
    "    # ìˆ˜ì§‘ëœ ëª¨ë“  ê¸°ì‚¬ ë°ì´í„° ì²˜ë¦¬\n",
    "    if all_articles:\n",
    "        # ë¦¬ìŠ¤íŠ¸ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "        df = pd.DataFrame(all_articles)\n",
    "        \n",
    "        # ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ID, ì œëª©, ë§í¬ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ íŒë³„)\n",
    "        df = df.drop_duplicates(subset=['id', 'title', 'link'])\n",
    "        \n",
    "        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ê¸°ì‚¬ê°€ ìœ„ì— ì˜¤ë„ë¡)\n",
    "        # na_position='last'ë¡œ ë‚ ì§œê°€ ì—†ëŠ” ê¸°ì‚¬ëŠ” ë§¨ ì•„ë˜ë¡œ\n",
    "        df = df.sort_values('pubDate', ascending=False, na_position='last')\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ë¹ˆ DataFrame ë°˜í™˜\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6. ìˆ˜ì§‘ ì„¤ì •ê°’ ì •ì˜ - í•˜ë“œì½”ë”© í•„ìš”\n",
    "ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ìœ„í•œ ê¸°ë³¸ ì„¤ì •ê°’ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤. í•„ìš”ì— ë”°ë¼ ì´ ê°’ë“¤ì„ ìˆ˜ì •í•˜ì—¬ ë‹¤ë¥¸ ê¸°ì—…ì´ë‚˜ ê¸°ê°„ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ìœ„í•œ ì„¤ì •ê°’ ì •ì˜\n",
    "# ìˆ˜ì§‘ ëŒ€ìƒ ê¸°ì—…ì˜ ì£¼ì‹ ì‹¬ë³¼ (ë¯¸êµ­ ìƒì¥ ê¸°ì—…)\n",
    "# ì˜ˆì‹œ: \"AAPL\"(Apple), \"GOOGL\"(Google), \"TSLA\"(Tesla), \"MSFT\"(Microsoft), \"AMZN\"(Amazon) ë“±\n",
    "target_symbol = \"AAPL\"\n",
    "\n",
    "# ì°¸ê³ ìš© ì‹œì‘ ë‚ ì§œ (ì‹¤ì œë¡œëŠ” 3ë…„ ì „ë¶€í„° ìë™ìœ¼ë¡œ ìˆ˜ì§‘ë¨)\n",
    "# YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì…ë ¥\n",
    "start_date = \"2025-06-14\"\n",
    "\n",
    "# í•œ ë²ˆì˜ API í˜¸ì¶œë‹¹ ìˆ˜ì§‘í•  ì¼ìˆ˜ ì„¤ì •\n",
    "# ê°’ì´ ì‘ì„ìˆ˜ë¡ ë” ë§ì€ API í˜¸ì¶œì„ í•˜ê²Œ ë˜ì–´ ë” ë§ì€ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŒ\n",
    "# í•˜ì§€ë§Œ API ì œí•œì— ë„ë‹¬í•  ê°€ëŠ¥ì„±ë„ ë†’ì•„ì§\n",
    "days_per_request = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-7. ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰\n",
    "ìœ„ì—ì„œ ì •ì˜í•œ ì„¤ì •ê°’ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œë¡œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ API í˜¸ì¶œì´ ì—¬ëŸ¬ ë²ˆ ë°œìƒí•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰\n",
    "# ìœ„ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ì™€ ì„¤ì •ê°’ì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘\n",
    "# ì´ ê³¼ì •ì€ API í˜¸ì¶œ ì œí•œìœ¼ë¡œ ì¸í•´ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ (ëª‡ ë¶„ì—ì„œ ëª‡ì‹­ ë¶„)\n",
    "df_extended_news = fetch_finnhub_news_extended(\n",
    "    symbol=target_symbol,           # ìˆ˜ì§‘í•  ê¸°ì—…ì˜ ì£¼ì‹ ì‹¬ë³¼\n",
    "    start_date=start_date,          # ì°¸ê³ ìš© ì‹œì‘ ë‚ ì§œ\n",
    "    days_per_request=days_per_request  # í•œ ë²ˆì— ìˆ˜ì§‘í•  ì¼ìˆ˜\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-8. ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸ ë° ê¸°ë³¸ ë¶„ì„\n",
    "ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ì˜ ê¸°ë³¸ì ì¸ í†µê³„ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì´ ê¸°ì‚¬ ìˆ˜, ìˆ˜ì§‘ ê¸°ê°„, ìœ íš¨í•œ ë‚ ì§œ ì •ë³´ ë“±ì„ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸ ë° ê¸°ë³¸ í†µê³„ ë¶„ì„\n",
    "# DataFrameì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸ (ìˆ˜ì§‘ì— ì„±ê³µí–ˆëŠ”ì§€ ì²´í¬)\n",
    "if not df_extended_news.empty:\n",
    "    # ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜ ì¶œë ¥\n",
    "    print(f\"ì´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {len(df_extended_news)}ê°œ\")\n",
    "    \n",
    "    # ë‚ ì§œë³„ ê¸°ì‚¬ ë¶„í¬ ë¶„ì„\n",
    "    if 'pubDate' in df_extended_news.columns:\n",
    "        # ìœ íš¨í•œ ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ê¸°ì‚¬ë“¤ë§Œ í•„í„°ë§\n",
    "        valid_dates = df_extended_news[df_extended_news['pubDate'].notna()].copy()\n",
    "        \n",
    "        if not valid_dates.empty:\n",
    "            # ë‚ ì§œë§Œ ì¶”ì¶œ (ì‹œê°„ ì •ë³´ ì œê±°)\n",
    "            valid_dates['date_only'] = valid_dates['pubDate'].dt.date\n",
    "            \n",
    "            # ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜ ì§‘ê³„ ë° ì •ë ¬\n",
    "            date_counts = valid_dates['date_only'].value_counts().sort_index()\n",
    "            \n",
    "            # ìˆ˜ì§‘ ê¸°ê°„ (ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œ ~ ê°€ì¥ ìµœê·¼ ë‚ ì§œ) ì¶œë ¥\n",
    "            print(f\"ìˆ˜ì§‘ ê¸°ê°„: {date_counts.index.min()} ~ {date_counts.index.max()}\")\n",
    "            \n",
    "            # ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” ê¸°ì‚¬ ìˆ˜ì™€ ì „ì²´ ê¸°ì‚¬ ìˆ˜ ë¹„êµ\n",
    "            print(f\"ìœ íš¨í•œ ë‚ ì§œ ê¸°ì‚¬: {len(valid_dates)}ê°œ / ì „ì²´ {len(df_extended_news)}ê°œ\")\n",
    "else:\n",
    "    # ìˆ˜ì§‘ì— ì‹¤íŒ¨í•œ ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥\n",
    "    print(\"ë‰´ìŠ¤ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-9. ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ í‘œ í˜•íƒœë¡œ í™•ì¸í•©ë‹ˆë‹¤. ì œëª©, ë°œí–‰ì²˜, ë°œí–‰ ë‚ ì§œ ë“± ì£¼ìš” ì •ë³´ë¥¼ ë¯¸ë¦¬ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "# ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "if not df_extended_news.empty:\n",
    "    print(\"ìµœì‹  ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    \n",
    "    # ì£¼ìš” ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒí•˜ì—¬ ìƒìœ„ 5ê°œ ê¸°ì‚¬ í‘œì‹œ\n",
    "    # title: ê¸°ì‚¬ ì œëª©, summary: ë³¸ë¬¸ ë‚´ìš©, pubDate: ë°œí–‰ ë‚ ì§œ\n",
    "    preview_data = df_extended_news[['title', 'summary','pubDate']].head()\n",
    "    \n",
    "    # Jupyter Notebookì—ì„œ í‘œ í˜•íƒœë¡œ ê¹”ë”í•˜ê²Œ ì¶œë ¥\n",
    "    display(preview_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-10. CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. íŒŒì¼ëª…ì—ëŠ” ê¸°ì—… ì‹¬ë³¼ê³¼ í˜„ì¬ ë‚ ì§œê°€ í¬í•¨ë˜ì–´ êµ¬ë¶„í•˜ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "# ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥ ì§„í–‰\n",
    "if not df_extended_news.empty:\n",
    "    # í˜„ì¬ ë‚ ì§œë¥¼ ISO í˜•ì‹ (YYYY-MM-DD)ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "    # today = datetime.date.today().isoformat()\n",
    "    \n",
    "    # íŒŒì¼ëª… ìƒì„±: \"ê¸°ì—…ì‹¬ë³¼_extended_news_ë‚ ì§œ.csv\" í˜•ì‹\n",
    "    # ì˜ˆ: \"GOOGL_extended_news_2025-06-19.csv\"\n",
    "    filename = f\"{target_symbol}_extended_news_{start_date}.csv\"\n",
    "    \n",
    "    # DataFrameì„ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    # index=False: í–‰ ë²ˆí˜¸ë¥¼ íŒŒì¼ì— í¬í•¨í•˜ì§€ ì•ŠìŒ\n",
    "    # encoding='utf-8-sig': í•œê¸€ ë“± ìœ ë‹ˆì½”ë“œ ë¬¸ìê°€ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì„¤ì •\n",
    "    df_extended_news.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥\n",
    "    print(f\"íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "else:\n",
    "    # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥\n",
    "    print(\"ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finnhub API ê¸°ë°˜ ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ ë° FinBERT ê°ì •ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì˜ ì½ê¸° ê²½ë¡œì™€ ê°ì •ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì…ë ¥ íŒŒì¼ ê²½ë¡œ: Finnhubì—ì„œ ìˆ˜ì§‘í•œ Apple ë‰´ìŠ¤ ë°ì´í„° CSV íŒŒì¼\n",
    "read_path = \"../data/AAPL_extended_news_2025-06-14.csv\"\n",
    "\n",
    "# ì¶œë ¥ íŒŒì¼ ê²½ë¡œ: FinBERT ê°ì •ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ CSV íŒŒì¼\n",
    "write_path = \"../data/apple_finbert_finnhub.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. ë°ì´í„° ë¡œë“œ\n",
    "pandasë¥¼ ì‚¬ìš©í•˜ì—¬ Finnhubì—ì„œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import pandas as pd\n",
    "\n",
    "# CSV íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë¡œë“œ\n",
    "# íŒŒì¼ì—ëŠ” id, title, summary, link, publisher, pubDate ë“±ì˜ ì»¬ëŸ¼ì´ í¬í•¨ë¨\n",
    "df = pd.read_csv(read_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. FinBERT ëª¨ë¸ ì„¤ì • ë° ë°ì´í„° ì „ì²˜ë¦¬\n",
    "ê¸ˆìœµ ë„ë©”ì¸ì— íŠ¹í™”ëœ FinBERT ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , ë‰´ìŠ¤ í…ìŠ¤íŠ¸ë¥¼ ê°ì •ë¶„ì„ì— ì í•©í•˜ê²Œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FinBERT ì „ì²˜ë¦¬ë¥¼ ìµœì†Œí•œìœ¼ë¡œ ìœ ì§€í•œ ì´ìœ \n",
    "\n",
    "- ì‚¬ì „í›ˆë ¨ ë°ì´í„°: ìˆ˜ë§Œ ê°œì˜ ê¸ˆìœµ ë‰´ìŠ¤, ë³´ê³ ì„œ, ì†Œì…œë¯¸ë””ì–´ í…ìŠ¤íŠ¸ë¡œ í›ˆë ¨\n",
    "- ë„ë©”ì¸ íŠ¹í™” ì–´íœ˜: ê¸ˆìœµ ìš©ì–´, í‘œí˜„, ë§¥ë½ì„ ì´ë¯¸ í•™ìŠµ\n",
    "- ë…¸ì´ì¦ˆ ì²˜ë¦¬ ëŠ¥ë ¥: ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆì— ì´ë¯¸ ê°•ê±´í•¨\n",
    "- ì»¨í…ìŠ¤íŠ¸ ì´í•´: ë¬¸ë§¥ì„ í†µí•œ ì˜ë¯¸ íŒŒì•…\n",
    "- ì „ì´í•™ìŠµ íš¨ê³¼: ëŒ€ê·œëª¨ ì‚¬ì „í›ˆë ¨ìœ¼ë¡œ ì¼ë°˜í™” ëŠ¥ë ¥ í™•ë³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì •ë¶„ì„ì„ ìœ„í•œ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# FinBERT ëª¨ë¸ ì„¤ì • - ê¸ˆìœµ ë‰´ìŠ¤ ê°ì •ë¶„ì„ì— íŠ¹í™”ëœ ì‚¬ì „í›ˆë ¨ ëª¨ë¸\n",
    "MODEL = \"yiyanghkust/finbert-tone\"  # Hugging Faceì˜ FinBERT-tone ëª¨ë¸\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ - í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# ì‚¬ì „í›ˆë ¨ëœ FinBERT ëª¨ë¸ ë¡œë“œ - ê¸ˆìœµ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# ê°ì •ë¶„ì„ íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "# return_all_scores=True: positive, neutral, negative ëª¨ë“  ì ìˆ˜ë¥¼ ë°˜í™˜\n",
    "finbert = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "# ì´ì „ì— ë¡œë“œí•œ ë‰´ìŠ¤ ë°ì´í„° ì‚¬ìš©\n",
    "df = pd.read_csv(read_path)\n",
    "\n",
    "# ë¹ˆ ê°’(NaN)ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´í•˜ì—¬ ì²˜ë¦¬ ì˜¤ë¥˜ ë°©ì§€\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ìƒì„± - ë‰´ìŠ¤ ì œëª©ì„ ê°ì •ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •\n",
    "df[\"text\"] = df[\"title\"] + df[\"summary\"]\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ BERT ëª¨ë¸ì˜ ìµœëŒ€ í† í° ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³ , \n",
    "# ì›ë³¸ì´ ê¸¸ì´ ì œí•œì„ ì´ˆê³¼í–ˆëŠ”ì§€ í”Œë˜ê·¸ë¡œ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜\n",
    "def truncate_and_flag(text, tokenizer, max_len=512):\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ë©´ì„œ max_lenì— ë§ê²Œ ìë¥´ê¸°\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_len)\n",
    "    \n",
    "    # ì˜ë¦° í† í°ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # ì›ë³¸ í…ìŠ¤íŠ¸ê°€ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í–ˆëŠ”ì§€ í™•ì¸\n",
    "    # 1: ì´ˆê³¼í•¨, 0: ì´ˆê³¼í•˜ì§€ ì•ŠìŒ\n",
    "    over_flag = 1 if len(tokenizer.encode(text)) > max_len else 0\n",
    "    \n",
    "    # ì˜ë¦°_í…ìŠ¤íŠ¸, ì´ˆê³¼_í”Œë˜ê·¸ ë°˜í™˜\n",
    "    return truncated_text, over_flag\n",
    "\n",
    "# ëª¨ë“  í…ìŠ¤íŠ¸ì— ëŒ€í•´ ê¸¸ì´ ì œí•œ ì ìš© ë° ì´ˆê³¼ í”Œë˜ê·¸ ìƒì„±\n",
    "# apply()ì™€ pd.Series()ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ ê²°ê³¼ë¥¼ ë‘ ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„í• \n",
    "df[[\"text\", \"over_512\"]] = df[\"text\"].apply(\n",
    "    lambda x: pd.Series(truncate_and_flag(x, tokenizer, max_len=512))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. FinBERT ê°ì •ë¶„ì„ ì‹¤í–‰\n",
    "ì „ì²˜ë¦¬ëœ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì— ëŒ€í•´ FinBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°ì •ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FinBERT ê°ì •ë¶„ì„ ì‹¤í–‰ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "# DataFrameì˜ text ì»¬ëŸ¼ì„ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "texts = df[\"text\"].tolist()\n",
    "\n",
    "# FinBERT ëª¨ë¸ë¡œ ë°°ì¹˜ ê°ì •ë¶„ì„ ìˆ˜í–‰\n",
    "# batch_size=8: í•œ ë²ˆì— 8ê°œì”© ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì†ë„ ìµœì í™”\n",
    "# GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ë” ì‘ì€ ê°’(ì˜ˆ: 4, 2)ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥\n",
    "results = finbert(texts, batch_size=8)\n",
    "\n",
    "# ê°ì •ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ë“¤ ì´ˆê¸°í™”\n",
    "pos_scores, neu_scores, neg_scores = [], [], []\n",
    "\n",
    "# ê° í…ìŠ¤íŠ¸ì˜ ê°ì •ë¶„ì„ ê²°ê³¼ë¥¼ ìˆœíšŒí•˜ë©° ì ìˆ˜ ì¶”ì¶œ\n",
    "for res in results:\n",
    "    # ê²°ê³¼ë¥¼ ë¼ë²¨:ì ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "    # ë¼ë²¨ì„ ì†Œë¬¸ìë¡œ í†µì¼í•˜ì—¬ ì¼ê´€ì„± í™•ë³´\n",
    "    d = {r[\"label\"].lower(): r[\"score\"] for r in res}\n",
    "    \n",
    "    # ê° ê°ì •ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ì—†ìœ¼ë©´ 0.0ìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •)\n",
    "    pos_scores.append(d.get(\"positive\", 0.0))   # ê¸ì • ê°ì • ì ìˆ˜\n",
    "    neu_scores.append(d.get(\"neutral\", 0.0))    # ì¤‘ë¦½ ê°ì • ì ìˆ˜  \n",
    "    neg_scores.append(d.get(\"negative\", 0.0))   # ë¶€ì • ê°ì • ì ìˆ˜\n",
    "\n",
    "# ê°ì •ë¶„ì„ ê²°ê³¼ë¥¼ DataFrameì— ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€\n",
    "df[\"finbert_positive\"] = pos_scores   # FinBERT ê¸ì • ì ìˆ˜ (0~1)\n",
    "df[\"finbert_neutral\"] = neu_scores    # FinBERT ì¤‘ë¦½ ì ìˆ˜ (0~1)\n",
    "df[\"finbert_negative\"] = neg_scores   # FinBERT ë¶€ì • ì ìˆ˜ (0~1)\n",
    "\n",
    "# ê°ì •ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ DataFrameì„ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "# index=False: í–‰ ë²ˆí˜¸ë¥¼ íŒŒì¼ì— í¬í•¨í•˜ì§€ ì•ŠìŒ\n",
    "df.to_csv(write_path, index=False)\n",
    "\n",
    "# ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥\n",
    "print(f\"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {write_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Rapid API ê¸°ë°˜ íŠ¸ìœ„í„°(X) ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import\n",
    "íŠ¸ìœ— ìˆ˜ì§‘ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP ìš”ì²­ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import requests\n",
    "\n",
    "# JSON ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import json\n",
    "\n",
    "# CSV íŒŒì¼ ì €ì¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import csv\n",
    "\n",
    "# íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os\n",
    "\n",
    "# ì§€ì—° ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import time\n",
    "\n",
    "# íƒ€ì… íŒíŒ…ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from typing import List, Dict, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. ì„¤ì •ê°’ ì •ì˜ - í•˜ë“œì½”ë”© í•„ìš”\n",
    "API í‚¤, ì‚¬ìš©ì ID, ìˆ˜ì§‘í•  íŠ¸ìœ— ìˆ˜ ë“± í¬ë¡¤ë§ì— í•„ìš”í•œ ì„¤ì •ê°’ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RapidAPIì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ (ë³´ì•ˆìƒ ì‹¤ì œ í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© ê¶Œì¥)\n",
    "API_KEY = \"5fac920861msh988e449f8d91b60p10459bjsnba691d3d2d81\"\n",
    "\n",
    "# ìˆ˜ì§‘í•  ì‚¬ìš©ìì˜ [ @ì´ë¦„ / ID(Twitter ì‚¬ìš©ì ê³ ìœ  ë²ˆí˜¸) / ì„¤ëª… ]\n",
    "'''\n",
    "@WhiteHouse / 1879644163769335808 / ë°±ì•…ê´€ = ë„ë„ë“œ íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹\n",
    "@SecScottBessent / 1889019333960998912 / ìŠ¤ì½§ë² ì„¼íŠ¸ ì¬ë¬´ì¥ê´€\n",
    "@JDVance / 1542228578 / ë°´ìŠ¤ ë¶€í†µë ¹\n",
    "@marcorubio / 15745368 / ë§ˆë¥´ì½” ë£¨ë¹„ì˜¤ êµ­ë¬´ì¥ê´€\n",
    "@elonmusk / 44196397 / ì¼ë¡  ë¨¸ìŠ¤í¬ í…ŒìŠ¬ë¼ CEO\n",
    "@sundarpichai / 14130366 / ìˆœë‹¤ë¥´ í”¼ì°¨ì´ êµ¬ê¸€ CEO\n",
    "@tim_cook / 1636590253 / íŒ€ ì¿¡ ì• í”Œ CEO\n",
    "@CathieDWood / 2361631088 / ARK Invest CEO, í˜ì‹  ì„±ì¥ì£¼ íˆ¬ì, ì‹œì¥ íŠ¸ë Œë“œ ì£¼ë„\n",
    "@BillAckman / 880412538625810432 / ë¹Œ ì•¡ë¨¼ í€ë“œë§¤ë‹ˆì €, í–‰ë™ì£¼ì˜ íˆ¬ì ì„±í–¥\n",
    "@RayDalio / 62603893 / ë¸Œë¦¬ì§€ì›Œí„° ì°½ë¦½ì, ê±°ì‹œê²½ì œ ë¶„ì„, íˆ¬ì ì „ëµê°€\n",
    "@michaelbatnick / 93529573 / íˆ¬ì ë¶„ì„, ê¸ˆìœµ ì¸ì‚¬ì´íŠ¸ ì œê³µ\n",
    "@LizAnnSonders / 2961589380 / ì°°ìŠ¤ìŠˆì™‘ ìˆ˜ì„ íˆ¬ìì „ëµê°€, ì‹œì¥ ì „ë§, íˆ¬ì ì „ëµ\n",
    "@Ajay_Bagga / 86437069 / ê¸€ë¡œë²Œ ë§¤í¬ë¡œ ì „ë¬¸ê°€, ì‹œì¥ ì „ë§, íˆ¬ì ì „ëµ\n",
    "'''\n",
    "USER_ID = \"86437069\"\n",
    "\n",
    "# ìˆ˜ì§‘í•  ìµœëŒ€ íŠ¸ìœ— ìˆ˜\n",
    "MAX_TWEETS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. RapidAPI Twitter í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì •ì˜\n",
    "íŠ¸ìœ— ìˆ˜ì§‘ ê¸°ëŠ¥ì„ ë‹´ì€ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ëª¨ë“  ë©”ì„œë“œê°€ í´ë˜ìŠ¤ ë‚´ë¶€ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RapidAPITweetCrawler:\n",
    "\n",
    "    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” í•¨ìˆ˜\n",
    "    def __init__(self, api_key: str):\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API í‚¤ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "        # RapidAPI ì¸ì¦ ì •ë³´ ì„¤ì •\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://twitter241.p.rapidapi.com/user-tweets\"\n",
    "        self.headers = {\n",
    "            \"x-rapidapi-key\": self.api_key,\n",
    "            \"x-rapidapi-host\": \"twitter241.p.rapidapi.com\"\n",
    "        }\n",
    "        \n",
    "        # API ìš”ì²­ë‹¹ íŠ¸ìœ— ìˆ˜ ì„¤ì • (ìµœëŒ€ 200ê¹Œì§€ ê°€ëŠ¥)\n",
    "        self.count_per_request = 200\n",
    "        \n",
    "        # cursor ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ìºì‹œ\n",
    "        self.used_cursors = set()\n",
    "\n",
    "    # API ì‘ë‹µ JSONì—ì„œ íŠ¸ìœ— ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜\n",
    "    def _parse_tweets_from_response(self, response_json: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        tweets_data = []\n",
    "        \n",
    "        try:\n",
    "            # Twitter APIì˜ timeline êµ¬ì¡°ì—ì„œ instructions ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "            instructions = response_json.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "            \n",
    "            # 'TimelineAddEntries' íƒ€ì…ì˜ instructionì—ì„œ íŠ¸ìœ— entriesë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "            timeline_entries = []\n",
    "            for instruction in instructions:\n",
    "                if instruction.get('type') == 'TimelineAddEntries':\n",
    "                    timeline_entries = instruction.get('entries', [])\n",
    "                    break\n",
    "            \n",
    "            if not timeline_entries:\n",
    "                return []\n",
    "\n",
    "            # ê° entryë¥¼ ìˆœíšŒí•˜ë©° íŠ¸ìœ— ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "            for entry in timeline_entries:\n",
    "                # 'TimelineTweet' íƒ€ì…ì˜ ì½˜í…ì¸ ë§Œ ì²˜ë¦¬\n",
    "                item_content = entry.get('content', {}).get('itemContent', {})\n",
    "                if item_content and item_content.get('itemType') == 'TimelineTweet':\n",
    "                    # íŠ¸ìœ— ê²°ê³¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "                    tweet_results = item_content.get('tweet_results', {})\n",
    "                    result = tweet_results.get('result', {})\n",
    "                    \n",
    "                    # legacy í•„ë“œì— ì‹¤ì œ íŠ¸ìœ— ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "                    legacy_data = result.get('legacy', {})\n",
    "                    \n",
    "                    if legacy_data:\n",
    "                        # íŠ¸ìœ— ìƒì„± ì‹œê°„ ì¶”ì¶œ\n",
    "                        created_at = legacy_data.get('created_at', 'N/A')\n",
    "                        full_text = \"\"\n",
    "                        \n",
    "                        # ë¦¬íŠ¸ìœ—(RT)ì¸ ê²½ìš° ì›ë³¸ íŠ¸ìœ—ì˜ full_textë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "                        if 'retweeted_status_result' in legacy_data:\n",
    "                            # ì›ë³¸ íŠ¸ìœ—ì˜ legacy ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "                            original_tweet_legacy = legacy_data.get('retweeted_status_result', {}).get('result', {}).get('legacy', {})\n",
    "                            full_text = original_tweet_legacy.get('full_text', '')\n",
    "                        else:\n",
    "                            # ì¼ë°˜ íŠ¸ìœ—ì€ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ full_textë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "                            full_text = legacy_data.get('full_text', '')\n",
    "\n",
    "                        # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì–‘ ë ê³µë°± ì œê±°\n",
    "                        full_text = full_text.replace('\\n', ' ').strip()\n",
    "                        \n",
    "                        # ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "                        tweets_data.append({\n",
    "                            'created_at': created_at,\n",
    "                            'full_text': full_text\n",
    "                        })\n",
    "        except (AttributeError, KeyError, IndexError):\n",
    "            # ë°ì´í„° íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "            pass\n",
    "            \n",
    "        return tweets_data\n",
    "\n",
    "    # API ì‘ë‹µì—ì„œ ë‹¤ìŒ í˜ì´ì§€ë¥¼ ìœ„í•œ cursor ê°’ì„ ì°¾ëŠ” í•¨ìˆ˜\n",
    "    def _find_next_cursor(self, response_json: Dict[str, Any]) -> Optional[str]:\n",
    "        try:\n",
    "            instructions = response_json.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "            \n",
    "            # ëª¨ë“  instruction íƒ€ì…ì—ì„œ cursor ì°¾ê¸°\n",
    "            all_cursors = []\n",
    "            \n",
    "            for instruction in instructions:\n",
    "                # TimelineAddEntriesì—ì„œ cursor ì°¾ê¸°\n",
    "                if instruction.get('type') == 'TimelineAddEntries':\n",
    "                    entries = instruction.get('entries', [])\n",
    "                    for entry in entries:\n",
    "                        content = entry.get('content', {})\n",
    "                        if content.get('entryType') == 'TimelineTimelineCursor':\n",
    "                            cursor_value = content.get('value')\n",
    "                            cursor_type = content.get('cursorType', '')\n",
    "                            \n",
    "                            if cursor_value and cursor_value not in self.used_cursors:\n",
    "                                all_cursors.append({\n",
    "                                    'value': cursor_value,\n",
    "                                    'type': cursor_type,\n",
    "                                    'priority': 1 if cursor_type == 'Bottom' else 2\n",
    "                                })\n",
    "                \n",
    "                # TimelineReplaceEntryì—ì„œë„ cursor ì°¾ê¸°\n",
    "                elif instruction.get('type') == 'TimelineReplaceEntry':\n",
    "                    entry = instruction.get('entry', {})\n",
    "                    content = entry.get('content', {})\n",
    "                    if content.get('entryType') == 'TimelineTimelineCursor':\n",
    "                        cursor_value = content.get('value')\n",
    "                        cursor_type = content.get('cursorType', '')\n",
    "                        \n",
    "                        if cursor_value and cursor_value not in self.used_cursors:\n",
    "                            all_cursors.append({\n",
    "                                'value': cursor_value,\n",
    "                                'type': cursor_type,\n",
    "                                'priority': 1 if cursor_type == 'Bottom' else 2\n",
    "                            })\n",
    "            \n",
    "            # cursorë¥¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬ (Bottomì´ ìš°ì„ )\n",
    "            if all_cursors:\n",
    "                all_cursors.sort(key=lambda x: x['priority'])\n",
    "                selected_cursor = all_cursors[0]['value']\n",
    "                self.used_cursors.add(selected_cursor)\n",
    "                return selected_cursor\n",
    "                \n",
    "        except (AttributeError, KeyError, IndexError):\n",
    "            pass\n",
    "            \n",
    "        return None\n",
    "\n",
    "    # íŠ¸ìœ— ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì™€ íŒŒì¼ ì´ë¦„ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜    \n",
    "    def _save_to_csv(self, tweets_list: List[Dict[str, str]], filename: str):\n",
    "\n",
    "        try:\n",
    "            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "                # 'utf-8-sig'ëŠ” Excelì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ BOMì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "                writer = csv.DictWriter(f, fieldnames=['created_at', 'full_text'])\n",
    "                writer.writeheader()\n",
    "                writer.writerows(tweets_list)\n",
    "            print(f\"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "        except IOError as e:\n",
    "            print(f\"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "    # íŠ¹ì • ì‚¬ìš©ìì˜ íŠ¸ìœ—ì„ ìˆ˜ì§‘í•˜ì—¬ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
    "    def fetch_user_tweets(self, user_id: str, max_tweets: int = 1000):\n",
    "        all_tweets = []\n",
    "        cursor = None\n",
    "        request_count = 0\n",
    "        max_requests = 100  # ë¬´í•œ ë£¨í”„ ë°©ì§€\n",
    "        consecutive_empty_responses = 0\n",
    "        \n",
    "        # cursor ìºì‹œ ì´ˆê¸°í™”\n",
    "        self.used_cursors.clear()\n",
    "        \n",
    "        print(f\"ì‚¬ìš©ì ID {user_id}ì˜ íŠ¸ìœ— ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        while len(all_tweets) < max_tweets and request_count < max_requests:\n",
    "            # countë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì • (ë‚¨ì€ íŠ¸ìœ— ìˆ˜ì— ë”°ë¼)\n",
    "            remaining_tweets = max_tweets - len(all_tweets)\n",
    "            current_count = min(self.count_per_request, remaining_tweets)\n",
    "            \n",
    "            querystring = {\n",
    "                \"user\": user_id,\n",
    "                \"count\": str(current_count)\n",
    "            }\n",
    "            if cursor:\n",
    "                querystring[\"cursor\"] = cursor\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.base_url, headers=self.headers, params=querystring, timeout=45)\n",
    "                request_count += 1\n",
    "                \n",
    "                if response.status_code == 429:  # Rate limit\n",
    "                    print(\"API ìš”ì²­ í•œë„ ì´ˆê³¼, 60ì´ˆ ëŒ€ê¸°...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                elif response.status_code != 200:\n",
    "                    if response.status_code >= 500:  # ì„œë²„ ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„\n",
    "                        print(f\"ì„œë²„ ì˜¤ë¥˜ ({response.status_code}), 10ì´ˆ í›„ ì¬ì‹œë„...\")\n",
    "                        time.sleep(10)\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(f\"API ìš”ì²­ ì‹¤íŒ¨: HTTP {response.status_code}\")\n",
    "                        break\n",
    "                    \n",
    "                data = response.json()\n",
    "                \n",
    "                # íŠ¸ìœ— ë°ì´í„° íŒŒì‹±\n",
    "                newly_fetched_tweets = self._parse_tweets_from_response(data)\n",
    "                \n",
    "                if not newly_fetched_tweets:\n",
    "                    consecutive_empty_responses += 1\n",
    "                    \n",
    "                    if consecutive_empty_responses >= 3:\n",
    "                        print(\"ì—°ì†ìœ¼ë¡œ ë¹ˆ ì‘ë‹µì„ ë°›ì•„ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_empty_responses = 0\n",
    "                \n",
    "                all_tweets.extend(newly_fetched_tweets)\n",
    "                \n",
    "                # ì¤‘ë³µ ì œê±° (created_at + full_text ê¸°ì¤€)\n",
    "                seen = set()\n",
    "                unique_tweets = []\n",
    "                for tweet in all_tweets:\n",
    "                    tweet_key = (tweet['created_at'], tweet['full_text'])\n",
    "                    if tweet_key not in seen:\n",
    "                        seen.add(tweet_key)\n",
    "                        unique_tweets.append(tweet)\n",
    "                \n",
    "                all_tweets = unique_tweets\n",
    "                \n",
    "                # ë‹¤ìŒ cursor ì°¾ê¸°\n",
    "                next_cursor = self._find_next_cursor(data)\n",
    "                if not next_cursor or next_cursor == cursor:\n",
    "                    print(\"ë” ì´ìƒ ìˆ˜ì§‘í•  íŠ¸ìœ—ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                    break\n",
    "                \n",
    "                cursor = next_cursor\n",
    "\n",
    "                # API rate limitë¥¼ ê³ ë ¤í•œ ëŒ€ê¸° ì‹œê°„ (ìš”ì²­ ìˆ˜ì— ë”°ë¼ ì¡°ì •)\n",
    "                if request_count % 10 == 0:  # 10ë²ˆì§¸ë§ˆë‹¤ ê¸´ ëŒ€ê¸°\n",
    "                    wait_time = 5\n",
    "                else:\n",
    "                    wait_time = 1\n",
    "                    \n",
    "                time.sleep(wait_time)\n",
    "\n",
    "            except requests.exceptions.Timeout:\n",
    "                print(\"ìš”ì²­ íƒ€ì„ì•„ì›ƒ, 5ì´ˆ í›„ ì¬ì‹œë„...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"ìš”ì²­ ì˜¤ë¥˜: {e}, 10ì´ˆ í›„ ì¬ì‹œë„...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"JSON íŒŒì‹± ì˜¤ë¥˜, 5ì´ˆ í›„ ì¬ì‹œë„...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "        if all_tweets:\n",
    "            # f\"user_@Ajay_Bagga_tweets.csv\"ì—ì„œ @ì´ë¦„ì€ í•˜ë“œì½”ë”©ìœ¼ë¡œ ë„£ì„ê²ƒ\n",
    "            # ì˜ˆì‹œ1: f\"user_@Ajay_Bagga_tweets.csv\"\n",
    "            # ì˜ˆì‹œ2: f\"user_@elonmusk_tweets.csv\"\n",
    "            filename = f\"user_@Ajay_Bagga_tweets.csv\"\n",
    "            self._save_to_csv(all_tweets, filename)\n",
    "            print(f\"ì´ {len(all_tweets)}ê°œì˜ íŠ¸ìœ—ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(\"ìˆ˜ì§‘ëœ íŠ¸ìœ—ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "        return all_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° íŠ¸ìœ— ìˆ˜ì§‘ ì‹¤í–‰\n",
    "ì„¤ì •ëœ API í‚¤ë¡œ í¬ë¡¤ëŸ¬ë¥¼ ìƒì„±í•˜ê³  íŠ¸ìœ— ìˆ˜ì§‘ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "crawler = RapidAPITweetCrawler(api_key=API_KEY)\n",
    "\n",
    "# íŠ¸ìœ— ìˆ˜ì§‘ ì‹¤í–‰\n",
    "collected_tweets = crawler.fetch_user_tweets(user_id=USER_ID, max_tweets=MAX_TWEETS)\n",
    "\n",
    "# ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "if collected_tweets:\n",
    "    print(f\"ì´ {len(collected_tweets)}ê°œì˜ íŠ¸ìœ—ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ê²°ê³¼ëŠ” user_@Ajay_Bagga_tweets.csv íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì²˜ìŒ 3ê°œ íŠ¸ìœ— ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(\"\\n=== ìˆ˜ì§‘ëœ íŠ¸ìœ— ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 3ê°œ) ===\")\n",
    "    for i, tweet in enumerate(collected_tweets[:3], 1):\n",
    "        print(f\"{i}. [{tweet['created_at']}] {tweet['full_text'][:100]}...\")\n",
    "else:\n",
    "    print(\"ìˆ˜ì§‘ëœ íŠ¸ìœ—ì´ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RAPID API ê¸°ë°˜ íŠ¸ìœ„í„°(X) ë°ì´í„° ì „ì²˜ë¦¬ ë° VADER ê°ì •ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import\n",
    "ê°ì •ë¶„ì„ê³¼ ë°ì´í„° ì²˜ë¦¬ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„ì„ ë° ì¡°ì‘ì„ ìœ„í•œ pandas ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "\n",
    "# VADER ê°ì •ë¶„ì„ ëª¨ë¸ì„ ìœ„í•œ NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ re ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. VADER ê°ì •ë¶„ì„ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ\n",
    "VADER ê°ì •ë¶„ì„ì— í•„ìš”í•œ ì–´íœ˜ ì‚¬ì „ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER ê°ì •ë¶„ì„ì— í•„ìš”í•œ ì–´íœ˜ ì‚¬ì „ì„ ë‹¤ìš´ë¡œë“œ\n",
    "# vader_lexicon: ê°ì • ì ìˆ˜ê°€ ë§¤í•‘ëœ ë‹¨ì–´ ì‚¬ì „ ë°ì´í„°\n",
    "# ìµœì´ˆ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ë©´ ë¡œì»¬ì— ì €ì¥ë¨\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. ë‹¤ì¤‘ ì‚¬ìš©ì íŠ¸ìœ— ë°ì´í„° ë³‘í•©\n",
    "X_data ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  `user_*.csv` íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•˜ì—¬ í†µí•© ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…ì— í•„ìš”í•œ ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import glob  # íŒŒì¼ íŒ¨í„´ ë§¤ì¹­ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os    # ìš´ì˜ì²´ì œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from datetime import datetime  # ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "# glob íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ \"user_\"ë¡œ ì‹œì‘í•˜ê³  \".csv\"ë¡œ ëë‚˜ëŠ” ëª¨ë“  íŒŒì¼ ì°¾ê¸°\n",
    "csv_files = glob.glob(\"../data/user_*.csv\")\n",
    "print(f\"ë°œê²¬ëœ ëª¨ë“  CSV íŒŒì¼: {csv_files}\")\n",
    "print(f\"ì´ {len(csv_files)}ê°œì˜ íŒŒì¼ì„ ë³‘í•©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ê° CSV íŒŒì¼ì˜ ë°ì´í„°í”„ë ˆì„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "dfs = []\n",
    "\n",
    "# ë°œê²¬ëœ ëª¨ë“  CSV íŒŒì¼ì„ ìˆœíšŒí•˜ë©° ë°ì´í„° ë¡œë“œ\n",
    "for file in csv_files:\n",
    "    # íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "    if os.path.exists(file):\n",
    "        # CSV íŒŒì¼ì„ pandas ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì½ê¸°\n",
    "        temp_df = pd.read_csv(file)\n",
    "        \n",
    "        # íŒŒì¼ëª…ì—ì„œ ì‚¬ìš©ìëª… ì¶”ì¶œ (ì˜ˆ: \"user_@elonmusk_tweets.csv\" â†’ \"@elonmusk\")\n",
    "        username = file.replace(\"user_\", \"\").replace(\"_tweets.csv\", \"\")\n",
    "        \n",
    "        # ê° í–‰ì— í•´ë‹¹ ì‚¬ìš©ìëª…ì„ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€\n",
    "        temp_df['username'] = username\n",
    "        \n",
    "        # ë¦¬ìŠ¤íŠ¸ì— ë°ì´í„°í”„ë ˆì„ ì¶”ê°€\n",
    "        dfs.append(temp_df)\n",
    "        print(f\"{file} ì½ê¸° ì™„ë£Œ - {len(temp_df)}ê°œ í–‰\")\n",
    "\n",
    "# ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ í•˜ë‚˜ë¡œ ë³‘í•©\n",
    "if dfs:\n",
    "    # concat í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¸ë¡œë¡œ ë³‘í•©í•˜ê³  ì¸ë±ìŠ¤ ë¦¬ì…‹\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"ğŸ‰ ì´ {len(df)}ê°œ í–‰ì´ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"ğŸ“Š í¬í•¨ëœ ì‚¬ìš©ì ìˆ˜: {len(df['username'].unique())}\")\n",
    "else:\n",
    "    print(\"ì½ì„ ìˆ˜ ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì „ì²˜ë¦¬ ì‹œì‘ - VADER ê°ì •ë¶„ì„ ì „ì²˜ë¦¬ë¥¼ ìµœì†Œí•œìœ¼ë¡œ ìœ ì§€í•œ ì´ìœ \n",
    "\n",
    "- ì†Œì…œë¯¸ë””ì–´ íŠ¹í™” ì„¤ê³„\n",
    "- ê°ì • ê°•í™” í‘œí˜„ ì¸ì‹: ëŒ€ë¬¸ì, ë°˜ë³µ ë¬¸ì, ì´ëª¨ì§€ ìë™ ì²˜ë¦¬\n",
    "- êµ¬ì–´ì²´ ì¹œí™”ì : ì¶•ì•½í˜•, ì†ì–´, ì¸í„°ë„· ìš©ì–´ì— ê°•í•¨\n",
    "- ë¬¸ë§¥ ì¸ì‹: ë¶€ì •ì–´, ê°•ì¡°ì–´, ìˆ˜ì‹ì–´ ì¡°í•© ì´í•´\n",
    "- ê³¼ë„í•œ ì „ì²˜ë¦¬ì˜ ìœ„í—˜ìœ¼ë¡œ ì¸í•œ ì •ë³´ ì†ì‹¤ ì‚¬ë¡€ ë°©ì§€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. URL ì œê±° í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "íŠ¸ìœ— í…ìŠ¤íŠ¸ì—ì„œ URLì„ ì œê±°í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. URLì€ ê°ì •ë¶„ì„ì— ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ HTTP/HTTPS URL íŒ¨í„´ì„ ì°¾ì•„ì„œ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´\n",
    "    # r'https?://\\S+' íŒ¨í„´ ì„¤ëª…:\n",
    "    # - https? : http ë˜ëŠ” https (? ëŠ” sê°€ ìˆê±°ë‚˜ ì—†ê±°ë‚˜)\n",
    "    # - :// : í”„ë¡œí† ì½œ êµ¬ë¶„ì\n",
    "    # - \\S+ : ê³µë°±ì´ ì•„ë‹Œ ë¬¸ìê°€ 1ê°œ ì´ìƒ ì—°ì† (URLì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„)\n",
    "    cleaned_text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # ì•ë’¤ ê³µë°± ì œê±°í•˜ì—¬ ë°˜í™˜\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. íŠ¸ìœ— í…ìŠ¤íŠ¸ì—ì„œ URL ì œê±° ì ìš©\n",
    "ì •ì˜í•œ URL ì œê±° í•¨ìˆ˜ë¥¼ ëª¨ë“  íŠ¸ìœ— í…ìŠ¤íŠ¸ì— ì ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì •ì œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasì˜ apply() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  íŠ¸ìœ— í…ìŠ¤íŠ¸ì— URL ì œê±° í•¨ìˆ˜ ì ìš©\n",
    "# apply()ëŠ” ì‹œë¦¬ì¦ˆì˜ ê° ìš”ì†Œì— í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì‹œë¦¬ì¦ˆë¥¼ ë°˜í™˜\n",
    "df['full_text'] = df['full_text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-6. ë¹ˆ ë¬¸ìì—´ì„ NaNìœ¼ë¡œ ë³€í™˜\n",
    "URL ì œê±° í›„ ë¹ˆ ë¬¸ìì—´ì´ ëœ í…ìŠ¤íŠ¸ë“¤ì„ pandasì˜ NaN(ê²°ì¸¡ê°’)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹ˆ ë¬¸ìì—´('')ì„ pandasì˜ NA(Not Available) ê°’ìœ¼ë¡œ ë³€í™˜\n",
    "# replace() í•¨ìˆ˜: ì²« ë²ˆì§¸ ì¸ìë¥¼ ë‘ ë²ˆì§¸ ì¸ìë¡œ ëŒ€ì²´\n",
    "# inplace=True: ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì„ ì§ì ‘ ìˆ˜ì • (ìƒˆë¡œìš´ ê°ì²´ ìƒì„± ì•ˆ í•¨)\n",
    "# pd.NA: pandas 2.0+ì—ì„œ ê¶Œì¥í•˜ëŠ” ê²°ì¸¡ê°’ í‘œí˜„\n",
    "df['full_text'].replace('', pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-7. ê²°ì¸¡ê°’(NaN) ì œê±°\n",
    "ê°ì •ë¶„ì„ì´ ë¶ˆê°€ëŠ¥í•œ ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ê²°ì¸¡ê°’ì„ ê°€ì§„ í–‰ë“¤ì„ ë°ì´í„°ì…‹ì—ì„œ ì œê±°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ê°’ì„ ê°€ì§„ í–‰ë“¤ì„ ì œê±°\n",
    "# subset=['full_text']: full_text ì»¬ëŸ¼ì—ì„œ NaN ê°’ì„ ê°€ì§„ í–‰ë“¤ë§Œ ì œê±°\n",
    "# inplace=True: ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì„ ì§ì ‘ ìˆ˜ì •\n",
    "# ê°ì •ë¶„ì„ì„ ìœ„í•´ì„œëŠ” í…ìŠ¤íŠ¸ ë‚´ìš©ì´ í•„ìˆ˜ì´ë¯€ë¡œ ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì œê±° í•„ìš”\n",
    "df.dropna(subset=['full_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-8. ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ë°ì´í„° ì •ë ¬\n",
    "íŠ¸ìœ„í„° API í˜•ì‹ì˜ ë‚ ì§œë¥¼ í‘œì¤€ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_format(date_str):\n",
    "\n",
    "    try:\n",
    "        # íŠ¸ìœ„í„° API ë‚ ì§œ í˜•ì‹: \"Mon Jun 16 02:50:54 +0000 2025\"\n",
    "        # pd.to_datetimeìœ¼ë¡œ íŒŒì‹±í•˜ê³  strftimeìœ¼ë¡œ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "        # format ë§¤ê°œë³€ìˆ˜ ì„¤ëª…:\n",
    "        # %a: ì¶•ì•½ëœ ìš”ì¼ëª… (Mon, Tue, ...)\n",
    "        # %b: ì¶•ì•½ëœ ì›”ëª… (Jan, Feb, ...)  \n",
    "        # %d: ì¼ (01-31)\n",
    "        # %H:%M:%S: ì‹œ:ë¶„:ì´ˆ\n",
    "        # %z: íƒ€ì„ì¡´ ì˜¤í”„ì…‹ (+0000)\n",
    "        # %Y: 4ìë¦¬ ì—°ë„\n",
    "        dt = pd.to_datetime(date_str, format='%a %b %d %H:%M:%S %z %Y')\n",
    "        \n",
    "        # íƒ€ì„ì¡´ ì œê±°í•˜ê³  \"YYYY-MM-DD HH:MM:SS\" í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "        return date_str\n",
    "\n",
    "# ë‚ ì§œ í˜•ì‹ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
    "print(\"ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì¤‘...\")\n",
    "\n",
    "# ëª¨ë“  íŠ¸ìœ—ì˜ created_at ì»¬ëŸ¼ì— ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜ ì ìš©\n",
    "df['created_at'] = df['created_at'].apply(convert_date_format)\n",
    "\n",
    "# ë¬¸ìì—´ì„ pandas datetime ê°ì²´ë¡œ ë³€í™˜ (ì •ë ¬ ë° ì‹œê°„ ì—°ì‚°ì„ ìœ„í•´)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# ìµœì‹  íŠ¸ìœ—ì´ ë§¨ ìœ„ì— ì˜¤ë„ë¡ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "# ascending=False: ë‚´ë¦¼ì°¨ìˆœ (í° ê°’ë¶€í„° ì‘ì€ ê°’ ìˆœ)\n",
    "# reset_index(drop=True): ì •ë ¬ í›„ ì¸ë±ìŠ¤ë¥¼ 0ë¶€í„° ë‹¤ì‹œ ì‹œì‘\n",
    "df = df.sort_values(by='created_at', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ì •ë ¬ ì™„ë£Œ!\")\n",
    "print(f\"ë‚ ì§œ ë²”ìœ„: {df['created_at'].min()} ~ {df['created_at'].max()}\")\n",
    "print(\"\\nê²°ê³¼ í™•ì¸:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-9. VADER ê°ì •ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "VADER(Valence Aware Dictionary and sEntiment Reasoner) ê°ì •ë¶„ì„ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER ê°ì •ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "# SentimentIntensityAnalyzer: VADER ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•œ í´ë˜ìŠ¤\n",
    "# - ì†Œì…œ ë¯¸ë””ì–´ í…ìŠ¤íŠ¸ì— íŠ¹í™”ëœ ê°ì •ë¶„ì„ ë„êµ¬\n",
    "# - ì´ëª¨í‹°ì½˜, ëŒ€ë¬¸ì, êµ¬ë‘ì , ë‹¨ì–´ ì¡°í•© ë“±ì„ ê³ ë ¤í•˜ì—¬ ê°ì • ì ìˆ˜ ê³„ì‚°\n",
    "# - positive, negative, neutral, compound ì ìˆ˜ë¥¼ ë°˜í™˜\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-10. ê°ì •ë¶„ì„ í•¨ìˆ˜ ì •ì˜\n",
    "í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ positive/negative/neutralë¡œ ë¶„ë¥˜í•˜ê³  ê° ê°ì • ì ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    # VADER ê°ì •ë¶„ì„ê¸°ë¡œ í…ìŠ¤íŠ¸ ë¶„ì„\n",
    "    # polarity_scores() ë°˜í™˜ê°’:\n",
    "    # - 'neg': negative ê°ì • ì ìˆ˜ (0~1)\n",
    "    # - 'neu': neutral ê°ì • ì ìˆ˜ (0~1)  \n",
    "    # - 'pos': positive ê°ì • ì ìˆ˜ (0~1)\n",
    "    # - 'compound': ë³µí•© ì ìˆ˜ (-1~1, ì „ì²´ì ì¸ ê°ì • ê°•ë„)\n",
    "    scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # compound ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°ì • ë¶„ë¥˜\n",
    "    compound = scores['compound']\n",
    "    \n",
    "    # VADER ê¶Œì¥ ì„ê³„ê°’ì„ ì‚¬ìš©í•œ ê°ì • ë¶„ë¥˜\n",
    "    if compound >= 0.05:\n",
    "        # compound >= 0.05: ê¸ì •ì  ê°ì •\n",
    "        sentiment = 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        # compound <= -0.05: ë¶€ì •ì  ê°ì •\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        # -0.05 < compound < 0.05: ì¤‘ë¦½ì  ê°ì •\n",
    "        sentiment = 'neutral'\n",
    "    \n",
    "    # pandas Seriesë¡œ ë°˜í™˜ (DataFrameì˜ ìƒˆ ì»¬ëŸ¼ë“¤ë¡œ í• ë‹¹í•˜ê¸° ìœ„í•¨)\n",
    "    return pd.Series([sentiment, scores['neg'], scores['neu'], scores['pos']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-11. ëª¨ë“  íŠ¸ìœ—ì— ê°ì •ë¶„ì„ ì‹¤í–‰\n",
    "ì •ì˜í•œ ê°ì •ë¶„ì„ í•¨ìˆ˜ë¥¼ ëª¨ë“  íŠ¸ìœ— í…ìŠ¤íŠ¸ì— ì ìš©í•˜ì—¬ ê°ì • ì»¬ëŸ¼ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasì˜ apply() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  íŠ¸ìœ—ì— ê°ì •ë¶„ì„ í•¨ìˆ˜ ì ìš©\n",
    "# analyze_sentiment() í•¨ìˆ˜ê°€ pd.Seriesë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ \n",
    "# ì—¬ëŸ¬ ì»¬ëŸ¼ì— ë™ì‹œì— í• ë‹¹ ê°€ëŠ¥\n",
    "# - sentiment: ê°ì • ë¶„ë¥˜ ë¼ë²¨ ('positive', 'negative', 'neutral')\n",
    "# - neg: ë¶€ì • ê°ì • ì ìˆ˜ (0~1)\n",
    "# - neu: ì¤‘ë¦½ ê°ì • ì ìˆ˜ (0~1)\n",
    "# - pos: ê¸ì • ê°ì • ì ìˆ˜ (0~1)\n",
    "df[['sentiment', 'neg', 'neu', 'pos']] = df['full_text'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-12. ìµœì¢… ë°ì´í„° ì»¬ëŸ¼ ì„ íƒ\n",
    "ë¶„ì„ì— í•„ìš”í•œ í•µì‹¬ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒí•˜ì—¬ ìµœì¢… ë°ì´í„°ì…‹ì„ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ë¶„ì„ ê²°ê³¼ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ\n",
    "# - created_at: íŠ¸ìœ— ì‘ì„± ë‚ ì§œ/ì‹œê°„\n",
    "# - full_text: ì „ì²˜ë¦¬ëœ íŠ¸ìœ— í…ìŠ¤íŠ¸ (URL ì œê±°ë¨)\n",
    "# - username: íŠ¸ìœ— ì‘ì„±ì (@ì‚¬ìš©ìëª…)\n",
    "# - sentiment: ê°ì • ë¶„ë¥˜ ê²°ê³¼ (positive/negative/neutral)\n",
    "# - neg: ë¶€ì • ê°ì • ì ìˆ˜\n",
    "# - neu: ì¤‘ë¦½ ê°ì • ì ìˆ˜  \n",
    "# - pos: ê¸ì • ê°ì • ì ìˆ˜\n",
    "df = df[['created_at', 'full_text', 'username', 'sentiment', 'neg', 'neu', 'pos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-13. ìµœì¢… ê²°ê³¼ ì €ì¥ ë° ìš”ì•½\n",
    "ê°ì •ë¶„ì„ì´ ì™„ë£Œëœ íŠ¸ìœ— ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ íŒŒì¼ ì €ì¥\n",
    "output_filename = \"merged_tweets_with_sentiment.csv\"\n",
    "\n",
    "# ê°ì •ë¶„ì„ì´ ì™„ë£Œëœ ë°ì´í„°í”„ë ˆì„ì„ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "# index=False: í–‰ ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì— í¬í•¨í•˜ì§€ ì•ŠìŒ\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "# ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì •ë³´ ì¶œë ¥\n",
    "print(f\"âœ… ê²°ê³¼ê°€ {output_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"ğŸ“Š ì´ {len(df)}ê°œì˜ íŠ¸ìœ—ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ê°ì •ë¶„ì„ì— í¬í•¨ëœ ì‚¬ìš©ì ëª©ë¡ (ì•ŒíŒŒë²³ ìˆœ ì •ë ¬)\n",
    "print(f\"ğŸ‘¥ í¬í•¨ëœ ì‚¬ìš©ì: {sorted(df['username'].unique())}\")\n",
    "\n",
    "# íŠ¸ìœ— ë°ì´í„°ì˜ ì‹œê°„ ë²”ìœ„\n",
    "print(f\"ğŸ“… ë‚ ì§œ ë²”ìœ„: {df['created_at'].min()} ~ {df['created_at'].max()}\")\n",
    "\n",
    "# ê°ì • ë¶„ë¥˜ë³„ íŠ¸ìœ— ìˆ˜ í†µê³„\n",
    "print(f\"\\nğŸ“ˆ ê°ì • ë¶„ë¥˜ë³„ í†µê³„:\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  - {sentiment}: {count}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "# ì‚¬ìš©ìë³„ íŠ¸ìœ— ìˆ˜ í†µê³„ (ìƒìœ„ 5ëª…)\n",
    "print(f\"\\nğŸ† ì‚¬ìš©ìë³„ íŠ¸ìœ— ìˆ˜ (ìƒìœ„ 5ëª…):\")\n",
    "user_counts = df['username'].value_counts().head(5)\n",
    "for username, count in user_counts.items():\n",
    "    print(f\"  - {username}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Y finance API ê¸°ë°˜ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. RSI ê³„ì‚° í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(prices, window=14):\n",
    "    \n",
    "    # ì „ì¼ ëŒ€ë¹„ ê°€ê²© ë³€í™”ëŸ‰ ê³„ì‚°\n",
    "    delta = prices.diff()\n",
    "    \n",
    "    # ìƒìŠ¹ì¼ì˜ ê°€ê²© ìƒìŠ¹í­ë§Œ ì¶”ì¶œ (í•˜ë½ì¼ì€ 0ìœ¼ë¡œ ì²˜ë¦¬)\n",
    "    # where ì¡°ê±´: delta > 0ì¸ ê²½ìš°ë§Œ í•´ë‹¹ ê°’ ì‚¬ìš©, ë‚˜ë¨¸ì§€ëŠ” 0\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    \n",
    "    # í•˜ë½ì¼ì˜ ê°€ê²© í•˜ë½í­ë§Œ ì¶”ì¶œ (ìƒìŠ¹ì¼ì€ 0ìœ¼ë¡œ ì²˜ë¦¬)\n",
    "    # -deltaë¡œ í•˜ë½í­ì„ ì–‘ìˆ˜ë¡œ ë³€í™˜\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    \n",
    "    # RS (Relative Strength) = í‰ê·  ìƒìŠ¹í­ / í‰ê·  í•˜ë½í­\n",
    "    rs = gain / loss\n",
    "    \n",
    "    # RSI = 100 - (100 / (1 + RS))\n",
    "    # RSIê°€ 70 ì´ìƒì´ë©´ ê³¼ë§¤ìˆ˜, 30 ì´í•˜ë©´ ê³¼ë§¤ë„ êµ¬ê°„ìœ¼ë¡œ í•´ì„\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    return rsi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. ì‹œê°„ ì¡°ì • í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_time_to_hour(df):\n",
    "    \n",
    "    # Datetime ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "    if 'Datetime' in df.columns:\n",
    "        # ë¬¸ìì—´ì„ pandas datetime ê°ì²´ë¡œ ë³€í™˜\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        \n",
    "        # ì‹œê°„ì„ ì‹œê°„ ë‹¨ìœ„ë¡œ ë‚´ë¦¼ (ë¶„, ì´ˆë¥¼ 0ìœ¼ë¡œ ë§Œë“¦)\n",
    "        # floor('H'): ì‹œê°„ ë‹¨ìœ„ë¡œ ë‚´ë¦¼ ì²˜ë¦¬\n",
    "        # ì˜ˆ: 2024-01-15 13:30:45 â†’ 2024-01-15 13:00:00\n",
    "        df['Datetime'] = df['Datetime'].dt.floor('H')\n",
    "        \n",
    "        # ë™ì¼í•œ ì‹œê°„ì´ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²½ìš° ë§ˆì§€ë§‰ ê°’ë§Œ ìœ ì§€\n",
    "        # keep='last': ì¤‘ë³µëœ ê°’ ì¤‘ ë§ˆì§€ë§‰ í–‰ì„ ìœ ì§€\n",
    "        df = df.drop_duplicates(subset=['Datetime'], keep='last')\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4. ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€ í•¨ìˆ˜ (1ì‹œê°„ ê°„ê²©ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_features(df):\n",
    "    \n",
    "    # === ìˆ˜ìµë¥  ê³„ì‚° ===\n",
    "    # ì „ ì‹œê°„ ëŒ€ë¹„ ìˆ˜ìµë¥  ê³„ì‚° (Close[t] - Close[t-1]) / Close[t-1]\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    \n",
    "    # ë¡œê·¸ ìˆ˜ìµë¥  ê³„ì‚° ln(Close[t] / Close[t-1])\n",
    "    # ì—°ì†ë³µë¦¬ ê°œë…ìœ¼ë¡œ, ì‘ì€ ë³€í™”ì—ì„œëŠ” ì¼ë°˜ ìˆ˜ìµë¥ ê³¼ ìœ ì‚¬\n",
    "    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # === ì´ë™í‰ê·  (Simple Moving Average) ===\n",
    "    # ë‹¨ìˆœ ì´ë™í‰ê· : ì§€ì • ê¸°ê°„ì˜ í‰ê·  ê°€ê²©\n",
    "    df['SMA_10'] = df['Close'].rolling(window=10).mean()  # 10ì‹œê°„ í‰ê· \n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()  # 20ì‹œê°„ í‰ê· \n",
    "    df['SMA_50'] = df['Close'].rolling(window=50).mean()  # 50ì‹œê°„ í‰ê· \n",
    "    \n",
    "    # === ì§€ìˆ˜ì´ë™í‰ê·  (Exponential Moving Average) ===\n",
    "    # ìµœê·¼ ë°ì´í„°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì´ë™í‰ê· \n",
    "    df['EMA_12'] = df['Close'].ewm(span=12).mean()  # 12ì‹œê°„ EMA\n",
    "    df['EMA_26'] = df['Close'].ewm(span=26).mean()  # 26ì‹œê°„ EMA\n",
    "    \n",
    "    # === MACD (Moving Average Convergence Divergence) ===\n",
    "    # ë‹¨ê¸° EMAì™€ ì¥ê¸° EMAì˜ ì°¨ì´ë¡œ ì¶”ì„¸ ë³€í™”ë¥¼ íŒŒì•…\n",
    "    df['MACD'] = df['EMA_12'] - df['EMA_26']  # MACD ë¼ì¸\n",
    "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()  # ì‹œê·¸ë„ ë¼ì¸ (9ì‹œê°„ EMA)\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']  # íˆìŠ¤í† ê·¸ë¨ (ë§¤ìˆ˜/ë§¤ë„ ì‹ í˜¸)\n",
    "    \n",
    "    # === RSI (Relative Strength Index) ===\n",
    "    # ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë©˜í…€ ì§€í‘œ (0~100)\n",
    "    df['RSI'] = calculate_rsi(df['Close'])\n",
    "    \n",
    "    # === ë³¼ë¦°ì € ë°´ë“œ (Bollinger Bands) ===\n",
    "    # ê°€ê²©ì˜ ë³€ë™ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê¸°ìˆ ì  ì§€í‘œ\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=20).mean()  # ì¤‘ì‹¬ì„  (20ì‹œê°„ ì´ë™í‰ê· )\n",
    "    bb_std = df['Close'].rolling(window=20).std()  # 20ì‹œê°„ í‘œì¤€í¸ì°¨\n",
    "    df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)  # ìƒë‹¨ ë°´ë“œ (í‰ê·  + 2*í‘œì¤€í¸ì°¨)\n",
    "    df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)  # í•˜ë‹¨ ë°´ë“œ (í‰ê·  - 2*í‘œì¤€í¸ì°¨)\n",
    "    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']  # ë°´ë“œ í­ (ë³€ë™ì„± ì§€í‘œ)\n",
    "    # í˜„ì¬ ê°€ê²©ì´ ë°´ë“œ ë‚´ì—ì„œ ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ”ì§€ (0~1, 0.5ê°€ ì¤‘ì•™)\n",
    "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "    \n",
    "    # === ë³€ë™ì„± ì§€í‘œ ===\n",
    "    # ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨ë¡œ ê°€ê²© ë³€ë™ì„± ì¸¡ì •\n",
    "    df['Volatility_10'] = df['Returns'].rolling(window=10).std()  # 10ì‹œê°„ ë³€ë™ì„±\n",
    "    df['Volatility_20'] = df['Returns'].rolling(window=20).std()  # 20ì‹œê°„ ë³€ë™ì„±\n",
    "    \n",
    "    # === ê°€ê²© ë³€í™” ì§€í‘œ ===\n",
    "    # ì‹œê°€ ëŒ€ë¹„ ì¢…ê°€ ë³€í™” (ì ˆëŒ€ê°’)\n",
    "    df['Price_Change'] = df['Close'] - df['Open']\n",
    "    \n",
    "    # ì‹œê°€ ëŒ€ë¹„ ì¢…ê°€ ë³€í™” (ë°±ë¶„ìœ¨)\n",
    "    df['Price_Change_Pct'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "    \n",
    "    # === High-Low ìŠ¤í”„ë ˆë“œ ===\n",
    "    # ë‹¹ì¼ ìµœê³ ê°€ì™€ ìµœì €ê°€ì˜ ì°¨ì´ (ì¼ì¤‘ ë³€ë™í­)\n",
    "    df['HL_Spread'] = df['High'] - df['Low']\n",
    "    \n",
    "    # ì¢…ê°€ ëŒ€ë¹„ ì¼ì¤‘ ë³€ë™í­ ë¹„ìœ¨\n",
    "    df['HL_Spread_Pct'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
    "    \n",
    "    # === ì‹œê°„ íŠ¹ì„± ===\n",
    "    # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„ì„ ìœ„í•œ íŠ¹ì„±\n",
    "    df['Hour'] = df['Datetime'].dt.hour  # ì‹œê°„ (0~23)\n",
    "    df['DayOfWeek'] = df['Datetime'].dt.dayofweek  # ìš”ì¼ (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)\n",
    "    df['Month'] = df['Datetime'].dt.month  # ì›” (1~12)\n",
    "    df['Quarter'] = df['Datetime'].dt.quarter  # ë¶„ê¸° (1~4)\n",
    "    \n",
    "    # === ê±°ë˜ì‹œê°„ ë¶„ë¥˜ ===\n",
    "    # ë¯¸êµ­ ì£¼ì‹ì‹œì¥ ê¸°ì¤€ ê±°ë˜ì‹œê°„ ë¶„ë¥˜\n",
    "    df['Is_Trading_Hours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 16)).astype(int)  # ì •ê·œ ê±°ë˜ì‹œê°„\n",
    "    df['Is_Market_Open'] = ((df['Hour'] >= 9) & (df['Hour'] < 16)).astype(int)     # ì‹œì¥ ê°œì¥ì‹œê°„\n",
    "    df['Is_Premarket'] = ((df['Hour'] >= 4) & (df['Hour'] < 9)).astype(int)       # í”„ë¦¬ë§ˆì¼“ (4:00-9:30)\n",
    "    df['Is_Aftermarket'] = ((df['Hour'] >= 16) & (df['Hour'] <= 20)).astype(int)  # ì• í”„í„°ë§ˆì¼“ (16:00-20:00)\n",
    "    df['Is_Extended_Hours'] = (df['Is_Premarket'] | df['Is_Aftermarket']).astype(int)  # ì—°ì¥ê±°ë˜ì‹œê°„\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-5. 1ì‹œê°„ ê°„ê²© ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hourly_stock_data(ticker, days=365, save_to_csv=True):\n",
    "    \n",
    "    try:\n",
    "        # yfinance 1ì‹œê°„ ê°„ê²© ë°ì´í„° ì œì•½ì‚¬í•­ í™•ì¸ (ìµœëŒ€ 730ì¼)\n",
    "        if days > 730:\n",
    "            days = 730\n",
    "        \n",
    "        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ê³„ì‚°\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        \n",
    "        # yfinanceë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì‹ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "        stock_data = yf.download(\n",
    "            ticker,  # í‹°ì»¤ ì‹¬ë³¼\n",
    "            start=start_date.strftime('%Y-%m-%d'),  # ì‹œì‘ì¼ (YYYY-MM-DD í˜•ì‹)\n",
    "            end=end_date.strftime('%Y-%m-%d'),      # ì¢…ë£Œì¼ (YYYY-MM-DD í˜•ì‹)\n",
    "            interval='1h',    # 1ì‹œê°„ ê°„ê²©\n",
    "            prepost=True,     # ì‹œì¥ ì™¸ ì‹œê°„ ë°ì´í„° í¬í•¨ (í”„ë¦¬ë§ˆì¼“, ì• í”„í„°ë§ˆì¼“)\n",
    "            progress=False    # ì§„í–‰ìƒí™© í‘œì‹œ ì•ˆ í•¨\n",
    "        )\n",
    "        \n",
    "        # ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° None ë°˜í™˜\n",
    "        if stock_data.empty:\n",
    "            return None\n",
    "        \n",
    "        # ì¸ë±ìŠ¤(ë‚ ì§œ/ì‹œê°„)ë¥¼ ì¼ë°˜ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜\n",
    "        stock_data = stock_data.reset_index()\n",
    "        \n",
    "        # === ì»¬ëŸ¼ëª… ì •ë¦¬ ì‘ì—… ===\n",
    "        # yfinanceì˜ ë‹¤ì–‘í•œ ë°˜í™˜ í˜•ì‹ì— ëŒ€ì‘\n",
    "        \n",
    "        # 'Date' ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ 'Datetime'ìœ¼ë¡œ ë³€ê²½\n",
    "        if 'Date' in stock_data.columns:\n",
    "            stock_data = stock_data.rename(columns={'Date': 'Datetime'})\n",
    "        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ì‹œê°„ ë°ì´í„°ì¸ ê²½ìš° 'Datetime'ìœ¼ë¡œ ë³€ê²½\n",
    "        elif stock_data.columns[0] not in ['Datetime', 'Date']:\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # === ë©€í‹°ë ˆë²¨ ì»¬ëŸ¼ ì²˜ë¦¬ ===\n",
    "        # yfinanceê°€ ë•Œë¡œ ê³„ì¸µì  ì»¬ëŸ¼ êµ¬ì¡°ë¡œ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°\n",
    "        if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "            new_columns = []\n",
    "            for col in stock_data.columns:\n",
    "                if isinstance(col, tuple):\n",
    "                    # íŠœí”Œì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ ì‹œê°„ ê´€ë ¨ì´ë©´ 'Datetime'ìœ¼ë¡œ\n",
    "                    if col[0] == 'Datetime' or 'Date' in str(col[0]):\n",
    "                        new_columns.append('Datetime')\n",
    "                    else:\n",
    "                        # ê·¸ ì™¸ëŠ” ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì‚¬ìš© (Open, High, Low, Close, Volume)\n",
    "                        new_columns.append(col[0])\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "            stock_data.columns = new_columns\n",
    "        \n",
    "        # ìµœì¢…ì ìœ¼ë¡œ 'Datetime' ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ì‚¬ìš©\n",
    "        if 'Datetime' not in stock_data.columns:\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # === ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ê°€ ===\n",
    "        # ì‹œê°„ì„ ì •ì‹œë¡œ ì¡°ì • (ì˜ˆ: 13:30 â†’ 13:00)\n",
    "        stock_data = adjust_time_to_hour(stock_data)\n",
    "        \n",
    "        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ë° ì¶”ê°€\n",
    "        stock_data = add_technical_features(stock_data)\n",
    "        \n",
    "        # === CSV íŒŒì¼ ì €ì¥ ===\n",
    "        if save_to_csv:\n",
    "            # íŒŒì¼ëª… í˜•ì‹: {TICKER}_1hour_data_{DAYS}days.csv\n",
    "            filename = f\"{ticker}_1hour_data_{days}days.csv\"\n",
    "            stock_data.to_csv(filename, index=False)  # ì¸ë±ìŠ¤ ì œì™¸í•˜ê³  ì €ì¥\n",
    "        \n",
    "        return stock_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        # ì—ëŸ¬ ë°œìƒ ì‹œ None ë°˜í™˜\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-6. ë‹¤ì¤‘ í‹°ì»¤ 1ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_tickers_hourly(tickers, days=365, save_individual=True, save_combined=True):\n",
    "    \n",
    "    # ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”\n",
    "    all_data = {}\n",
    "    \n",
    "    # ê° í‹°ì»¤ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘\n",
    "    for ticker in tickers:\n",
    "        # ê°œë³„ í‹°ì»¤ ë°ì´í„° ìˆ˜ì§‘ (save_to_csvëŠ” save_individual ì„¤ì •ì— ë”°ë¼)\n",
    "        data = get_hourly_stock_data(ticker, days=days, save_to_csv=save_individual)\n",
    "        \n",
    "        # ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ìœ íš¨í•œ ê²½ìš° ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n",
    "        if data is not None:\n",
    "            all_data[ticker] = data\n",
    "    \n",
    "    # === í†µí•© ë°ì´í„° íŒŒì¼ ìƒì„± ===\n",
    "    if save_combined and all_data:\n",
    "        # ëª¨ë“  í‹°ì»¤ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹  ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        combined_data = pd.DataFrame()\n",
    "        \n",
    "        # ê° í‹°ì»¤ ë°ì´í„°ì— í‹°ì»¤ ì»¬ëŸ¼ ì¶”ê°€ í›„ í†µí•©\n",
    "        for ticker, data in all_data.items():\n",
    "            # ì›ë³¸ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ì—¬ ìˆ˜ì •\n",
    "            ticker_data = data.copy()\n",
    "            \n",
    "            # í‹°ì»¤ ì‹ë³„ì„ ìœ„í•œ 'Ticker' ì»¬ëŸ¼ ì¶”ê°€\n",
    "            ticker_data['Ticker'] = ticker\n",
    "            \n",
    "            # ê¸°ì¡´ í†µí•© ë°ì´í„°ì— í˜„ì¬ í‹°ì»¤ ë°ì´í„° ì¶”ê°€\n",
    "            # ignore_index=True: ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œ ìƒì„± (ì—°ì†ì ì¸ ë²ˆí˜¸)\n",
    "            combined_data = pd.concat([combined_data, ticker_data], ignore_index=True)\n",
    "        \n",
    "        # í†µí•© ë°ì´í„° CSV íŒŒì¼ ì €ì¥\n",
    "        combined_filename = f\"multiple_stocks_1hour_data_{days}days.csv\"\n",
    "        combined_data.to_csv(combined_filename, index=False)\n",
    "    \n",
    "    # í‹°ì»¤ë³„ ë°ì´í„°ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-7. ë°ì´í„° ìš”ì•½ ë¶„ì„ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_summary(data_dict):\n",
    "    \n",
    "    # ë”•ì…”ë„ˆë¦¬ì˜ ê° í‹°ì»¤ì™€ ë°ì´í„°ì— ëŒ€í•´ ë°˜ë³µ\n",
    "    for ticker, data in data_dict.items():\n",
    "        # ë°ì´í„°ê°€ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ë¶„ì„ ì§„í–‰\n",
    "        if data is not None:\n",
    "            # ì „ì²´ ë°ì´í„°ì—ì„œ ê²°ì¸¡ì¹˜(NaN) ê°œìˆ˜ ê³„ì‚°\n",
    "            # isnull(): ê° ì…€ì´ ê²°ì¸¡ì¹˜ì¸ì§€ True/False ë°˜í™˜\n",
    "            # sum().sum(): ë¨¼ì € ê° ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ë¥¼ êµ¬í•œ í›„, ì „ì²´ í•©ê³„ ê³„ì‚°\n",
    "            missing_count = data.isnull().sum().sum()\n",
    "            \n",
    "            # ê²°ê³¼ ì¶œë ¥ (ë°ì´í„° í¬ì¸íŠ¸ ê°œìˆ˜ì™€ ê²°ì¸¡ì¹˜ ê°œìˆ˜)\n",
    "            # len(data): ì´ í–‰(ë°ì´í„° í¬ì¸íŠ¸) ê°œìˆ˜\n",
    "            # :,: ì²œ ë‹¨ìœ„ êµ¬ë¶„ì ì¶”ê°€ (ì˜ˆ: 1000 â†’ 1,000)\n",
    "            print(f\"{ticker}: {len(data):,}ê°œ í¬ì¸íŠ¸, ê²°ì¸¡ì¹˜: {missing_count}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-8. ì£¼ìš” ì¢…ëª© 1ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ìˆ˜ì§‘ ëŒ€ìƒ ì£¼ìš” ì¢…ëª© ì„¤ì • ===\n",
    "# ëŒ€í˜•ì£¼ ê¸°ìˆ ì£¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„ ì • (ì‹œê°€ì´ì•¡, ê±°ë˜ëŸ‰, ë³€ë™ì„± ê³ ë ¤)\n",
    "# - AAPL: ì• í”Œ (ì•„ì´í°, ë§¥ë¶ ë“± í•˜ë“œì›¨ì–´)\n",
    "# - AMZN: ì•„ë§ˆì¡´ (ì „ììƒê±°ë˜, í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤)\n",
    "# - TSLA: í…ŒìŠ¬ë¼ (ì „ê¸°ìë™ì°¨, ì—ë„ˆì§€)\n",
    "# - GOOGL: êµ¬ê¸€ (ê²€ìƒ‰ì—”ì§„, ê´‘ê³ , í´ë¼ìš°ë“œ)\n",
    "# - MSFT: ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ (ìœˆë„ìš°, ì˜¤í”¼ìŠ¤, í´ë¼ìš°ë“œ)\n",
    "tickers = ['AAPL', 'AMZN', 'TSLA', 'GOOGL', 'MSFT']\n",
    "\n",
    "# === 1ì‹œê°„ ê°„ê²© ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ ===\n",
    "# days=365: 1ë…„ê°„ì˜ ë°ì´í„° ìˆ˜ì§‘ (ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„° í™•ë³´)\n",
    "# save_individual=True: ê° ì¢…ëª©ë³„ ê°œë³„ CSV íŒŒì¼ ìƒì„±\n",
    "# save_combined=True: ëª¨ë“  ì¢…ëª©ì„ í†µí•©í•œ CSV íŒŒì¼ë„ ìƒì„±\n",
    "all_stock_data = get_multiple_tickers_hourly(tickers, days=365)\n",
    "\n",
    "# === ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ë¶„ì„ ===\n",
    "# ê° ì¢…ëª©ë³„ ë°ì´í„° í¬ì¸íŠ¸ ê°œìˆ˜ì™€ ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸\n",
    "analyze_data_summary(all_stock_data)\n",
    "\n",
    "# ìˆ˜ì§‘ ì™„ë£Œëœ ì¢…ëª© ê°œìˆ˜ ì¶œë ¥\n",
    "print(f\"\\\\në°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(all_stock_data)}ê°œ ì¢…ëª©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-9. AAPL ë°ì´í„° ìƒì„¸ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AAPL ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„¸ ë¶„ì„ ===\n",
    "if 'AAPL' in all_stock_data:\n",
    "    # AAPL 1ì‹œê°„ ë°ì´í„°ë¥¼ ë³€ìˆ˜ì— ì €ì¥\n",
    "    aapl_1h = all_stock_data['AAPL']\n",
    "    \n",
    "    # === ê¸°ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===\n",
    "    print(\"AAPL 1ì‹œê°„ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    # í•µì‹¬ OHLCV ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ìƒìœ„ 5ê°œ í–‰ ì¶œë ¥\n",
    "    # OHLCV: Open(ì‹œê°€), High(ê³ ê°€), Low(ì €ê°€), Close(ì¢…ê°€), Volume(ê±°ë˜ëŸ‰)\n",
    "    print(aapl_1h[['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']].head())\n",
    "    \n",
    "    # === ê±°ë˜ì‹œê°„ ë°ì´í„° í•„í„°ë§ ===\n",
    "    # LSTM í•™ìŠµì—ëŠ” ì •ê·œ ê±°ë˜ì‹œê°„ ë°ì´í„°ê°€ ë” ì•ˆì •ì \n",
    "    # Is_Trading_Hours == 1: ë¯¸êµ­ ì‹œì¥ ì •ê·œ ê±°ë˜ì‹œê°„ (9:30-16:00 ET)\n",
    "    trading_hours = aapl_1h[aapl_1h['Is_Trading_Hours'] == 1]\n",
    "    \n",
    "    # === í•™ìŠµìš© ë°ì´í„° ë¶„ì„ ===\n",
    "    print(f\"\\\\ní•™ìŠµìš© ë°ì´í„° ë¶„ì„:\")\n",
    "    print(f\"ì „ì²´ ì‹œê°„ ê°œìˆ˜: {len(aapl_1h):,}ê°œ\")  # í”„ë¦¬ë§ˆì¼“, ì• í”„í„°ë§ˆì¼“ í¬í•¨\n",
    "    print(f\"ê±°ë˜ì‹œê°„ ê°œìˆ˜: {len(trading_hours):,}ê°œ\")  # ì •ê·œ ê±°ë˜ì‹œê°„ë§Œ\n",
    "    \n",
    "    # === ê¸°ìˆ ì  ì§€í‘œ ì»¬ëŸ¼ í™•ì¸ ===\n",
    "    # LSTM ëª¨ë¸ì˜ ì…ë ¥ íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©í•  ì£¼ìš” ê¸°ìˆ ì  ì§€í‘œë“¤\n",
    "    tech_indicators = [col for col in aapl_1h.columns if col in ['SMA_10', 'SMA_20', 'RSI', 'MACD', 'BB_Upper', 'BB_Lower']]\n",
    "    print(f\"ê¸°ìˆ ì  ì§€í‘œ ì»¬ëŸ¼: {tech_indicators}\")\n",
    "    \n",
    "    # === ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===\n",
    "    # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ê¸°ìˆ ì  ì§€í‘œ í™•ì¸\n",
    "    missing_indicators = aapl_1h[tech_indicators].isnull().sum()\n",
    "    print(f\"\\\\nê¸°ìˆ ì  ì§€í‘œë³„ ê²°ì¸¡ì¹˜:\")\n",
    "    for indicator, missing_count in missing_indicators.items():\n",
    "        if missing_count > 0:\n",
    "            print(f\"  {indicator}: {missing_count}ê°œ\")\n",
    "    \n",
    "    # === ë°ì´í„° ë²”ìœ„ ì •ë³´ ===\n",
    "    print(f\"\\\\në°ì´í„° ê¸°ê°„:\")\n",
    "    print(f\"ì‹œì‘: {aapl_1h['Datetime'].min()}\")\n",
    "    print(f\"ì¢…ë£Œ: {aapl_1h['Datetime'].max()}\")\n",
    "    print(f\"ì´ ê¸°ê°„: {(aapl_1h['Datetime'].max() - aapl_1h['Datetime'].min()).days}ì¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-10. ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ ìµœì¢… ìš”ì•½ ===\n",
    "print(\"\\\\n ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "\n",
    "# ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ëœ ì¢…ëª© ëª©ë¡ ì¶œë ¥\n",
    "print(f\"ìˆ˜ì§‘ëœ ì¢…ëª©: {list(all_stock_data.keys())}\")\n",
    "\n",
    "# === ìƒì„±ëœ CSV íŒŒì¼ ëª©ë¡ í™•ì¸ ===\n",
    "print(\"\\\\n ìƒì„±ëœ CSV íŒŒì¼:\")\n",
    "\n",
    "# í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ CSV íŒŒì¼ë§Œ í•„í„°ë§\n",
    "import os\n",
    "# ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ CSV íŒŒì¼ ì¤‘ ìˆ˜ì§‘ ëŒ€ìƒ í‹°ì»¤ê°€ í¬í•¨ëœ íŒŒì¼ë§Œ ì„ íƒ\n",
    "csv_files = [f for f in os.listdir('.') if f.endswith('.csv') and any(ticker in f for ticker in tickers)]\n",
    "\n",
    "# ê° ìƒì„±ëœ CSV íŒŒì¼ì„ ì¶œë ¥\n",
    "for file in csv_files:\n",
    "    print(f\"  {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. íŠ¸ìœ— - ì£¼ê°€ ë°ì´í„° ë³‘í•© ë° ë¼ë²¨ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# íŠ¸ìœ—-ì£¼ê°€ ë°ì´í„° ë³‘í•© ë° ë¼ë²¨ë§ (ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œ)\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd  # ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•œ pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "stock_path = \"./AAPL_1hour_data_365days.csv\"  # AAPL 1ì‹œê°„ ë‹¨ìœ„ ì£¼ê°€ ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ë™ì¼)\n",
    "tweets_path = \"./merged_tweets_with_sentiment.csv\"  # VADER ê°ì •ë¶„ì„ì´ ì™„ë£Œëœ íŠ¸ìœ— ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ë‚ ì§œ ì»¬ëŸ¼ì„ ìë™ìœ¼ë¡œ datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜)\n",
    "stock_df = pd.read_csv(stock_path, parse_dates=[\"Datetime\"])  # ì£¼ê°€ ë°ì´í„° ë¡œë“œ, Datetime ì»¬ëŸ¼ì„ ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ íŒŒì‹±\n",
    "tweets_df = pd.read_csv(tweets_path, parse_dates=[\"created_at\"])  # íŠ¸ìœ— ë°ì´í„° ë¡œë“œ, created_at ì»¬ëŸ¼ì„ ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ íŒŒì‹±\n",
    "\n",
    "# íƒ€ì„ì¡´ ì œê±° (ì„œë¡œ ë‹¤ë¥¸ íƒ€ì„ì¡´ìœ¼ë¡œ ì¸í•œ ë³‘í•© ë¬¸ì œ í•´ê²°)\n",
    "stock_df[\"Datetime\"] = stock_df[\"Datetime\"].dt.tz_localize(None)  # ì£¼ê°€ ë°ì´í„°ì˜ íƒ€ì„ì¡´ ì •ë³´ ì œê±° (UTCë‚˜ ë¡œì»¬ íƒ€ì„ì¡´ ë¬´ê´€í•˜ê²Œ ë§Œë“¦)\n",
    "tweets_df[\"created_at\"] = tweets_df[\"created_at\"].dt.tz_localize(None)  # íŠ¸ìœ— ë°ì´í„°ì˜ íƒ€ì„ì¡´ ì •ë³´ ì œê±°\n",
    "\n",
    "# ì‹œê³„ì—´ ë°ì´í„° ì •ë ¬ (ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ê³¼ê±°/ë¯¸ë˜ íƒìƒ‰ì„ ì •í™•í•˜ê²Œ í•¨)\n",
    "stock_df = stock_df.sort_values(\"Datetime\").reset_index(drop=True)  # ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ì¸ë±ìŠ¤ ì¬ì„¤ì •\n",
    "tweets_df = tweets_df.sort_values(\"created_at\").reset_index(drop=True)  # íŠ¸ìœ—ë„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ê±°ë˜ ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ ì œê±° (ëª¨ë¸ í•™ìŠµì— ë¶ˆí•„ìš”í•œ boolean ë³€ìˆ˜ë“¤ ì œê±°)\n",
    "exclude_cols = ['Is_Trading_Hours', 'Is_Market_Open', 'Is_Premarket', 'Is_Aftermarket', 'Is_Extended_Hours']  # ì œê±°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
    "stock_df = stock_df.drop(columns=[col for col in exclude_cols if col in stock_df.columns])  # í•´ë‹¹ ì»¬ëŸ¼ë“¤ì´ ì¡´ì¬í•˜ë©´ ì œê±°\n",
    "\n",
    "# ë³‘í•© ê²°ê³¼ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "tweet_rows = []  # ê° íŠ¸ìœ—ì— ëŒ€í•œ íŠ¹ì„±ê³¼ ë¼ë²¨ì„ ë‹´ì„ ë”•ì…”ë„ˆë¦¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# ê° íŠ¸ìœ—ë³„ë¡œ ë°˜ë³µ ì²˜ë¦¬ (íŠ¸ìœ— ë°œìƒ ì‹œì  ê¸°ì¤€ìœ¼ë¡œ ì£¼ê°€ ë³€í™” íŒ¨í„´ ì¶”ì¶œ)\n",
    "for idx, tweet_row in tweets_df.iterrows():  # íŠ¸ìœ— ë°ì´í„°í”„ë ˆì„ì˜ ê° í–‰ì„ ìˆœíšŒ\n",
    "    tweet_time = tweet_row['created_at']  # í˜„ì¬ íŠ¸ìœ—ì˜ ìƒì„± ì‹œê°„ ì¶”ì¶œ\n",
    "    \n",
    "    if idx % 1000 == 0:  # ì§„í–‰ìƒí™© ì¶œë ¥ (1000ê°œë§ˆë‹¤)\n",
    "        print(f\"ì²˜ë¦¬ ì¤‘: {idx}/{len(tweets_df)} íŠ¸ìœ—\")\n",
    "\n",
    "    # íŠ¸ìœ— ë°œí–‰ ì´í›„ ê°€ì¥ ê°€ê¹Œìš´ ë¯¸ë˜ ì£¼ê°€ ë°ì´í„° ì°¾ê¸° (ì˜ˆì¸¡ íƒ€ê²Ÿ)\n",
    "    future_stock = stock_df[stock_df['Datetime'] > tweet_time].head(1)  # íŠ¸ìœ— ì‹œê°„ ì´í›„ì˜ ì²« ë²ˆì§¸ ì£¼ê°€ ë°ì´í„° ì„ íƒ\n",
    "    if future_stock.empty:  # ë¯¸ë˜ ì£¼ê°€ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ (íŠ¸ìœ—ì´ ê°€ì¥ ìµœê·¼ì¸ ê²½ìš°)\n",
    "        continue  # í•´ë‹¹ íŠ¸ìœ—ì€ ê±´ë„ˆë›°ê³  ë‹¤ìŒ íŠ¸ìœ—ìœ¼ë¡œ ì§„í–‰\n",
    "\n",
    "    # ì˜ˆì¸¡ íƒ€ê²Ÿì´ ë  ì£¼ê°€ ì •ë³´ ì¶”ì¶œ\n",
    "    target_row = future_stock.iloc[0]  # ë¯¸ë˜ ì£¼ê°€ ë°ì´í„°ì˜ ì²« ë²ˆì§¸ í–‰\n",
    "    target_time = target_row['Datetime']  # íƒ€ê²Ÿ ì£¼ê°€ì˜ ì‹œê°„\n",
    "    target_close = target_row['Close']  # íƒ€ê²Ÿ ì£¼ê°€ì˜ ì¢…ê°€ (ì˜ˆì¸¡í•´ì•¼ í•  ê°’)\n",
    "\n",
    "    # íƒ€ê²Ÿ ì‹œì  ì´ì „ì˜ ê³¼ê±° 3ê°œ ì£¼ê°€ ë°ì´í„° ì¶”ì¶œ (ì‹œê³„ì—´ íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©)\n",
    "    past_rows = stock_df[stock_df['Datetime'] < target_time].tail(3)  # íƒ€ê²Ÿ ì‹œê°„ ì´ì „ì˜ ë§ˆì§€ë§‰ 3ê°œ ì£¼ê°€ ë°ì´í„°\n",
    "    if len(past_rows) < 3:  # ì¶©ë¶„í•œ ê³¼ê±° ë°ì´í„°ê°€ ì—†ìœ¼ë©´ (ë°ì´í„° ì´ˆê¸° ì‹œì )\n",
    "        continue  # í•´ë‹¹ íŠ¸ìœ—ì€ ê±´ë„ˆë›°ê³  ë‹¤ìŒ íŠ¸ìœ—ìœ¼ë¡œ ì§„í–‰\n",
    "\n",
    "    # ìˆ˜ìµë¥  ê³„ì‚°ì„ ìœ„í•œ ê¸°ì¤€ì  (ê³¼ê±° ë°ì´í„° ì¤‘ ê°€ì¥ ìµœê·¼ ì¢…ê°€)\n",
    "    past_last_close = past_rows.iloc[-1]['Close']  # ê³¼ê±° 3ê°œ ë°ì´í„° ì¤‘ ê°€ì¥ ìµœê·¼ ì¢…ê°€\n",
    "\n",
    "    # ìˆ˜ìµë¥  ê³„ì‚° ë° ë¶„ë¥˜ ë¼ë²¨ ìƒì„±\n",
    "    return_pct = (target_close - past_last_close) / past_last_close * 100  # ë°±ë¶„ìœ¨ ìˆ˜ìµë¥  ê³„ì‚°\n",
    "    # 3í´ë˜ìŠ¤ ë¶„ë¥˜: 0.4% ì´ìƒ ìƒìŠ¹(1), -0.4% ì´í•˜ í•˜ë½(-1), ê·¸ ì™¸ ë³´í•©(0)\n",
    "    label = 1 if return_pct >= 0.4 else (-1 if return_pct <= -0.4 else 0)  # ì„ê³„ê°’ ê¸°ë°˜ ë¼ë²¨ë§\n",
    "\n",
    "    # í•œ í–‰ì˜ ë°ì´í„°ë¥¼ ë‹´ì„ ë”•ì…”ë„ˆë¦¬ ìƒì„± (íŠ¹ì„± + ë¼ë²¨)\n",
    "    row = {\n",
    "        \"tweet_id\": idx,  # íŠ¸ìœ— ì¸ë±ìŠ¤ (ê³ ìœ  ID ì—­í• )\n",
    "        \"tweet_time\": tweet_time,  # íŠ¸ìœ— ìƒì„± ì‹œê°„\n",
    "        \"target_close\": target_close,  # ì˜ˆì¸¡ íƒ€ê²Ÿ ì¢…ê°€\n",
    "        \"target_return_pct\": return_pct,  # ì‹¤ì œ ìˆ˜ìµë¥  (ì—°ì†ê°’)\n",
    "        \"target_multi_raw\": label,  # ì›ë³¸ ë¶„ë¥˜ ë¼ë²¨ (-1, 0, 1)\n",
    "        \"vader_positive\": tweet_row['pos'],  # VADER ê¸ì • ê°ì • ì ìˆ˜\n",
    "        \"vader_neutral\": tweet_row['neu'],  # VADER ì¤‘ë¦½ ê°ì • ì ìˆ˜\n",
    "        \"vader_negative\": tweet_row['neg'],  # VADER ë¶€ì • ê°ì • ì ìˆ˜\n",
    "    }\n",
    "\n",
    "    # ê³¼ê±° 3ê°œ ì£¼ê°€ ë°ì´í„°ë¥¼ í‰ë©´í™”(flatten)í•˜ì—¬ íŠ¹ì„±ìœ¼ë¡œ ì¶”ê°€\n",
    "    for i, (_, stock_row) in enumerate(past_rows.iterrows(), 1):  # ê³¼ê±° 3ê°œ ë°ì´í„°ë¥¼ ìˆœíšŒ (1ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤)\n",
    "        for col in stock_df.columns:  # ì£¼ê°€ ë°ì´í„°ì˜ ëª¨ë“  ì»¬ëŸ¼ì— ëŒ€í•´\n",
    "            if col == \"Datetime\":  # ë‚ ì§œ ì»¬ëŸ¼ì€ ì œì™¸\n",
    "                continue\n",
    "            # x1_Open, x1_High, x2_Open, x2_High, x3_Open, x3_High ë“±ì˜ í˜•íƒœë¡œ íŠ¹ì„± ìƒì„±\n",
    "            row[f\"x{i}_{col}\"] = stock_row[col]  # ië²ˆì§¸ ê³¼ê±° ë°ì´í„°ì˜ ê° ì»¬ëŸ¼ê°’ì„ íŠ¹ì„±ìœ¼ë¡œ ì¶”ê°€\n",
    "\n",
    "    tweet_rows.append(row)  # ì™„ì„±ëœ í–‰ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "\n",
    "# ëª¨ë“  í–‰ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "tweet_merged_df = pd.DataFrame(tweet_rows)  # ë¦¬ìŠ¤íŠ¸ì˜ ë”•ì…”ë„ˆë¦¬ë“¤ì„ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "\n",
    "# ê°ì • ë¼ë²¨ì„ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ (ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ìœ„í•´)\n",
    "# sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}  # ê°ì • ë¼ë²¨ì„ ìˆ«ìë¡œ ë§¤í•‘\n",
    "# tweet_merged_df[\"sentiment_numeric\"] = tweet_merged_df[\"sentiment_label\"].map(sentiment_map)  # ìˆ˜ì¹˜í˜• ê°ì • ë¼ë²¨ ìƒì„±\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ìœ„í•œ íƒ€ê²Ÿ ë¼ë²¨ ë³€í™˜ (ìŒìˆ˜ ë¼ë²¨ì„ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ë³€ê²½)\n",
    "label_map = {-1: 0, 0: 1, 1: 2}  # -1(í•˜ë½)â†’0, 0(ë³´í•©)â†’1, 1(ìƒìŠ¹)â†’2ë¡œ ë§¤í•‘\n",
    "tweet_merged_df[\"target_multi\"] = tweet_merged_df[\"target_multi_raw\"].map(label_map)  # ìƒˆë¡œìš´ ë¼ë²¨ ì»¬ëŸ¼ ìƒì„±\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "tweet_merged_df.to_csv(\"tweet_stock_classification.csv\", index=False)  # ì¸ë±ìŠ¤ ì—†ì´ CSVë¡œ ì €ì¥\n",
    "\n",
    "# ê²°ê³¼ ìš”ì•½ ì •ë³´ ì¶œë ¥\n",
    "print(\"ë³‘í•© ì™„ë£Œ: tweet_stock_classification.csv ì €ì¥ë¨\") # ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
