{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Finnhub API 기반 뉴스 데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 라이브러리 Import\n",
    "필요한 Python 라이브러리들을 가져옵니다. 웹 API 호출, 데이터 처리, 날짜 처리, 환경변수 관리 등에 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import\n",
    "import requests           # HTTP 요청을 위한 라이브러리 (Finnhub API 호출용)\n",
    "import pandas as pd       # 데이터 분석 및 처리를 위한 라이브러리 (DataFrame 사용)\n",
    "import datetime           # 날짜 및 시간 처리를 위한 라이브러리\n",
    "import os                 # 운영체제 관련 기능 (환경변수 접근용)\n",
    "from dotenv import load_dotenv  # .env 파일에서 환경변수 로드하는 라이브러리\n",
    "import time               # 시간 지연 처리를 위한 라이브러리 (API 호출 제한 관리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 환경 설정 로드\n",
    ".env 파일에서 Finnhub API 키를 로드합니다. API 키는 보안상 코드에 직접 입력하지 않고 환경변수로 관리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정 로드\n",
    "load_dotenv()  # 현재 디렉토리의 .env 파일에서 환경변수를 로드\n",
    "# .env 파일에서 'finhub' 키로 저장된 Finnhub API 키를 가져옴\n",
    "# 주의: 환경변수 이름이 'finnhub'가 아닌 'finhub'로 설정되어 있음\n",
    "FINHUB_API_KEY = os.getenv(\"finhub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. API 호출 제한 설정\n",
    "Finnhub API는 분당 호출 횟수에 제한이 있습니다. API 호출 간격을 조절하여 제한을 위반하지 않도록 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 호출 제한 설정\n",
    "# Finnhub Free Tier는 분당 60회 호출 제한이 있음\n",
    "API_CALLS_PER_MINUTE = 60\n",
    "\n",
    "# 각 API 호출 간의 최소 대기 시간 계산 (초 단위)\n",
    "# 60초 / 60회 = 1초 간격으로 호출하여 제한 내에서 안전하게 호출\n",
    "DELAY_BETWEEN_CALLS = 60.0 / API_CALLS_PER_MINUTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. 안전한 datetime 변환 함수\n",
    "Finnhub API에서 받은 timestamp를 pandas datetime으로 변환할 때 발생할 수 있는 오류를 방지하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 안전한 datetime 변환 함수\n",
    "def safe_datetime_conversion(timestamp):\n",
    "\n",
    "    # 빈 값이나 0값 체크\n",
    "    if not timestamp or timestamp == 0:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 유닉스 타임스탬프 범위 확인 (1970-01-01 이후만 허용)\n",
    "        # 음수값은 1970년 이전을 의미하므로 제외\n",
    "        if timestamp < 0:\n",
    "            return None\n",
    "            \n",
    "        # pandas에서 처리 가능한 최대 timestamp 확인\n",
    "        # 2262-04-11 이후는 pandas에서 overflow 발생\n",
    "        if timestamp > 9223372036:  # 약 2262-04-11 23:47:16\n",
    "            return None\n",
    "            \n",
    "        # Unix timestamp를 pandas datetime으로 변환\n",
    "        # unit='s'는 초 단위임을 의미\n",
    "        return pd.to_datetime(timestamp, unit='s')\n",
    "        \n",
    "    except (ValueError, OutOfBoundsDatetime, OverflowError):\n",
    "        # pandas에서 발생하는 날짜 범위 오류 처리\n",
    "        return None\n",
    "    except Exception:\n",
    "        # 기타 예상치 못한 오류 처리\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5. Finnhub 뉴스 수집 함수 (메인 함수)\n",
    "특정 기업의 뉴스를 대량으로 수집하는 핵심 함수입니다. 날짜 구간을 나누어 여러 번 API 호출하여 최대한 많은 뉴스를 수집합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finnhub 뉴스 수집 함수 (대량 수집 최적화)\n",
    "def fetch_finnhub_news_extended(symbol: str = \"GOOGL\", start_date: str = \"2025-06-14\", days_per_request: int = 30) -> pd.DataFrame:\n",
    "\n",
    "    # API 키가 없으면 빈 DataFrame 반환\n",
    "    if not FINHUB_API_KEY:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Finnhub Company News API 엔드포인트\n",
    "    url = \"https://finnhub.io/api/v1/company-news\"\n",
    "    \n",
    "    try:\n",
    "        # 현재 날짜 기준으로 수집 기간 설정\n",
    "        today = datetime.date.today()\n",
    "        \n",
    "        # 3년 전부터 현재까지 수집 시도 (Free Tier 제한 테스트)\n",
    "        three_years_ago = today - datetime.timedelta(days=1095)  # 1095일 = 약 3년\n",
    "        actual_start = three_years_ago  # 실제 수집 시작 날짜\n",
    "        actual_end = today             # 실제 수집 종료 날짜\n",
    "        \n",
    "    except ValueError:\n",
    "        # 날짜 처리 오류시 빈 DataFrame 반환\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 모든 수집된 기사를 저장할 리스트\n",
    "    all_articles = []\n",
    "    # 현재 처리 중인 날짜\n",
    "    current_date = actual_start\n",
    "    # API 호출 횟수 카운터\n",
    "    request_count = 0\n",
    "    \n",
    "    # 시작 날짜부터 종료 날짜까지 반복 처리\n",
    "    while current_date < actual_end:\n",
    "        # 각 요청의 종료 날짜 계산 (지정된 일수만큼 또는 전체 종료 날짜까지)\n",
    "        period_end = min(current_date + datetime.timedelta(days=days_per_request), actual_end)\n",
    "        \n",
    "        # API 요청 파라미터 설정\n",
    "        params = {\n",
    "            \"symbol\": symbol,                           # 주식 심볼\n",
    "            \"from\": current_date.isoformat(),           # 시작 날짜 (YYYY-MM-DD)\n",
    "            \"to\": period_end.isoformat(),               # 종료 날짜 (YYYY-MM-DD)\n",
    "            \"token\": FINHUB_API_KEY                     # API 인증 토큰\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            request_count += 1  # API 호출 횟수 증가\n",
    "            \n",
    "            # API 호출 제한을 위한 딜레이 (분당 60회 제한 준수)\n",
    "            time.sleep(DELAY_BETWEEN_CALLS)\n",
    "            \n",
    "            # HTTP GET 요청으로 뉴스 데이터 가져오기\n",
    "            res = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            # HTTP 429 (Too Many Requests) 오류 처리\n",
    "            if res.status_code == 429:\n",
    "                time.sleep(10)  # 10초 대기 후 재시도\n",
    "                res = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            # HTTP 403 (Forbidden) 오류시 수집 중단 (API 제한 도달)\n",
    "            if res.status_code == 403:\n",
    "                break\n",
    "            \n",
    "            # 기타 HTTP 오류 처리\n",
    "            if res.status_code != 200:\n",
    "                if res.status_code != 429:\n",
    "                    # 429가 아닌 오류는 다음 기간으로 건너뛰기\n",
    "                    current_date = period_end + datetime.timedelta(days=1)\n",
    "                    continue\n",
    "                else:\n",
    "                    # 429 오류는 30초 대기 후 재시도\n",
    "                    time.sleep(30)\n",
    "                    continue\n",
    "\n",
    "            # JSON 응답 데이터 파싱\n",
    "            data = res.json()\n",
    "            \n",
    "            # 응답이 리스트가 아닌 경우 (오류 응답 등) 처리\n",
    "            if not isinstance(data, list):\n",
    "                if isinstance(data, dict) and 'error' in data:\n",
    "                    # API 제한 관련 오류 메시지 확인\n",
    "                    if 'limit' in data['error'].lower():\n",
    "                        break  # 제한 도달시 수집 중단\n",
    "                # 다음 기간으로 이동\n",
    "                current_date = period_end + datetime.timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            # 해당 기간의 뉴스 데이터 처리\n",
    "            period_articles = []\n",
    "            for item in data:\n",
    "                # API에서 받은 timestamp를 안전하게 datetime으로 변환\n",
    "                pub_date = safe_datetime_conversion(item.get(\"datetime\"))\n",
    "                \n",
    "                # 뉴스 기사 정보를 딕셔너리로 구성\n",
    "                article = {\n",
    "                    \"id\": item.get(\"id\"),                           # 기사 고유 ID\n",
    "                    \"title\": item.get(\"headline\", \"\"),             # 기사 제목\n",
    "                    \"summary\": item.get(\"summary\", \"\"),            # 기사 요약\n",
    "                    \"link\": item.get(\"url\", \"\"),                   # 기사 URL\n",
    "                    \"publisher\": item.get(\"publisher\", \"\"),        # 발행사\n",
    "                    \"category\": item.get(\"category\", \"\"),          # 카테고리\n",
    "                    \"pubDate\": pub_date,                           # 발행 날짜\n",
    "                    \"image\": item.get(\"image\", \"\"),                # 이미지 URL\n",
    "                    \"related\": item.get(\"related\", \"\"),            # 관련 정보\n",
    "                    \"source\": item.get(\"source\", \"\"),              # 소스\n",
    "                    \"collection_period\": f\"{current_date.isoformat()}_{period_end.isoformat()}\"  # 수집 기간 정보\n",
    "                }\n",
    "                period_articles.append(article)\n",
    "            \n",
    "            # 현재 기간의 기사들을 전체 리스트에 추가\n",
    "            all_articles.extend(period_articles)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # 예외 발생시 API 제한 관련 오류인지 확인\n",
    "            if \"limit\" in str(e).lower() or \"403\" in str(e):\n",
    "                break  # API 제한 관련 오류시 수집 중단\n",
    "        \n",
    "        # 다음 기간으로 이동 (하루 간격으로 겹치지 않게)\n",
    "        current_date = period_end + datetime.timedelta(days=1)\n",
    "        \n",
    "        # API 호출 제한 방지를 위한 추가 대기\n",
    "        # 10회 호출마다 2초 추가 대기로 안전성 확보\n",
    "        if request_count % 10 == 0:\n",
    "            time.sleep(2)\n",
    "\n",
    "    # 수집된 모든 기사 데이터 처리\n",
    "    if all_articles:\n",
    "        # 리스트를 pandas DataFrame으로 변환\n",
    "        df = pd.DataFrame(all_articles)\n",
    "        \n",
    "        # 중복 기사 제거 (ID, 제목, 링크 기준으로 중복 판별)\n",
    "        df = df.drop_duplicates(subset=['id', 'title', 'link'])\n",
    "        \n",
    "        # 날짜순으로 정렬 (최신 기사가 위에 오도록)\n",
    "        # na_position='last'로 날짜가 없는 기사는 맨 아래로\n",
    "        df = df.sort_values('pubDate', ascending=False, na_position='last')\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # 수집된 기사가 없으면 빈 DataFrame 반환\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6. 수집 설정값 정의\n",
    "뉴스 수집을 위한 기본 설정값들을 정의합니다. 필요에 따라 이 값들을 수정하여 다른 기업이나 기간의 뉴스를 수집할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 수집을 위한 설정값 정의\n",
    "# 수집 대상 기업의 주식 심볼 (미국 상장 기업)\n",
    "# 예시: \"AAPL\"(Apple), \"GOOGL\"(Google), \"TSLA\"(Tesla), \"MSFT\"(Microsoft), \"AMZN\"(Amazon) 등\n",
    "target_symbol = \"GOOGL\"\n",
    "\n",
    "# 참고용 시작 날짜 (실제로는 3년 전부터 자동으로 수집됨)\n",
    "# YYYY-MM-DD 형식으로 입력\n",
    "start_date = \"2025-06-14\"\n",
    "\n",
    "# 한 번의 API 호출당 수집할 일수 설정\n",
    "# 값이 작을수록 더 많은 API 호출을 하게 되어 더 많은 뉴스를 수집할 수 있음\n",
    "# 하지만 API 제한에 도달할 가능성도 높아짐\n",
    "days_per_request = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-7. 뉴스 데이터 수집 실행\n",
    "위에서 정의한 설정값들을 사용하여 실제로 뉴스 데이터를 수집합니다. 이 과정에서 API 호출이 여러 번 발생하므로 시간이 걸릴 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 데이터 수집 실행\n",
    "# 위에서 정의한 함수와 설정값을 사용하여 뉴스 수집을 시작\n",
    "# 이 과정은 API 호출 제한으로 인해 시간이 걸릴 수 있음 (몇 분에서 몇십 분)\n",
    "df_extended_news = fetch_finnhub_news_extended(\n",
    "    symbol=target_symbol,           # 수집할 기업의 주식 심볼\n",
    "    start_date=start_date,          # 참고용 시작 날짜\n",
    "    days_per_request=days_per_request  # 한 번에 수집할 일수\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-8. 수집 결과 확인 및 기본 분석\n",
    "수집된 뉴스 데이터의 기본적인 통계 정보를 확인합니다. 총 기사 수, 수집 기간, 유효한 날짜 정보 등을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 수집 기사 수: 8563개\n",
      "수집 기간: 2024-06-24 ~ 2025-06-18\n",
      "유효한 날짜 기사: 8561개 / 전체 8563개\n"
     ]
    }
   ],
   "source": [
    "# 수집 결과 확인 및 기본 통계 분석\n",
    "# DataFrame이 비어있지 않은지 확인 (수집에 성공했는지 체크)\n",
    "if not df_extended_news.empty:\n",
    "    # 총 수집된 기사 수 출력\n",
    "    print(f\"총 수집 기사 수: {len(df_extended_news)}개\")\n",
    "    \n",
    "    # 날짜별 기사 분포 분석\n",
    "    if 'pubDate' in df_extended_news.columns:\n",
    "        # 유효한 날짜 정보가 있는 기사들만 필터링\n",
    "        valid_dates = df_extended_news[df_extended_news['pubDate'].notna()].copy()\n",
    "        \n",
    "        if not valid_dates.empty:\n",
    "            # 날짜만 추출 (시간 정보 제거)\n",
    "            valid_dates['date_only'] = valid_dates['pubDate'].dt.date\n",
    "            \n",
    "            # 날짜별 기사 수 집계 및 정렬\n",
    "            date_counts = valid_dates['date_only'].value_counts().sort_index()\n",
    "            \n",
    "            # 수집 기간 (가장 오래된 날짜 ~ 가장 최근 날짜) 출력\n",
    "            print(f\"수집 기간: {date_counts.index.min()} ~ {date_counts.index.max()}\")\n",
    "            \n",
    "            # 유효한 날짜가 있는 기사 수와 전체 기사 수 비교\n",
    "            print(f\"유효한 날짜 기사: {len(valid_dates)}개 / 전체 {len(df_extended_news)}개\")\n",
    "else:\n",
    "    # 수집에 실패한 경우 오류 메시지 출력\n",
    "    print(\"뉴스 수집에 실패했습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-9. 데이터 미리보기\n",
    "수집된 뉴스 데이터의 일부를 표 형태로 확인합니다. 제목, 발행처, 발행 날짜 등 주요 정보를 미리 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최신 뉴스 미리보기:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>pubDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>Amazon's Zoox, Alphabet's Waymo in NYC, Oracle...</td>\n",
       "      <td>Yahoo Finance's John Hyland examines some of t...</td>\n",
       "      <td>2025-06-18 17:16:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8370</th>\n",
       "      <td>Waymo applies for special permit to bring its ...</td>\n",
       "      <td>Waymo is doing what was once unthinkable: brin...</td>\n",
       "      <td>2025-06-18 17:03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>Waymo has set its robotaxi sights on NYC</td>\n",
       "      <td>Waymo said it's working on a return to New Yor...</td>\n",
       "      <td>2025-06-18 16:59:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>Uber and Lyft stock fall after Waymo applies f...</td>\n",
       "      <td>Investing.com -- Uber (NYSE:UBER) stock fell 2...</td>\n",
       "      <td>2025-06-18 16:44:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>VW to sell ID. Buzz robotaxis next year, takin...</td>\n",
       "      <td>Volkswagen-backed autonomous tech company MOIA...</td>\n",
       "      <td>2025-06-18 16:19:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "8369  Amazon's Zoox, Alphabet's Waymo in NYC, Oracle...   \n",
       "8370  Waymo applies for special permit to bring its ...   \n",
       "8371           Waymo has set its robotaxi sights on NYC   \n",
       "8373  Uber and Lyft stock fall after Waymo applies f...   \n",
       "8372  VW to sell ID. Buzz robotaxis next year, takin...   \n",
       "\n",
       "                                                summary             pubDate  \n",
       "8369  Yahoo Finance's John Hyland examines some of t... 2025-06-18 17:16:46  \n",
       "8370  Waymo is doing what was once unthinkable: brin... 2025-06-18 17:03:40  \n",
       "8371  Waymo said it's working on a return to New Yor... 2025-06-18 16:59:04  \n",
       "8373  Investing.com -- Uber (NYSE:UBER) stock fell 2... 2025-06-18 16:44:50  \n",
       "8372  Volkswagen-backed autonomous tech company MOIA... 2025-06-18 16:19:53  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 수집된 뉴스 데이터 미리보기\n",
    "# 데이터가 정상적으로 수집되었는지 확인\n",
    "if not df_extended_news.empty:\n",
    "    print(\"최신 뉴스 미리보기:\")\n",
    "    \n",
    "    # 주요 컬럼들만 선택하여 상위 5개 기사 표시\n",
    "    # title: 기사 제목, summary: 본문 내용, pubDate: 발행 날짜\n",
    "    preview_data = df_extended_news[['title', 'summary','pubDate']].head()\n",
    "    \n",
    "    # Jupyter Notebook에서 표 형태로 깔끔하게 출력\n",
    "    display(preview_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-10. CSV 파일로 저장\n",
    "수집된 뉴스 데이터를 CSV 파일로 저장합니다. 파일명에는 기업 심볼과 현재 날짜가 포함되어 구분하기 쉽게 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 저장 완료: GOOGL_extended_news_2025-06-13.csv\n"
     ]
    }
   ],
   "source": [
    "# 수집된 뉴스 데이터를 CSV 파일로 저장\n",
    "# 데이터가 있는 경우에만 저장 진행\n",
    "if not df_extended_news.empty:\n",
    "    # 현재 날짜를 ISO 형식 (YYYY-MM-DD)으로 가져오기\n",
    "    # today = datetime.date.today().isoformat()\n",
    "    \n",
    "    # 파일명 생성: \"기업심볼_extended_news_날짜.csv\" 형식\n",
    "    # 예: \"GOOGL_extended_news_2025-06-19.csv\"\n",
    "    filename = f\"{target_symbol}_extended_news_{start_date}.csv\"\n",
    "    \n",
    "    # DataFrame을 CSV 파일로 저장\n",
    "    # index=False: 행 번호를 파일에 포함하지 않음\n",
    "    # encoding='utf-8-sig': 한글 등 유니코드 문자가 깨지지 않도록 설정\n",
    "    df_extended_news.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 저장 완료 메시지 출력\n",
    "    print(f\"파일 저장 완료: {filename}\")\n",
    "else:\n",
    "    # 데이터가 없는 경우 경고 메시지 출력\n",
    "    print(\"저장할 데이터가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finnhub API 기반 뉴스 데이터 전처리 및 FinBERT 감정분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 파일 경로 설정\n",
    "분석할 뉴스 데이터 파일의 읽기 경로와 감정분석 결과를 저장할 경로를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 파일 경로: Finnhub에서 수집한 Apple 뉴스 데이터 CSV 파일\n",
    "read_path = \"../data/AAPL_extended_news_2025-06-14.csv\"\n",
    "\n",
    "# 출력 파일 경로: FinBERT 감정분석 결과가 추가된 CSV 파일\n",
    "write_path = \"../data/apple_finbert_finnhub.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 데이터 로드\n",
    "pandas를 사용하여 Finnhub에서 수집한 뉴스 데이터를 DataFrame으로 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분석을 위한 pandas 라이브러리 import\n",
    "import pandas as pd\n",
    "\n",
    "# CSV 파일에서 뉴스 데이터를 DataFrame으로 로드\n",
    "# 파일에는 id, title, summary, link, publisher, pubDate 등의 컬럼이 포함됨\n",
    "df = pd.read_csv(read_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. FinBERT 모델 설정 및 데이터 전처리\n",
    "금융 도메인에 특화된 FinBERT 모델을 로드하고, 뉴스 텍스트를 감정분석에 적합하게 전처리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FinBERT 전처리를 최소한으로 유지한 이유\n",
    "\n",
    "- 사전훈련 데이터: 수만 개의 금융 뉴스, 보고서, 소셜미디어 텍스트로 훈련\n",
    "- 도메인 특화 어휘: 금융 용어, 표현, 맥락을 이미 학습\n",
    "- 노이즈 처리 능력: 일반적인 텍스트 노이즈에 이미 강건함\n",
    "- 컨텍스트 이해: 문맥을 통한 의미 파악\n",
    "- 전이학습 효과: 대규모 사전훈련으로 일반화 능력 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정분석을 위한 transformers 라이브러리 import\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# FinBERT 모델 설정 - 금융 뉴스 감정분석에 특화된 사전훈련 모델\n",
    "MODEL = \"yiyanghkust/finbert-tone\"  # Hugging Face의 FinBERT-tone 모델\n",
    "\n",
    "# 토크나이저 로드 - 텍스트를 토큰으로 변환하는 도구\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# 사전훈련된 FinBERT 모델 로드 - 금융 텍스트의 감정을 분류하는 모델\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# 감정분석 파이프라인 생성\n",
    "# return_all_scores=True: positive, neutral, negative 모든 점수를 반환\n",
    "finbert = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "# 이전에 로드한 뉴스 데이터 사용\n",
    "df = pd.read_csv(read_path)\n",
    "\n",
    "# 빈 값(NaN)을 빈 문자열로 대체하여 처리 오류 방지\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# 분석할 텍스트 컬럼 생성 - 뉴스 제목을 감정분석 대상으로 설정\n",
    "# title 컬럼의 내용을 text 컬럼으로 복사\n",
    "df[\"text\"] = df[\"title\"] \n",
    "\n",
    "# 텍스트를 BERT 모델의 최대 토큰 길이에 맞게 자르고, \n",
    "# 원본이 길이 제한을 초과했는지 플래그로 표시하는 함수\n",
    "def truncate_and_flag(text, tokenizer, max_len=512):\n",
    "    \n",
    "    # 텍스트를 토큰으로 변환하면서 max_len에 맞게 자르기\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_len)\n",
    "    \n",
    "    # 잘린 토큰을 다시 텍스트로 변환\n",
    "    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # 원본 텍스트가 최대 길이를 초과했는지 확인\n",
    "    # 1: 초과함, 0: 초과하지 않음\n",
    "    over_flag = 1 if len(tokenizer.encode(text)) > max_len else 0\n",
    "    \n",
    "    # 잘린_텍스트, 초과_플래그 반환\n",
    "    return truncated_text, over_flag\n",
    "\n",
    "# 모든 텍스트에 대해 길이 제한 적용 및 초과 플래그 생성\n",
    "# apply()와 pd.Series()를 사용하여 함수 결과를 두 개의 컬럼으로 분할\n",
    "df[[\"text\", \"over_512\"]] = df[\"text\"].apply(\n",
    "    lambda x: pd.Series(truncate_and_flag(x, tokenizer, max_len=512))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. FinBERT 감정분석 실행\n",
    "전처리된 뉴스 텍스트에 대해 FinBERT 모델을 사용하여 감정분석을 수행하고 결과를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FinBERT 감정분석 실행을 위한 텍스트 리스트 준비\n",
    "# DataFrame의 text 컬럼을 파이썬 리스트로 변환\n",
    "texts = df[\"text\"].tolist()\n",
    "\n",
    "# FinBERT 모델로 배치 감정분석 수행\n",
    "# batch_size=8: 한 번에 8개씩 처리하여 메모리 효율성과 속도 최적화\n",
    "# GPU 메모리 부족시 더 작은 값(예: 4, 2)으로 조정 가능\n",
    "results = finbert(texts, batch_size=8)\n",
    "\n",
    "# 감정분석 결과를 저장할 빈 리스트들 초기화\n",
    "pos_scores, neu_scores, neg_scores = [], [], []\n",
    "\n",
    "# 각 텍스트의 감정분석 결과를 순회하며 점수 추출\n",
    "for res in results:\n",
    "    # 결과를 라벨:점수 형태의 딕셔너리로 변환\n",
    "    # 라벨을 소문자로 통일하여 일관성 확보\n",
    "    d = {r[\"label\"].lower(): r[\"score\"] for r in res}\n",
    "    \n",
    "    # 각 감정에 대한 점수를 리스트에 추가 (없으면 0.0으로 기본값 설정)\n",
    "    pos_scores.append(d.get(\"positive\", 0.0))   # 긍정 감정 점수\n",
    "    neu_scores.append(d.get(\"neutral\", 0.0))    # 중립 감정 점수  \n",
    "    neg_scores.append(d.get(\"negative\", 0.0))   # 부정 감정 점수\n",
    "\n",
    "# 감정분석 결과를 DataFrame에 새로운 컬럼으로 추가\n",
    "df[\"finbert_positive\"] = pos_scores   # FinBERT 긍정 점수 (0~1)\n",
    "df[\"finbert_neutral\"] = neu_scores    # FinBERT 중립 점수 (0~1)\n",
    "df[\"finbert_negative\"] = neg_scores   # FinBERT 부정 점수 (0~1)\n",
    "\n",
    "# 감정분석 결과가 추가된 DataFrame을 CSV 파일로 저장\n",
    "# index=False: 행 번호를 파일에 포함하지 않음\n",
    "df.to_csv(write_path, index=False)\n",
    "\n",
    "# 저장 완료 메시지 출력\n",
    "print(f\"결과 저장 완료: {write_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Rapid API 기반 트위터(X) 데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. 라이브러리 Import\n",
    "트윗 수집에 필요한 라이브러리들을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP 요청을 위한 라이브러리\n",
    "import requests\n",
    "\n",
    "# JSON 데이터 처리를 위한 라이브러리\n",
    "import json\n",
    "\n",
    "# CSV 파일 저장을 위한 라이브러리\n",
    "import csv\n",
    "\n",
    "# 파일 시스템 작업을 위한 라이브러리\n",
    "import os\n",
    "\n",
    "# 지연 시간 처리를 위한 라이브러리\n",
    "import time\n",
    "\n",
    "# 타입 힌팅을 위한 라이브러리\n",
    "from typing import List, Dict, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. 설정값 정의\n",
    "API 키, 사용자 ID, 수집할 트윗 수 등 크롤링에 필요한 설정값들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RapidAPI에서 발급받은 API 키 (보안상 실제 키는 환경변수 사용 권장)\n",
    "API_KEY = \"5fac920861msh988e449f8d91b60p10459bjsnba691d3d2d81\"\n",
    "\n",
    "# 수집할 사용자의 [ @이름 / ID(Twitter 사용자 고유 번호) / 설명 ]\n",
    "'''\n",
    "@WhiteHouse / 1879644163769335808 / 백악관 = 도널드 트럼프 대통령\n",
    "@SecScottBessent / 1889019333960998912 / 스콧베센트 재무장관\n",
    "@JDVance / 1542228578 / 밴스 부통령\n",
    "@marcorubio / 15745368 / 마르코 루비오 국무장관\n",
    "@elonmusk / 44196397 / 일론 머스크 테슬라 CEO\n",
    "@sundarpichai / 14130366 / 순다르 피차이 구글 CEO\n",
    "@tim_cook / 1636590253 / 팀 쿡 애플 CEO\n",
    "@CathieDWood / 2361631088 / ARK Invest CEO, 혁신 성장주 투자, 시장 트렌드 주도\n",
    "@BillAckman / 880412538625810432 / 빌 액먼 펀드매니저, 행동주의 투자 성향\n",
    "@RayDalio / 62603893 / 브리지워터 창립자, 거시경제 분석, 투자 전략가\n",
    "@michaelbatnick / 93529573 / 투자 분석, 금융 인사이트 제공\n",
    "@LizAnnSonders / 2961589380 / 찰스슈왑 수석 투자전략가, 시장 전망, 투자 전략\n",
    "@Ajay_Bagga / 86437069 / 글로벌 매크로 전문가, 시장 전망, 투자 전략\n",
    "'''\n",
    "USER_ID = \"86437069\"\n",
    "\n",
    "# 수집할 최대 트윗 수\n",
    "MAX_TWEETS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. RapidAPI Twitter 크롤러 클래스 정의\n",
    "트윗 수집 기능을 담은 클래스를 정의합니다. 모든 메서드가 클래스 내부에 포함되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RapidAPITweetCrawler:\n",
    "\n",
    "    # 크롤러 초기화 함수\n",
    "    def __init__(self, api_key: str):\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API 키가 제공되지 않았습니다.\")\n",
    "            \n",
    "        # RapidAPI 인증 정보 설정\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://twitter241.p.rapidapi.com/user-tweets\"\n",
    "        self.headers = {\n",
    "            \"x-rapidapi-key\": self.api_key,\n",
    "            \"x-rapidapi-host\": \"twitter241.p.rapidapi.com\"\n",
    "        }\n",
    "        \n",
    "        # API 요청당 트윗 수 설정 (최대 200까지 가능)\n",
    "        self.count_per_request = 200\n",
    "        \n",
    "        # cursor 중복 방지를 위한 캐시\n",
    "        self.used_cursors = set()\n",
    "\n",
    "    # API 응답 JSON에서 트윗 데이터를 파싱하는 함수\n",
    "    def _parse_tweets_from_response(self, response_json: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        tweets_data = []\n",
    "        \n",
    "        try:\n",
    "            # Twitter API의 timeline 구조에서 instructions 리스트를 찾습니다.\n",
    "            instructions = response_json.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "            \n",
    "            # 'TimelineAddEntries' 타입의 instruction에서 트윗 entries를 찾습니다.\n",
    "            timeline_entries = []\n",
    "            for instruction in instructions:\n",
    "                if instruction.get('type') == 'TimelineAddEntries':\n",
    "                    timeline_entries = instruction.get('entries', [])\n",
    "                    break\n",
    "            \n",
    "            if not timeline_entries:\n",
    "                return []\n",
    "\n",
    "            # 각 entry를 순회하며 트윗 데이터를 추출합니다.\n",
    "            for entry in timeline_entries:\n",
    "                # 'TimelineTweet' 타입의 콘텐츠만 처리\n",
    "                item_content = entry.get('content', {}).get('itemContent', {})\n",
    "                if item_content and item_content.get('itemType') == 'TimelineTweet':\n",
    "                    # 트윗 결과 데이터를 가져옵니다.\n",
    "                    tweet_results = item_content.get('tweet_results', {})\n",
    "                    result = tweet_results.get('result', {})\n",
    "                    \n",
    "                    # legacy 필드에 실제 트윗 데이터가 있습니다.\n",
    "                    legacy_data = result.get('legacy', {})\n",
    "                    \n",
    "                    if legacy_data:\n",
    "                        # 트윗 생성 시간 추출\n",
    "                        created_at = legacy_data.get('created_at', 'N/A')\n",
    "                        full_text = \"\"\n",
    "                        \n",
    "                        # 리트윗(RT)인 경우 원본 트윗의 full_text를 가져옵니다.\n",
    "                        if 'retweeted_status_result' in legacy_data:\n",
    "                            # 원본 트윗의 legacy 데이터를 찾습니다.\n",
    "                            original_tweet_legacy = legacy_data.get('retweeted_status_result', {}).get('result', {}).get('legacy', {})\n",
    "                            full_text = original_tweet_legacy.get('full_text', '')\n",
    "                        else:\n",
    "                            # 일반 트윗은 기존 방식대로 full_text를 가져옵니다.\n",
    "                            full_text = legacy_data.get('full_text', '')\n",
    "\n",
    "                        # 줄바꿈 문자를 공백으로 변환하고 양 끝 공백 제거\n",
    "                        full_text = full_text.replace('\\n', ' ').strip()\n",
    "                        \n",
    "                        # 추출한 데이터를 리스트에 추가\n",
    "                        tweets_data.append({\n",
    "                            'created_at': created_at,\n",
    "                            'full_text': full_text\n",
    "                        })\n",
    "        except (AttributeError, KeyError, IndexError):\n",
    "            # 데이터 파싱 중 오류 발생 시 빈 리스트 반환\n",
    "            pass\n",
    "            \n",
    "        return tweets_data\n",
    "\n",
    "    # API 응답에서 다음 페이지를 위한 cursor 값을 찾는 함수\n",
    "    def _find_next_cursor(self, response_json: Dict[str, Any]) -> Optional[str]:\n",
    "        try:\n",
    "            instructions = response_json.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "            \n",
    "            # 모든 instruction 타입에서 cursor 찾기\n",
    "            all_cursors = []\n",
    "            \n",
    "            for instruction in instructions:\n",
    "                # TimelineAddEntries에서 cursor 찾기\n",
    "                if instruction.get('type') == 'TimelineAddEntries':\n",
    "                    entries = instruction.get('entries', [])\n",
    "                    for entry in entries:\n",
    "                        content = entry.get('content', {})\n",
    "                        if content.get('entryType') == 'TimelineTimelineCursor':\n",
    "                            cursor_value = content.get('value')\n",
    "                            cursor_type = content.get('cursorType', '')\n",
    "                            \n",
    "                            if cursor_value and cursor_value not in self.used_cursors:\n",
    "                                all_cursors.append({\n",
    "                                    'value': cursor_value,\n",
    "                                    'type': cursor_type,\n",
    "                                    'priority': 1 if cursor_type == 'Bottom' else 2\n",
    "                                })\n",
    "                \n",
    "                # TimelineReplaceEntry에서도 cursor 찾기\n",
    "                elif instruction.get('type') == 'TimelineReplaceEntry':\n",
    "                    entry = instruction.get('entry', {})\n",
    "                    content = entry.get('content', {})\n",
    "                    if content.get('entryType') == 'TimelineTimelineCursor':\n",
    "                        cursor_value = content.get('value')\n",
    "                        cursor_type = content.get('cursorType', '')\n",
    "                        \n",
    "                        if cursor_value and cursor_value not in self.used_cursors:\n",
    "                            all_cursors.append({\n",
    "                                'value': cursor_value,\n",
    "                                'type': cursor_type,\n",
    "                                'priority': 1 if cursor_type == 'Bottom' else 2\n",
    "                            })\n",
    "            \n",
    "            # cursor를 우선순위에 따라 정렬 (Bottom이 우선)\n",
    "            if all_cursors:\n",
    "                all_cursors.sort(key=lambda x: x['priority'])\n",
    "                selected_cursor = all_cursors[0]['value']\n",
    "                self.used_cursors.add(selected_cursor)\n",
    "                return selected_cursor\n",
    "                \n",
    "        except (AttributeError, KeyError, IndexError):\n",
    "            pass\n",
    "            \n",
    "        return None\n",
    "\n",
    "    # 트윗 데이터 리스트와 파일 이름을 저장하는 함수    \n",
    "    def _save_to_csv(self, tweets_list: List[Dict[str, str]], filename: str):\n",
    "\n",
    "        try:\n",
    "            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "                # 'utf-8-sig'는 Excel에서 한글이 깨지지 않도록 BOM을 추가합니다.\n",
    "                writer = csv.DictWriter(f, fieldnames=['created_at', 'full_text'])\n",
    "                writer.writeheader()\n",
    "                writer.writerows(tweets_list)\n",
    "            print(f\"CSV 파일 저장 완료: {filename}\")\n",
    "        except IOError as e:\n",
    "            print(f\"파일 저장 중 오류 발생: {e}\")\n",
    "\n",
    "    # 특정 사용자의 트윗을 수집하여 CSV 파일로 저장하는 함수\n",
    "    def fetch_user_tweets(self, user_id: str, max_tweets: int = 1000):\n",
    "        all_tweets = []\n",
    "        cursor = None\n",
    "        request_count = 0\n",
    "        max_requests = 100  # 무한 루프 방지\n",
    "        consecutive_empty_responses = 0\n",
    "        \n",
    "        # cursor 캐시 초기화\n",
    "        self.used_cursors.clear()\n",
    "        \n",
    "        print(f\"사용자 ID {user_id}의 트윗 수집을 시작합니다...\")\n",
    "        \n",
    "        while len(all_tweets) < max_tweets and request_count < max_requests:\n",
    "            # count를 동적으로 조정 (남은 트윗 수에 따라)\n",
    "            remaining_tweets = max_tweets - len(all_tweets)\n",
    "            current_count = min(self.count_per_request, remaining_tweets)\n",
    "            \n",
    "            querystring = {\n",
    "                \"user\": user_id,\n",
    "                \"count\": str(current_count)\n",
    "            }\n",
    "            if cursor:\n",
    "                querystring[\"cursor\"] = cursor\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.base_url, headers=self.headers, params=querystring, timeout=45)\n",
    "                request_count += 1\n",
    "                \n",
    "                if response.status_code == 429:  # Rate limit\n",
    "                    print(\"API 요청 한도 초과, 60초 대기...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                elif response.status_code != 200:\n",
    "                    if response.status_code >= 500:  # 서버 에러인 경우 재시도\n",
    "                        print(f\"서버 오류 ({response.status_code}), 10초 후 재시도...\")\n",
    "                        time.sleep(10)\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(f\"API 요청 실패: HTTP {response.status_code}\")\n",
    "                        break\n",
    "                    \n",
    "                data = response.json()\n",
    "                \n",
    "                # 트윗 데이터 파싱\n",
    "                newly_fetched_tweets = self._parse_tweets_from_response(data)\n",
    "                \n",
    "                if not newly_fetched_tweets:\n",
    "                    consecutive_empty_responses += 1\n",
    "                    \n",
    "                    if consecutive_empty_responses >= 3:\n",
    "                        print(\"연속으로 빈 응답을 받아 수집을 종료합니다.\")\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_empty_responses = 0\n",
    "                \n",
    "                all_tweets.extend(newly_fetched_tweets)\n",
    "                \n",
    "                # 중복 제거 (created_at + full_text 기준)\n",
    "                seen = set()\n",
    "                unique_tweets = []\n",
    "                for tweet in all_tweets:\n",
    "                    tweet_key = (tweet['created_at'], tweet['full_text'])\n",
    "                    if tweet_key not in seen:\n",
    "                        seen.add(tweet_key)\n",
    "                        unique_tweets.append(tweet)\n",
    "                \n",
    "                all_tweets = unique_tweets\n",
    "                \n",
    "                # 다음 cursor 찾기\n",
    "                next_cursor = self._find_next_cursor(data)\n",
    "                if not next_cursor or next_cursor == cursor:\n",
    "                    print(\"더 이상 수집할 트윗이 없습니다.\")\n",
    "                    break\n",
    "                \n",
    "                cursor = next_cursor\n",
    "\n",
    "                # API rate limit를 고려한 대기 시간 (요청 수에 따라 조정)\n",
    "                if request_count % 10 == 0:  # 10번째마다 긴 대기\n",
    "                    wait_time = 5\n",
    "                else:\n",
    "                    wait_time = 1\n",
    "                    \n",
    "                time.sleep(wait_time)\n",
    "\n",
    "            except requests.exceptions.Timeout:\n",
    "                print(\"요청 타임아웃, 5초 후 재시도...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"요청 오류: {e}, 10초 후 재시도...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"JSON 파싱 오류, 5초 후 재시도...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "        if all_tweets:\n",
    "            # f\"user_@Ajay_Bagga_tweets.csv\"에서 @이름은 하드코딩으로 넣을것\n",
    "            # 예시1: f\"user_@Ajay_Bagga_tweets.csv\"\n",
    "            # 예시2: f\"user_@elonmusk_tweets.csv\"\n",
    "            filename = f\"user_@Ajay_Bagga_tweets.csv\"\n",
    "            self._save_to_csv(all_tweets, filename)\n",
    "            print(f\"총 {len(all_tweets)}개의 트윗을 수집했습니다.\")\n",
    "        else:\n",
    "            print(\"수집된 트윗이 없습니다.\")\n",
    "            \n",
    "        return all_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. 크롤러 인스턴스 생성 및 트윗 수집 실행\n",
    "설정된 API 키로 크롤러를 생성하고 트윗 수집을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자 ID 86437069의 트윗 수집을 시작합니다...\n",
      "CSV 파일 저장 완료: user_@Ajay_Bagga_tweets.csv\n",
      "총 586개의 트윗을 수집했습니다.\n",
      "총 586개의 트윗을 수집했습니다.\n",
      "결과는 user_@Ajay_Bagga_tweets.csv 파일에 저장되었습니다.\n",
      "\n",
      "=== 수집된 트윗 미리보기 (처음 3개) ===\n",
      "1. [Thu Jun 19 05:01:31 +0000 2025] Wars lead to value destruction in markets . Fortunately, history shows that geopolitical risk spikes...\n",
      "2. [Thu Jun 19 04:54:05 +0000 2025] Tanker rates double as shipowners steer clear of Strait of Hormuz...\n",
      "3. [Thu Jun 19 04:53:39 +0000 2025] Putin says solution to conflict is up to Iran and Israel...\n"
     ]
    }
   ],
   "source": [
    "# 크롤러 인스턴스 생성\n",
    "crawler = RapidAPITweetCrawler(api_key=API_KEY)\n",
    "\n",
    "# 트윗 수집 실행\n",
    "collected_tweets = crawler.fetch_user_tweets(user_id=USER_ID, max_tweets=MAX_TWEETS)\n",
    "\n",
    "# 수집 결과 요약 출력\n",
    "if collected_tweets:\n",
    "    print(f\"총 {len(collected_tweets)}개의 트윗을 수집했습니다.\")\n",
    "    print(f\"결과는 user_@Ajay_Bagga_tweets.csv 파일에 저장되었습니다.\")\n",
    "    \n",
    "    # 처음 3개 트윗 미리보기\n",
    "    print(\"\\n=== 수집된 트윗 미리보기 (처음 3개) ===\")\n",
    "    for i, tweet in enumerate(collected_tweets[:3], 1):\n",
    "        print(f\"{i}. [{tweet['created_at']}] {tweet['full_text'][:100]}...\")\n",
    "else:\n",
    "    print(\"수집된 트윗이 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RAPID API 기반 트위터(X) 데이터 전처리 및 VADER 감정분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. 필수 라이브러리 Import\n",
    "감정분석과 데이터 처리에 필요한 라이브러리들을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분석 및 조작을 위한 pandas 라이브러리\n",
    "import pandas as pd\n",
    "\n",
    "# VADER 감정분석 모델을 위한 NLTK 라이브러리\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# 정규표현식을 사용한 텍스트 전처리를 위한 re 라이브러리\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. VADER 감정분석 사전 다운로드\n",
    "VADER 감정분석에 필요한 어휘 사전을 다운로드합니다. (최초 1회만 실행)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\82102\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VADER 감정분석에 필요한 어휘 사전을 다운로드\n",
    "# vader_lexicon: 감정 점수가 매핑된 단어 사전 데이터\n",
    "# 최초 한 번만 실행하면 로컬에 저장됨\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. 다중 사용자 트윗 데이터 병합\n",
    "X_data 디렉토리의 모든 `user_*.csv` 파일들을 하나로 병합하여 통합 데이터셋을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 모든 CSV 파일: ['../data\\\\user_@Ajay_Bagga_tweets.csv', '../data\\\\user_@BillAckman_tweets.csv', '../data\\\\user_@CathieDWood_tweets.csv', '../data\\\\user_@elonmusk_tweets.csv', '../data\\\\user_@JDVance_tweets.csv', '../data\\\\user_@LizAnnSonders_tweets.csv', '../data\\\\user_@marcorubio_tweets.csv', '../data\\\\user_@michaelbatnick_tweets.csv', '../data\\\\user_@RayDalio_tweets.csv', '../data\\\\user_@SecScottBessent_tweets.csv', '../data\\\\user_@sundarpichai_tweets.csv', '../data\\\\user_@tim_cook_tweets.csv', '../data\\\\user_@WhiteHouse_tweets.csv']\n",
      "총 13개의 파일을 병합합니다.\n",
      "../data\\user_@Ajay_Bagga_tweets.csv 읽기 완료 - 586개 행\n",
      "../data\\user_@BillAckman_tweets.csv 읽기 완료 - 800개 행\n",
      "../data\\user_@CathieDWood_tweets.csv 읽기 완료 - 670개 행\n",
      "../data\\user_@elonmusk_tweets.csv 읽기 완료 - 764개 행\n",
      "../data\\user_@JDVance_tweets.csv 읽기 완료 - 701개 행\n",
      "../data\\user_@LizAnnSonders_tweets.csv 읽기 완료 - 673개 행\n",
      "../data\\user_@marcorubio_tweets.csv 읽기 완료 - 806개 행\n",
      "../data\\user_@michaelbatnick_tweets.csv 읽기 완료 - 760개 행\n",
      "../data\\user_@RayDalio_tweets.csv 읽기 완료 - 725개 행\n",
      "../data\\user_@SecScottBessent_tweets.csv 읽기 완료 - 255개 행\n",
      "../data\\user_@sundarpichai_tweets.csv 읽기 완료 - 585개 행\n",
      "../data\\user_@tim_cook_tweets.csv 읽기 완료 - 838개 행\n",
      "../data\\user_@WhiteHouse_tweets.csv 읽기 완료 - 651개 행\n",
      "🎉 총 8814개 행이 병합되었습니다!\n",
      "📊 포함된 사용자 수: 13\n"
     ]
    }
   ],
   "source": [
    "# 파일 시스템 작업에 필요한 추가 라이브러리 import\n",
    "import glob  # 파일 패턴 매칭을 위한 라이브러리\n",
    "import os    # 운영체제 인터페이스를 위한 라이브러리\n",
    "from datetime import datetime  # 날짜/시간 처리를 위한 라이브러리\n",
    "\n",
    "# glob 패턴을 사용하여 현재 디렉토리에서 \"user_\"로 시작하고 \".csv\"로 끝나는 모든 파일 찾기\n",
    "csv_files = glob.glob(\"../data/user_*.csv\")\n",
    "print(f\"발견된 모든 CSV 파일: {csv_files}\")\n",
    "print(f\"총 {len(csv_files)}개의 파일을 병합합니다.\")\n",
    "\n",
    "# 각 CSV 파일의 데이터프레임을 저장할 리스트 초기화\n",
    "dfs = []\n",
    "\n",
    "# 발견된 모든 CSV 파일을 순회하며 데이터 로드\n",
    "for file in csv_files:\n",
    "    # 파일이 실제로 존재하는지 확인\n",
    "    if os.path.exists(file):\n",
    "        # CSV 파일을 pandas 데이터프레임으로 읽기\n",
    "        temp_df = pd.read_csv(file)\n",
    "        \n",
    "        # 파일명에서 사용자명 추출 (예: \"user_@elonmusk_tweets.csv\" → \"@elonmusk\")\n",
    "        username = file.replace(\"user_\", \"\").replace(\"_tweets.csv\", \"\")\n",
    "        \n",
    "        # 각 행에 해당 사용자명을 컬럼으로 추가\n",
    "        temp_df['username'] = username\n",
    "        \n",
    "        # 리스트에 데이터프레임 추가\n",
    "        dfs.append(temp_df)\n",
    "        print(f\"{file} 읽기 완료 - {len(temp_df)}개 행\")\n",
    "\n",
    "# 모든 데이터프레임을 하나로 병합\n",
    "if dfs:\n",
    "    # concat 함수를 사용하여 세로로 병합하고 인덱스 리셋\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"🎉 총 {len(df)}개 행이 병합되었습니다!\")\n",
    "    print(f\"📊 포함된 사용자 수: {len(df['username'].unique())}\")\n",
    "else:\n",
    "    print(\"읽을 수 있는 파일이 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 시작 - VADER 감정분석 전처리를 최소한으로 유지한 이유\n",
    "\n",
    "- 소셜미디어 특화 설계\n",
    "- 감정 강화 표현 인식: 대문자, 반복 문자, 이모지 자동 처리\n",
    "- 구어체 친화적: 축약형, 속어, 인터넷 용어에 강함\n",
    "- 문맥 인식: 부정어, 강조어, 수식어 조합 이해\n",
    "- 과도한 전처리의 위험으로 인한 정보 손실 사례 방지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. URL 제거 함수 정의\n",
    "\n",
    "트윗 텍스트에서 URL을 제거하는 전처리 함수를 정의합니다. URL은 감정분석에 불필요한 노이즈입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    # 정규표현식을 사용하여 HTTP/HTTPS URL 패턴을 찾아서 공백으로 대체\n",
    "    # r'https?://\\S+' 패턴 설명:\n",
    "    # - https? : http 또는 https (? 는 s가 있거나 없거나)\n",
    "    # - :// : 프로토콜 구분자\n",
    "    # - \\S+ : 공백이 아닌 문자가 1개 이상 연속 (URL의 나머지 부분)\n",
    "    cleaned_text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # 앞뒤 공백 제거하여 반환\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. 트윗 텍스트에서 URL 제거 적용\n",
    "정의한 URL 제거 함수를 모든 트윗 텍스트에 적용하여 데이터를 정제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas의 apply() 함수를 사용하여 모든 트윗 텍스트에 URL 제거 함수 적용\n",
    "# apply()는 시리즈의 각 요소에 함수를 적용하여 새로운 시리즈를 반환\n",
    "df['full_text'] = df['full_text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-6. 빈 문자열을 NaN으로 변환\n",
    "URL 제거 후 빈 문자열이 된 텍스트들을 pandas의 NaN(결측값)으로 변환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_21124\\4143330987.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['full_text'].replace('', pd.NA, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 빈 문자열('')을 pandas의 NA(Not Available) 값으로 변환\n",
    "# replace() 함수: 첫 번째 인자를 두 번째 인자로 대체\n",
    "# inplace=True: 원본 데이터프레임을 직접 수정 (새로운 객체 생성 안 함)\n",
    "# pd.NA: pandas 2.0+에서 권장하는 결측값 표현\n",
    "df['full_text'].replace('', pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-7. 결측값(NaN) 제거\n",
    "감정분석이 불가능한 빈 텍스트나 결측값을 가진 행들을 데이터셋에서 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna() 함수를 사용하여 결측값을 가진 행들을 제거\n",
    "# subset=['full_text']: full_text 컬럼에서 NaN 값을 가진 행들만 제거\n",
    "# inplace=True: 원본 데이터프레임을 직접 수정\n",
    "# 감정분석을 위해서는 텍스트 내용이 필수이므로 빈 텍스트는 제거 필요\n",
    "df.dropna(subset=['full_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-8. 날짜 형식 변환 및 데이터 정렬\n",
    "트위터 API 형식의 날짜를 표준 날짜 형식으로 변환하고, 최신 순으로 정렬합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜 형식 변환 중...\n",
      "날짜 형식 변환 및 정렬 완료!\n",
      "날짜 범위: 2020-01-17 01:05:39 ~ 2025-06-19 05:01:31\n",
      "\n",
      "결과 확인:\n",
      "           created_at                                          full_text  \\\n",
      "0 2025-06-19 05:01:31  Wars lead to value destruction in markets . Fo...   \n",
      "1 2025-06-19 04:54:05  Tanker rates double as shipowners steer clear ...   \n",
      "2 2025-06-19 04:53:39  Putin says solution to conflict is up to Iran ...   \n",
      "3 2025-06-19 04:53:16  Trump says he ‘may or may not’ strike Iran US ...   \n",
      "4 2025-06-19 04:53:01  Was travelling and so could not cover the FOMC...   \n",
      "\n",
      "              username  \n",
      "0  ../data\\@Ajay_Bagga  \n",
      "1  ../data\\@Ajay_Bagga  \n",
      "2  ../data\\@Ajay_Bagga  \n",
      "3  ../data\\@Ajay_Bagga  \n",
      "4  ../data\\@Ajay_Bagga  \n"
     ]
    }
   ],
   "source": [
    "def convert_date_format(date_str):\n",
    "\n",
    "    try:\n",
    "        # 트위터 API 날짜 형식: \"Mon Jun 16 02:50:54 +0000 2025\"\n",
    "        # pd.to_datetime으로 파싱하고 strftime으로 원하는 형식으로 변환\n",
    "        # format 매개변수 설명:\n",
    "        # %a: 축약된 요일명 (Mon, Tue, ...)\n",
    "        # %b: 축약된 월명 (Jan, Feb, ...)  \n",
    "        # %d: 일 (01-31)\n",
    "        # %H:%M:%S: 시:분:초\n",
    "        # %z: 타임존 오프셋 (+0000)\n",
    "        # %Y: 4자리 연도\n",
    "        dt = pd.to_datetime(date_str, format='%a %b %d %H:%M:%S %z %Y')\n",
    "        \n",
    "        # 타임존 제거하고 \"YYYY-MM-DD HH:MM:SS\" 형식으로 변환\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        # 파싱 실패 시 원본 그대로 반환\n",
    "        return date_str\n",
    "\n",
    "# 날짜 형식 변환 프로세스 시작\n",
    "print(\"날짜 형식 변환 중...\")\n",
    "\n",
    "# 모든 트윗의 created_at 컬럼에 날짜 변환 함수 적용\n",
    "df['created_at'] = df['created_at'].apply(convert_date_format)\n",
    "\n",
    "# 문자열을 pandas datetime 객체로 변환 (정렬 및 시간 연산을 위해)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# 최신 트윗이 맨 위에 오도록 내림차순 정렬\n",
    "# ascending=False: 내림차순 (큰 값부터 작은 값 순)\n",
    "# reset_index(drop=True): 정렬 후 인덱스를 0부터 다시 시작\n",
    "df = df.sort_values(by='created_at', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"날짜 형식 변환 및 정렬 완료!\")\n",
    "print(f\"날짜 범위: {df['created_at'].min()} ~ {df['created_at'].max()}\")\n",
    "print(\"\\n결과 확인:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-9. VADER 감정분석기 초기화\n",
    "VADER(Valence Aware Dictionary and sEntiment Reasoner) 감정분석 모델을 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER 감정분석기 인스턴스 생성\n",
    "# SentimentIntensityAnalyzer: VADER 알고리즘을 구현한 클래스\n",
    "# - 소셜 미디어 텍스트에 특화된 감정분석 도구\n",
    "# - 이모티콘, 대문자, 구두점, 단어 조합 등을 고려하여 감정 점수 계산\n",
    "# - positive, negative, neutral, compound 점수를 반환\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-10. 감정분석 함수 정의\n",
    "텍스트를 분석하여 positive/negative/neutral로 분류하고 각 감정 점수를 반환하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    # VADER 감정분석기로 텍스트 분석\n",
    "    # polarity_scores() 반환값:\n",
    "    # - 'neg': negative 감정 점수 (0~1)\n",
    "    # - 'neu': neutral 감정 점수 (0~1)  \n",
    "    # - 'pos': positive 감정 점수 (0~1)\n",
    "    # - 'compound': 복합 점수 (-1~1, 전체적인 감정 강도)\n",
    "    scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # compound 점수를 기준으로 감정 분류\n",
    "    compound = scores['compound']\n",
    "    \n",
    "    # VADER 권장 임계값을 사용한 감정 분류\n",
    "    if compound >= 0.05:\n",
    "        # compound >= 0.05: 긍정적 감정\n",
    "        sentiment = 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        # compound <= -0.05: 부정적 감정\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        # -0.05 < compound < 0.05: 중립적 감정\n",
    "        sentiment = 'neutral'\n",
    "    \n",
    "    # pandas Series로 반환 (DataFrame의 새 컬럼들로 할당하기 위함)\n",
    "    return pd.Series([sentiment, scores['neg'], scores['neu'], scores['pos']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-11. 모든 트윗에 감정분석 실행\n",
    "정의한 감정분석 함수를 모든 트윗 텍스트에 적용하여 감정 컬럼들을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas의 apply() 함수를 사용하여 모든 트윗에 감정분석 함수 적용\n",
    "# analyze_sentiment() 함수가 pd.Series를 반환하므로 \n",
    "# 여러 컬럼에 동시에 할당 가능\n",
    "# - sentiment: 감정 분류 라벨 ('positive', 'negative', 'neutral')\n",
    "# - neg: 부정 감정 점수 (0~1)\n",
    "# - neu: 중립 감정 점수 (0~1)\n",
    "# - pos: 긍정 감정 점수 (0~1)\n",
    "df[['sentiment', 'neg', 'neu', 'pos']] = df['full_text'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-12. 최종 데이터 컬럼 선택\n",
    "분석에 필요한 핵심 컬럼들만 선택하여 최종 데이터셋을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 분석 결과에 필요한 컬럼들만 선택\n",
    "# - created_at: 트윗 작성 날짜/시간\n",
    "# - full_text: 전처리된 트윗 텍스트 (URL 제거됨)\n",
    "# - username: 트윗 작성자 (@사용자명)\n",
    "# - sentiment: 감정 분류 결과 (positive/negative/neutral)\n",
    "# - neg: 부정 감정 점수\n",
    "# - neu: 중립 감정 점수  \n",
    "# - pos: 긍정 감정 점수\n",
    "df = df[['created_at', 'full_text', 'username', 'sentiment', 'neg', 'neu', 'pos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-13. 최종 결과 저장 및 요약\n",
    "감정분석이 완료된 트윗 데이터를 CSV 파일로 저장하고 처리 결과를 요약합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 결과가 merged_tweets_with_sentiment.csv에 저장되었습니다.\n",
      "📊 총 8814개의 트윗이 처리되었습니다.\n",
      "👥 포함된 사용자: ['../data\\\\@Ajay_Bagga', '../data\\\\@BillAckman', '../data\\\\@CathieDWood', '../data\\\\@JDVance', '../data\\\\@LizAnnSonders', '../data\\\\@RayDalio', '../data\\\\@SecScottBessent', '../data\\\\@WhiteHouse', '../data\\\\@elonmusk', '../data\\\\@marcorubio', '../data\\\\@michaelbatnick', '../data\\\\@sundarpichai', '../data\\\\@tim_cook']\n",
      "📅 날짜 범위: 2020-01-17 01:05:39 ~ 2025-06-19 05:01:31\n",
      "\n",
      "📈 감정 분류별 통계:\n",
      "  - positive: 4632개 (52.6%)\n",
      "  - negative: 2204개 (25.0%)\n",
      "  - neutral: 1978개 (22.4%)\n",
      "\n",
      "🏆 사용자별 트윗 수 (상위 5명):\n",
      "  - ../data\\@tim_cook: 838개\n",
      "  - ../data\\@marcorubio: 806개\n",
      "  - ../data\\@BillAckman: 800개\n",
      "  - ../data\\@elonmusk: 764개\n",
      "  - ../data\\@michaelbatnick: 760개\n"
     ]
    }
   ],
   "source": [
    "# 결과 파일 저장\n",
    "output_filename = \"merged_tweets_with_sentiment.csv\"\n",
    "\n",
    "# 감정분석이 완료된 데이터프레임을 CSV 파일로 저장\n",
    "# index=False: 행 인덱스를 파일에 포함하지 않음\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "# 처리 결과 요약 정보 출력\n",
    "print(f\"✅ 결과가 {output_filename}에 저장되었습니다.\")\n",
    "print(f\"📊 총 {len(df)}개의 트윗이 처리되었습니다.\")\n",
    "\n",
    "# 감정분석에 포함된 사용자 목록 (알파벳 순 정렬)\n",
    "print(f\"👥 포함된 사용자: {sorted(df['username'].unique())}\")\n",
    "\n",
    "# 트윗 데이터의 시간 범위\n",
    "print(f\"📅 날짜 범위: {df['created_at'].min()} ~ {df['created_at'].max()}\")\n",
    "\n",
    "# 감정 분류별 트윗 수 통계\n",
    "print(f\"\\n📈 감정 분류별 통계:\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  - {sentiment}: {count}개 ({percentage:.1f}%)\")\n",
    "\n",
    "# 사용자별 트윗 수 통계 (상위 5명)\n",
    "print(f\"\\n🏆 사용자별 트윗 수 (상위 5명):\")\n",
    "user_counts = df['username'].value_counts().head(5)\n",
    "for username, count in user_counts.items():\n",
    "    print(f\"  - {username}: {count}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Y finance API 기반 주가 데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. 필수 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. RSI 계산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(prices, window=14):\n",
    "    \n",
    "    # 전일 대비 가격 변화량 계산\n",
    "    delta = prices.diff()\n",
    "    \n",
    "    # 상승일의 가격 상승폭만 추출 (하락일은 0으로 처리)\n",
    "    # where 조건: delta > 0인 경우만 해당 값 사용, 나머지는 0\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    \n",
    "    # 하락일의 가격 하락폭만 추출 (상승일은 0으로 처리)\n",
    "    # -delta로 하락폭을 양수로 변환\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    \n",
    "    # RS (Relative Strength) = 평균 상승폭 / 평균 하락폭\n",
    "    rs = gain / loss\n",
    "    \n",
    "    # RSI = 100 - (100 / (1 + RS))\n",
    "    # RSI가 70 이상이면 과매수, 30 이하면 과매도 구간으로 해석\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    return rsi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. 시간 조정 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_time_to_hour(df):\n",
    "    \n",
    "    # Datetime 컬럼이 존재하는지 확인\n",
    "    if 'Datetime' in df.columns:\n",
    "        # 문자열을 pandas datetime 객체로 변환\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        \n",
    "        # 시간을 시간 단위로 내림 (분, 초를 0으로 만듦)\n",
    "        # floor('H'): 시간 단위로 내림 처리\n",
    "        # 예: 2024-01-15 13:30:45 → 2024-01-15 13:00:00\n",
    "        df['Datetime'] = df['Datetime'].dt.floor('H')\n",
    "        \n",
    "        # 동일한 시간이 여러 개 있는 경우 마지막 값만 유지\n",
    "        # keep='last': 중복된 값 중 마지막 행을 유지\n",
    "        df = df.drop_duplicates(subset=['Datetime'], keep='last')\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4. 기술적 지표 추가 함수 (1시간 간격용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_features(df):\n",
    "    \n",
    "    # === 수익률 계산 ===\n",
    "    # 전 시간 대비 수익률 계산 (Close[t] - Close[t-1]) / Close[t-1]\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    \n",
    "    # 로그 수익률 계산 ln(Close[t] / Close[t-1])\n",
    "    # 연속복리 개념으로, 작은 변화에서는 일반 수익률과 유사\n",
    "    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # === 이동평균 (Simple Moving Average) ===\n",
    "    # 단순 이동평균: 지정 기간의 평균 가격\n",
    "    df['SMA_10'] = df['Close'].rolling(window=10).mean()  # 10시간 평균\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()  # 20시간 평균\n",
    "    df['SMA_50'] = df['Close'].rolling(window=50).mean()  # 50시간 평균\n",
    "    \n",
    "    # === 지수이동평균 (Exponential Moving Average) ===\n",
    "    # 최근 데이터에 더 높은 가중치를 부여하는 이동평균\n",
    "    df['EMA_12'] = df['Close'].ewm(span=12).mean()  # 12시간 EMA\n",
    "    df['EMA_26'] = df['Close'].ewm(span=26).mean()  # 26시간 EMA\n",
    "    \n",
    "    # === MACD (Moving Average Convergence Divergence) ===\n",
    "    # 단기 EMA와 장기 EMA의 차이로 추세 변화를 파악\n",
    "    df['MACD'] = df['EMA_12'] - df['EMA_26']  # MACD 라인\n",
    "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()  # 시그널 라인 (9시간 EMA)\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']  # 히스토그램 (매수/매도 신호)\n",
    "    \n",
    "    # === RSI (Relative Strength Index) ===\n",
    "    # 과매수/과매도 상태를 나타내는 모멘텀 지표 (0~100)\n",
    "    df['RSI'] = calculate_rsi(df['Close'])\n",
    "    \n",
    "    # === 볼린저 밴드 (Bollinger Bands) ===\n",
    "    # 가격의 변동성을 기반으로 한 기술적 지표\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=20).mean()  # 중심선 (20시간 이동평균)\n",
    "    bb_std = df['Close'].rolling(window=20).std()  # 20시간 표준편차\n",
    "    df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)  # 상단 밴드 (평균 + 2*표준편차)\n",
    "    df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)  # 하단 밴드 (평균 - 2*표준편차)\n",
    "    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']  # 밴드 폭 (변동성 지표)\n",
    "    # 현재 가격이 밴드 내에서 어느 위치에 있는지 (0~1, 0.5가 중앙)\n",
    "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "    \n",
    "    # === 변동성 지표 ===\n",
    "    # 수익률의 표준편차로 가격 변동성 측정\n",
    "    df['Volatility_10'] = df['Returns'].rolling(window=10).std()  # 10시간 변동성\n",
    "    df['Volatility_20'] = df['Returns'].rolling(window=20).std()  # 20시간 변동성\n",
    "    \n",
    "    # === 가격 변화 지표 ===\n",
    "    # 시가 대비 종가 변화 (절대값)\n",
    "    df['Price_Change'] = df['Close'] - df['Open']\n",
    "    \n",
    "    # 시가 대비 종가 변화 (백분율)\n",
    "    df['Price_Change_Pct'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "    \n",
    "    # === High-Low 스프레드 ===\n",
    "    # 당일 최고가와 최저가의 차이 (일중 변동폭)\n",
    "    df['HL_Spread'] = df['High'] - df['Low']\n",
    "    \n",
    "    # 종가 대비 일중 변동폭 비율\n",
    "    df['HL_Spread_Pct'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
    "    \n",
    "    # === 시간 특성 ===\n",
    "    # 시간대별 패턴 분석을 위한 특성\n",
    "    df['Hour'] = df['Datetime'].dt.hour  # 시간 (0~23)\n",
    "    df['DayOfWeek'] = df['Datetime'].dt.dayofweek  # 요일 (0=월요일, 6=일요일)\n",
    "    df['Month'] = df['Datetime'].dt.month  # 월 (1~12)\n",
    "    df['Quarter'] = df['Datetime'].dt.quarter  # 분기 (1~4)\n",
    "    \n",
    "    # === 거래시간 분류 ===\n",
    "    # 미국 주식시장 기준 거래시간 분류\n",
    "    df['Is_Trading_Hours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 16)).astype(int)  # 정규 거래시간\n",
    "    df['Is_Market_Open'] = ((df['Hour'] >= 9) & (df['Hour'] < 16)).astype(int)     # 시장 개장시간\n",
    "    df['Is_Premarket'] = ((df['Hour'] >= 4) & (df['Hour'] < 9)).astype(int)       # 프리마켓 (4:00-9:30)\n",
    "    df['Is_Aftermarket'] = ((df['Hour'] >= 16) & (df['Hour'] <= 20)).astype(int)  # 애프터마켓 (16:00-20:00)\n",
    "    df['Is_Extended_Hours'] = (df['Is_Premarket'] | df['Is_Aftermarket']).astype(int)  # 연장거래시간\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-5. 1시간 간격 주식 데이터 수집 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hourly_stock_data(ticker, days=365, save_to_csv=True):\n",
    "    \n",
    "    try:\n",
    "        # yfinance 1시간 간격 데이터 제약사항 확인 (최대 730일)\n",
    "        if days > 730:\n",
    "            days = 730\n",
    "        \n",
    "        # 현재 날짜 기준으로 시작일과 종료일 계산\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        \n",
    "        # yfinance를 사용하여 주식 데이터 다운로드\n",
    "        stock_data = yf.download(\n",
    "            ticker,  # 티커 심볼\n",
    "            start=start_date.strftime('%Y-%m-%d'),  # 시작일 (YYYY-MM-DD 형식)\n",
    "            end=end_date.strftime('%Y-%m-%d'),      # 종료일 (YYYY-MM-DD 형식)\n",
    "            interval='1h',    # 1시간 간격\n",
    "            prepost=True,     # 시장 외 시간 데이터 포함 (프리마켓, 애프터마켓)\n",
    "            progress=False    # 진행상황 표시 안 함\n",
    "        )\n",
    "        \n",
    "        # 데이터가 비어있는 경우 None 반환\n",
    "        if stock_data.empty:\n",
    "            return None\n",
    "        \n",
    "        # 인덱스(날짜/시간)를 일반 컬럼으로 변환\n",
    "        stock_data = stock_data.reset_index()\n",
    "        \n",
    "        # === 컬럼명 정리 작업 ===\n",
    "        # yfinance의 다양한 반환 형식에 대응\n",
    "        \n",
    "        # 'Date' 컬럼이 있으면 'Datetime'으로 변경\n",
    "        if 'Date' in stock_data.columns:\n",
    "            stock_data = stock_data.rename(columns={'Date': 'Datetime'})\n",
    "        # 첫 번째 컬럼이 시간 데이터인 경우 'Datetime'으로 변경\n",
    "        elif stock_data.columns[0] not in ['Datetime', 'Date']:\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # === 멀티레벨 컬럼 처리 ===\n",
    "        # yfinance가 때로 계층적 컬럼 구조로 데이터를 반환하는 경우\n",
    "        if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "            new_columns = []\n",
    "            for col in stock_data.columns:\n",
    "                if isinstance(col, tuple):\n",
    "                    # 튜플의 첫 번째 요소가 시간 관련이면 'Datetime'으로\n",
    "                    if col[0] == 'Datetime' or 'Date' in str(col[0]):\n",
    "                        new_columns.append('Datetime')\n",
    "                    else:\n",
    "                        # 그 외는 첫 번째 요소만 사용 (Open, High, Low, Close, Volume)\n",
    "                        new_columns.append(col[0])\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "            stock_data.columns = new_columns\n",
    "        \n",
    "        # 최종적으로 'Datetime' 컬럼이 없으면 첫 번째 컬럼을 사용\n",
    "        if 'Datetime' not in stock_data.columns:\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # === 데이터 전처리 및 특성 추가 ===\n",
    "        # 시간을 정시로 조정 (예: 13:30 → 13:00)\n",
    "        stock_data = adjust_time_to_hour(stock_data)\n",
    "        \n",
    "        # 기술적 지표 계산 및 추가\n",
    "        stock_data = add_technical_features(stock_data)\n",
    "        \n",
    "        # === CSV 파일 저장 ===\n",
    "        if save_to_csv:\n",
    "            # 파일명 형식: {TICKER}_1hour_data_{DAYS}days.csv\n",
    "            filename = f\"{ticker}_1hour_data_{days}days.csv\"\n",
    "            stock_data.to_csv(filename, index=False)  # 인덱스 제외하고 저장\n",
    "        \n",
    "        return stock_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        # 에러 발생 시 None 반환\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-6. 다중 티커 1시간 데이터 수집 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_tickers_hourly(tickers, days=365, save_individual=True, save_combined=True):\n",
    "    \n",
    "    # 수집된 데이터를 저장할 딕셔너리 초기화\n",
    "    all_data = {}\n",
    "    \n",
    "    # 각 티커에 대해 순차적으로 데이터 수집\n",
    "    for ticker in tickers:\n",
    "        # 개별 티커 데이터 수집 (save_to_csv는 save_individual 설정에 따라)\n",
    "        data = get_hourly_stock_data(ticker, days=days, save_to_csv=save_individual)\n",
    "        \n",
    "        # 수집된 데이터가 유효한 경우 딕셔너리에 저장\n",
    "        if data is not None:\n",
    "            all_data[ticker] = data\n",
    "    \n",
    "    # === 통합 데이터 파일 생성 ===\n",
    "    if save_combined and all_data:\n",
    "        # 모든 티커 데이터를 하나로 합칠 빈 데이터프레임 생성\n",
    "        combined_data = pd.DataFrame()\n",
    "        \n",
    "        # 각 티커 데이터에 티커 컬럼 추가 후 통합\n",
    "        for ticker, data in all_data.items():\n",
    "            # 원본 데이터를 복사하여 수정\n",
    "            ticker_data = data.copy()\n",
    "            \n",
    "            # 티커 식별을 위한 'Ticker' 컬럼 추가\n",
    "            ticker_data['Ticker'] = ticker\n",
    "            \n",
    "            # 기존 통합 데이터에 현재 티커 데이터 추가\n",
    "            # ignore_index=True: 인덱스를 새로 생성 (연속적인 번호)\n",
    "            combined_data = pd.concat([combined_data, ticker_data], ignore_index=True)\n",
    "        \n",
    "        # 통합 데이터 CSV 파일 저장\n",
    "        combined_filename = f\"multiple_stocks_1hour_data_{days}days.csv\"\n",
    "        combined_data.to_csv(combined_filename, index=False)\n",
    "    \n",
    "    # 티커별 데이터가 담긴 딕셔너리 반환\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-7. 데이터 요약 분석 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_summary(data_dict):\n",
    "    \n",
    "    # 딕셔너리의 각 티커와 데이터에 대해 반복\n",
    "    for ticker, data in data_dict.items():\n",
    "        # 데이터가 유효한 경우에만 분석 진행\n",
    "        if data is not None:\n",
    "            # 전체 데이터에서 결측치(NaN) 개수 계산\n",
    "            # isnull(): 각 셀이 결측치인지 True/False 반환\n",
    "            # sum().sum(): 먼저 각 컬럼별 결측치 개수를 구한 후, 전체 합계 계산\n",
    "            missing_count = data.isnull().sum().sum()\n",
    "            \n",
    "            # 결과 출력 (데이터 포인트 개수와 결측치 개수)\n",
    "            # len(data): 총 행(데이터 포인트) 개수\n",
    "            # :,: 천 단위 구분자 추가 (예: 1000 → 1,000)\n",
    "            print(f\"{ticker}: {len(data):,}개 포인트, 결측치: {missing_count}개\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-8. 주요 종목 1시간 데이터 수집 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL: 3,905개 포인트, 결측치: 217개\n",
      "AMZN: 3,905개 포인트, 결측치: 217개\n",
      "TSLA: 3,905개 포인트, 결측치: 217개\n",
      "GOOGL: 3,905개 포인트, 결측치: 217개\n",
      "MSFT: 3,905개 포인트, 결측치: 217개\n",
      "\\n데이터 수집 완료: 5개 종목\n"
     ]
    }
   ],
   "source": [
    "# === 수집 대상 주요 종목 설정 ===\n",
    "# 대형주 기술주 중심으로 선정 (시가총액, 거래량, 변동성 고려)\n",
    "# - AAPL: 애플 (아이폰, 맥북 등 하드웨어)\n",
    "# - AMZN: 아마존 (전자상거래, 클라우드 서비스)\n",
    "# - TSLA: 테슬라 (전기자동차, 에너지)\n",
    "# - GOOGL: 구글 (검색엔진, 광고, 클라우드)\n",
    "# - MSFT: 마이크로소프트 (윈도우, 오피스, 클라우드)\n",
    "tickers = ['AAPL', 'AMZN', 'TSLA', 'GOOGL', 'MSFT']\n",
    "\n",
    "# === 1시간 간격 데이터 수집 실행 ===\n",
    "# days=365: 1년간의 데이터 수집 (충분한 학습 데이터 확보)\n",
    "# save_individual=True: 각 종목별 개별 CSV 파일 생성\n",
    "# save_combined=True: 모든 종목을 통합한 CSV 파일도 생성\n",
    "all_stock_data = get_multiple_tickers_hourly(tickers, days=365)\n",
    "\n",
    "# === 수집 결과 요약 분석 ===\n",
    "# 각 종목별 데이터 포인트 개수와 결측치 개수 확인\n",
    "analyze_data_summary(all_stock_data)\n",
    "\n",
    "# 수집 완료된 종목 개수 출력\n",
    "print(f\"\\\\n데이터 수집 완료: {len(all_stock_data)}개 종목\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-9. AAPL 데이터 상세 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL 1시간 데이터 미리보기:\n",
      "                   Datetime    Open    High     Low   Close  Volume\n",
      "0 2024-06-20 08:00:00+00:00  214.74  215.26  214.20  214.83       0\n",
      "1 2024-06-20 09:00:00+00:00  214.80  214.95  214.50  214.70       0\n",
      "2 2024-06-20 10:00:00+00:00  214.69  214.75  214.50  214.67       0\n",
      "3 2024-06-20 11:00:00+00:00  214.65  214.69  213.67  213.91       0\n",
      "4 2024-06-20 12:00:00+00:00  213.90  215.30  213.37  213.94       0\n",
      "\\n학습용 데이터 분석:\n",
      "전체 시간 개수: 3,905개\n",
      "거래시간 개수: 1,999개\n",
      "기술적 지표 컬럼: ['SMA_10', 'SMA_20', 'MACD', 'RSI', 'BB_Upper', 'BB_Lower']\n",
      "\\n기술적 지표별 결측치:\n",
      "  SMA_10: 9개\n",
      "  SMA_20: 19개\n",
      "  RSI: 13개\n",
      "  BB_Upper: 19개\n",
      "  BB_Lower: 19개\n",
      "\\n데이터 기간:\n",
      "시작: 2024-06-20 08:00:00+00:00\n",
      "종료: 2025-06-18 23:00:00+00:00\n",
      "총 기간: 363일\n"
     ]
    }
   ],
   "source": [
    "# === AAPL 데이터 존재 여부 확인 및 상세 분석 ===\n",
    "if 'AAPL' in all_stock_data:\n",
    "    # AAPL 1시간 데이터를 변수에 저장\n",
    "    aapl_1h = all_stock_data['AAPL']\n",
    "    \n",
    "    # === 기본 데이터 미리보기 ===\n",
    "    print(\"AAPL 1시간 데이터 미리보기:\")\n",
    "    # 핵심 OHLCV 컬럼만 선택하여 상위 5개 행 출력\n",
    "    # OHLCV: Open(시가), High(고가), Low(저가), Close(종가), Volume(거래량)\n",
    "    print(aapl_1h[['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']].head())\n",
    "    \n",
    "    # === 거래시간 데이터 필터링 ===\n",
    "    # LSTM 학습에는 정규 거래시간 데이터가 더 안정적\n",
    "    # Is_Trading_Hours == 1: 미국 시장 정규 거래시간 (9:30-16:00 ET)\n",
    "    trading_hours = aapl_1h[aapl_1h['Is_Trading_Hours'] == 1]\n",
    "    \n",
    "    # === 학습용 데이터 분석 ===\n",
    "    print(f\"\\\\n학습용 데이터 분석:\")\n",
    "    print(f\"전체 시간 개수: {len(aapl_1h):,}개\")  # 프리마켓, 애프터마켓 포함\n",
    "    print(f\"거래시간 개수: {len(trading_hours):,}개\")  # 정규 거래시간만\n",
    "    \n",
    "    # === 기술적 지표 컬럼 확인 ===\n",
    "    # LSTM 모델의 입력 특성으로 사용할 주요 기술적 지표들\n",
    "    tech_indicators = [col for col in aapl_1h.columns if col in ['SMA_10', 'SMA_20', 'RSI', 'MACD', 'BB_Upper', 'BB_Lower']]\n",
    "    print(f\"기술적 지표 컬럼: {tech_indicators}\")\n",
    "    \n",
    "    # === 데이터 품질 분석 ===\n",
    "    # 결측치가 있는 기술적 지표 확인\n",
    "    missing_indicators = aapl_1h[tech_indicators].isnull().sum()\n",
    "    print(f\"\\\\n기술적 지표별 결측치:\")\n",
    "    for indicator, missing_count in missing_indicators.items():\n",
    "        if missing_count > 0:\n",
    "            print(f\"  {indicator}: {missing_count}개\")\n",
    "    \n",
    "    # === 데이터 범위 정보 ===\n",
    "    print(f\"\\\\n데이터 기간:\")\n",
    "    print(f\"시작: {aapl_1h['Datetime'].min()}\")\n",
    "    print(f\"종료: {aapl_1h['Datetime'].max()}\")\n",
    "    print(f\"총 기간: {(aapl_1h['Datetime'].max() - aapl_1h['Datetime'].min()).days}일\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-10. 데이터 수집 완료 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n 주식 데이터 수집 완료!\n",
      "수집된 종목: ['AAPL', 'AMZN', 'TSLA', 'GOOGL', 'MSFT']\n",
      "\\n 생성된 CSV 파일:\n",
      "  AAPL_1hour_data_365days.csv\n",
      "  AMZN_1hour_data_365days.csv\n",
      "  GOOGL_1hour_data_365days.csv\n",
      "  MSFT_1hour_data_365days.csv\n",
      "  TSLA_1hour_data_365days.csv\n"
     ]
    }
   ],
   "source": [
    "# === 데이터 수집 완료 최종 요약 ===\n",
    "print(\"\\\\n 주식 데이터 수집 완료!\")\n",
    "\n",
    "# 성공적으로 수집된 종목 목록 출력\n",
    "print(f\"수집된 종목: {list(all_stock_data.keys())}\")\n",
    "\n",
    "# === 생성된 CSV 파일 목록 확인 ===\n",
    "print(\"\\\\n 생성된 CSV 파일:\")\n",
    "\n",
    "# 현재 디렉토리의 파일 목록을 가져와서 CSV 파일만 필터링\n",
    "import os\n",
    "# 리스트 컴프리헨션으로 CSV 파일 중 수집 대상 티커가 포함된 파일만 선택\n",
    "csv_files = [f for f in os.listdir('.') if f.endswith('.csv') and any(ticker in f for ticker in tickers)]\n",
    "\n",
    "# 각 생성된 CSV 파일을 출력\n",
    "for file in csv_files:\n",
    "    print(f\"  {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 트윗 - 주가 데이터 병합 및 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 중: 0/8814 트윗\n",
      "처리 중: 1000/8814 트윗\n",
      "처리 중: 2000/8814 트윗\n",
      "처리 중: 3000/8814 트윗\n",
      "처리 중: 4000/8814 트윗\n",
      "처리 중: 5000/8814 트윗\n",
      "처리 중: 6000/8814 트윗\n",
      "처리 중: 7000/8814 트윗\n",
      "처리 중: 8000/8814 트윗\n",
      "병합 완료: tweet_stock_classification.csv 저장됨\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 트윗-주가 데이터 병합 및 라벨링 (시계열 특성 추출)\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd  # 데이터 조작 및 분석을 위한 pandas 라이브러리 임포트\n",
    "\n",
    "# 파일 경로 설정\n",
    "stock_path = \"./AAPL_1hour_data_365days.csv\"  # AAPL 1시간 단위 주가 데이터 파일 경로 (동일)\n",
    "tweets_path = \"./merged_tweets_with_sentiment.csv\"  # VADER 감정분석이 완료된 트윗 데이터 파일 경로\n",
    "\n",
    "# 데이터 불러오기 (날짜 컬럼을 자동으로 datetime 타입으로 변환)\n",
    "stock_df = pd.read_csv(stock_path, parse_dates=[\"Datetime\"])  # 주가 데이터 로드, Datetime 컬럼을 날짜 타입으로 파싱\n",
    "tweets_df = pd.read_csv(tweets_path, parse_dates=[\"created_at\"])  # 트윗 데이터 로드, created_at 컬럼을 날짜 타입으로 파싱\n",
    "\n",
    "# 타임존 제거 (서로 다른 타임존으로 인한 병합 문제 해결)\n",
    "stock_df[\"Datetime\"] = stock_df[\"Datetime\"].dt.tz_localize(None)  # 주가 데이터의 타임존 정보 제거 (UTC나 로컬 타임존 무관하게 만듦)\n",
    "tweets_df[\"created_at\"] = tweets_df[\"created_at\"].dt.tz_localize(None)  # 트윗 데이터의 타임존 정보 제거\n",
    "\n",
    "# 시계열 데이터 정렬 (시간 순서대로 정렬하여 과거/미래 탐색을 정확하게 함)\n",
    "stock_df = stock_df.sort_values(\"Datetime\").reset_index(drop=True)  # 날짜 순으로 정렬하고 인덱스 재설정\n",
    "tweets_df = tweets_df.sort_values(\"created_at\").reset_index(drop=True)  # 트윗도 시간순으로 정렬\n",
    "\n",
    "# 불필요한 거래 시간 관련 컬럼 제거 (모델 학습에 불필요한 boolean 변수들 제거)\n",
    "exclude_cols = ['Is_Trading_Hours', 'Is_Market_Open', 'Is_Premarket', 'Is_Aftermarket', 'Is_Extended_Hours']  # 제거할 컬럼 리스트\n",
    "stock_df = stock_df.drop(columns=[col for col in exclude_cols if col in stock_df.columns])  # 해당 컬럼들이 존재하면 제거\n",
    "\n",
    "# 병합 결과를 저장할 빈 리스트 초기화\n",
    "tweet_rows = []  # 각 트윗에 대한 특성과 라벨을 담을 딕셔너리들의 리스트\n",
    "\n",
    "# 각 트윗별로 반복 처리 (트윗 발생 시점 기준으로 주가 변화 패턴 추출)\n",
    "for idx, tweet_row in tweets_df.iterrows():  # 트윗 데이터프레임의 각 행을 순회\n",
    "    tweet_time = tweet_row['created_at']  # 현재 트윗의 생성 시간 추출\n",
    "    \n",
    "    if idx % 1000 == 0:  # 진행상황 출력 (1000개마다)\n",
    "        print(f\"처리 중: {idx}/{len(tweets_df)} 트윗\")\n",
    "\n",
    "    # 트윗 발행 이후 가장 가까운 미래 주가 데이터 찾기 (예측 타겟)\n",
    "    future_stock = stock_df[stock_df['Datetime'] > tweet_time].head(1)  # 트윗 시간 이후의 첫 번째 주가 데이터 선택\n",
    "    if future_stock.empty:  # 미래 주가 데이터가 없으면 (트윗이 가장 최근인 경우)\n",
    "        continue  # 해당 트윗은 건너뛰고 다음 트윗으로 진행\n",
    "\n",
    "    # 예측 타겟이 될 주가 정보 추출\n",
    "    target_row = future_stock.iloc[0]  # 미래 주가 데이터의 첫 번째 행\n",
    "    target_time = target_row['Datetime']  # 타겟 주가의 시간\n",
    "    target_close = target_row['Close']  # 타겟 주가의 종가 (예측해야 할 값)\n",
    "\n",
    "    # 타겟 시점 이전의 과거 3개 주가 데이터 추출 (시계열 특성으로 사용)\n",
    "    past_rows = stock_df[stock_df['Datetime'] < target_time].tail(3)  # 타겟 시간 이전의 마지막 3개 주가 데이터\n",
    "    if len(past_rows) < 3:  # 충분한 과거 데이터가 없으면 (데이터 초기 시점)\n",
    "        continue  # 해당 트윗은 건너뛰고 다음 트윗으로 진행\n",
    "\n",
    "    # 수익률 계산을 위한 기준점 (과거 데이터 중 가장 최근 종가)\n",
    "    past_last_close = past_rows.iloc[-1]['Close']  # 과거 3개 데이터 중 가장 최근 종가\n",
    "\n",
    "    # 수익률 계산 및 분류 라벨 생성\n",
    "    return_pct = (target_close - past_last_close) / past_last_close * 100  # 백분율 수익률 계산\n",
    "    # 3클래스 분류: 0.4% 이상 상승(1), -0.4% 이하 하락(-1), 그 외 보합(0)\n",
    "    label = 1 if return_pct >= 0.4 else (-1 if return_pct <= -0.4 else 0)  # 임계값 기반 라벨링\n",
    "\n",
    "    # 한 행의 데이터를 담을 딕셔너리 생성 (특성 + 라벨)\n",
    "    row = {\n",
    "        \"tweet_id\": idx,  # 트윗 인덱스 (고유 ID 역할)\n",
    "        \"tweet_time\": tweet_time,  # 트윗 생성 시간\n",
    "        \"target_close\": target_close,  # 예측 타겟 종가\n",
    "        \"target_return_pct\": return_pct,  # 실제 수익률 (연속값)\n",
    "        \"target_multi_raw\": label,  # 원본 분류 라벨 (-1, 0, 1)\n",
    "        \"vader_positive\": tweet_row['pos'],  # VADER 긍정 감정 점수\n",
    "        \"vader_neutral\": tweet_row['neu'],  # VADER 중립 감정 점수\n",
    "        \"vader_negative\": tweet_row['neg'],  # VADER 부정 감정 점수\n",
    "    }\n",
    "\n",
    "    # 과거 3개 주가 데이터를 평면화(flatten)하여 특성으로 추가\n",
    "    for i, (_, stock_row) in enumerate(past_rows.iterrows(), 1):  # 과거 3개 데이터를 순회 (1부터 시작하는 인덱스)\n",
    "        for col in stock_df.columns:  # 주가 데이터의 모든 컬럼에 대해\n",
    "            if col == \"Datetime\":  # 날짜 컬럼은 제외\n",
    "                continue\n",
    "            # x1_Open, x1_High, x2_Open, x2_High, x3_Open, x3_High 등의 형태로 특성 생성\n",
    "            row[f\"x{i}_{col}\"] = stock_row[col]  # i번째 과거 데이터의 각 컬럼값을 특성으로 추가\n",
    "\n",
    "    tweet_rows.append(row)  # 완성된 행 데이터를 리스트에 추가\n",
    "\n",
    "# 모든 행 데이터를 하나의 DataFrame으로 변환\n",
    "tweet_merged_df = pd.DataFrame(tweet_rows)  # 리스트의 딕셔너리들을 DataFrame으로 변환\n",
    "\n",
    "# 감정 라벨을 수치형으로 변환 (머신러닝 알고리즘을 위해)\n",
    "# sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}  # 감정 라벨을 숫자로 매핑\n",
    "# tweet_merged_df[\"sentiment_numeric\"] = tweet_merged_df[\"sentiment_label\"].map(sentiment_map)  # 수치형 감정 라벨 생성\n",
    "\n",
    "# 머신러닝 알고리즘을 위한 타겟 라벨 변환 (음수 라벨을 0부터 시작하도록 변경)\n",
    "label_map = {-1: 0, 0: 1, 1: 2}  # -1(하락)→0, 0(보합)→1, 1(상승)→2로 매핑\n",
    "tweet_merged_df[\"target_multi\"] = tweet_merged_df[\"target_multi_raw\"].map(label_map)  # 새로운 라벨 컬럼 생성\n",
    "\n",
    "# 최종 결과를 CSV 파일로 저장\n",
    "tweet_merged_df.to_csv(\"tweet_stock_classification.csv\", index=False)  # 인덱스 없이 CSV로 저장\n",
    "\n",
    "# 결과 요약 정보 출력\n",
    "print(\"병합 완료: tweet_stock_classification.csv 저장됨\") # 완료 메시지 출력\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
