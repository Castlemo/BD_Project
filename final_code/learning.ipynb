{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finnhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "# .env 설정 로드\n",
    "load_dotenv()\n",
    "FINHUB_API_KEY = os.getenv(\"finhub\")  # 변수명은 'finnhub' 아닌 'finhub'\n",
    "\n",
    "# API 호출 제한 (60 calls/minute)\n",
    "API_CALLS_PER_MINUTE = 60\n",
    "DELAY_BETWEEN_CALLS = 60.0 / API_CALLS_PER_MINUTE  # 1초 간격\n",
    "\n",
    "\n",
    "def safe_datetime_conversion(timestamp):\n",
    "    \"\"\"\n",
    "    타임스탬프를 안전하게 datetime으로 변환합니다.\n",
    "    Out of bounds nanosecond timestamp 오류를 방지합니다.\n",
    "    \"\"\"\n",
    "    if not timestamp or timestamp == 0:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 유닉스 타임스탬프 범위 확인 (1970-01-01 이후)\n",
    "        if timestamp < 0:\n",
    "            return None\n",
    "            \n",
    "        # 너무 큰 값 확인 (2262년 이후는 pandas에서 처리 불가)\n",
    "        if timestamp > 9223372036:  # 2262-04-11 정도\n",
    "            return None\n",
    "            \n",
    "        # 정상적인 변환 시도\n",
    "        return pd.to_datetime(timestamp, unit='s')\n",
    "        \n",
    "    except (ValueError, OutOfBoundsDatetime, OverflowError):\n",
    "        # 변환 실패 시 None 반환\n",
    "        return None\n",
    "    except Exception:\n",
    "        # 기타 예외 시 None 반환\n",
    "        return None\n",
    "\n",
    "\n",
    "# Finnhub에서 회사 뉴스 수집 (최적화된 버전 - 대량 수집)\n",
    "def fetch_finnhub_news_extended(symbol: str = \"GOOGL\", start_date: str = \"2025-06-14\", days_per_request: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    하나의 심볼에 대해 최대한 많은 뉴스를 수집합니다.\n",
    "    날짜 구간을 나누어서 여러 번 API 호출하여 더 많은 기사를 수집합니다.\n",
    "    \n",
    "    Args:\n",
    "        symbol: 주식 심볼 (예: \"AAPL\")\n",
    "        start_date: 시작 날짜 (YYYY-MM-DD)\n",
    "        days_per_request: 한 번의 API 호출당 수집할 일수 (기본: 30일)\n",
    "    \"\"\"\n",
    "    if not FINHUB_API_KEY:\n",
    "        print(\"[ERROR] Finnhub API 키가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    url = \"https://finnhub.io/api/v1/company-news\"\n",
    "    \n",
    "    try:\n",
    "        start_date_obj = datetime.datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "        today = datetime.date.today()\n",
    "        \n",
    "        print(f\"[INFO] 요청 시작 날짜: {start_date_obj}\")\n",
    "        print(f\"[INFO] 현재 날짜: {today}\")\n",
    "        \n",
    "        # Free Tier 제한을 넘어서 최대한 많이 수집 시도\n",
    "        two_years_ago = today - datetime.timedelta(days=730)  # 2년 전\n",
    "        three_years_ago = today - datetime.timedelta(days=1095)  # 3년 전\n",
    "        \n",
    "        print(f\"[INFO] 🚀 Free Tier 제한 돌파 시도: 2-3년간 뉴스 수집을 시도합니다!\")\n",
    "        print(f\"[INFO] 2년 전 날짜: {two_years_ago}\")\n",
    "        print(f\"[INFO] 3년 전 날짜: {three_years_ago}\")\n",
    "        \n",
    "        # 3년 전부터 현재까지로 시도 (API가 어디까지 허용하는지 테스트)\n",
    "        actual_start = three_years_ago\n",
    "        actual_end = today\n",
    "        \n",
    "        print(f\"[INFO] {symbol} 뉴스 대량 수집 시작 (제한 돌파 시도)\")\n",
    "        print(f\"[INFO] 실제 수집 기간: {actual_start.isoformat()} ~ {actual_end.isoformat()}\")\n",
    "        print(f\"[INFO] 수집 기간: {(actual_end - actual_start).days}일 (약 3년)\")\n",
    "        print(f\"[INFO] 예상 API 호출 횟수: {(actual_end - actual_start).days // days_per_request + 1}회\")\n",
    "        print(f\"[WARNING] API가 제한할 수 있습니다. 테스트 중...\")\n",
    "        \n",
    "    except ValueError:\n",
    "        print(f\"[ERROR] 잘못된 날짜 형식: {start_date}. YYYY-MM-DD 형식을 사용하세요.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_articles = []\n",
    "    current_date = actual_start\n",
    "    request_count = 0\n",
    "    \n",
    "    while current_date < actual_end:\n",
    "        # 각 요청의 종료 날짜 계산\n",
    "        period_end = min(current_date + datetime.timedelta(days=days_per_request), actual_end)\n",
    "        \n",
    "        params = {\n",
    "            \"symbol\": symbol,\n",
    "            \"from\": current_date.isoformat(),\n",
    "            \"to\": period_end.isoformat(),\n",
    "            \"token\": FINHUB_API_KEY\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            request_count += 1\n",
    "            print(f\"[INFO] API 호출 {request_count}: {current_date.isoformat()} ~ {period_end.isoformat()}\")\n",
    "            \n",
    "            # API 호출 제한을 위한 딜레이\n",
    "            time.sleep(DELAY_BETWEEN_CALLS)\n",
    "            \n",
    "            res = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if res.status_code == 429:\n",
    "                print(f\"[WARNING] API 호출 제한 도달. 더 긴 대기 후 재시도...\")\n",
    "                time.sleep(10)\n",
    "                res = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if res.status_code == 403:\n",
    "                print(f\"[ERROR] API 접근 거부 (기간: {current_date} ~ {period_end}): Free Tier 제한 도달 가능성\")\n",
    "                print(f\"[INFO] 현재까지 수집된 기사: {len(all_articles)}개\")\n",
    "                break\n",
    "            \n",
    "            if res.status_code != 200:\n",
    "                print(f\"[WARNING] API 요청 실패 (기간: {current_date} ~ {period_end}): HTTP {res.status_code}\")\n",
    "                print(f\"[INFO] 응답 내용: {res.text[:200]}...\")\n",
    "                \n",
    "                # 429 (Too Many Requests)가 아니라면 계속 진행\n",
    "                if res.status_code != 429:\n",
    "                    current_date = period_end + datetime.timedelta(days=1)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"[INFO] 호출 제한으로 인한 대기...\")\n",
    "                    time.sleep(30)\n",
    "                    continue\n",
    "\n",
    "            data = res.json()\n",
    "            \n",
    "            if not isinstance(data, list):\n",
    "                print(f\"[WARNING] 예상과 다른 응답 형식 (기간: {current_date} ~ {period_end})\")\n",
    "                if isinstance(data, dict) and 'error' in data:\n",
    "                    print(f\"[ERROR] API 오류: {data['error']}\")\n",
    "                    if 'limit' in data['error'].lower():\n",
    "                        print(f\"[INFO] Free Tier 제한 도달. 현재까지 수집: {len(all_articles)}개\")\n",
    "                        break\n",
    "                current_date = period_end + datetime.timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            # 해당 기간의 뉴스 수집\n",
    "            period_articles = []\n",
    "            for item in data:\n",
    "                # 안전한 datetime 변환 사용\n",
    "                pub_date = safe_datetime_conversion(item.get(\"datetime\"))\n",
    "                \n",
    "                article = {\n",
    "                    \"id\": item.get(\"id\"),\n",
    "                    \"title\": item.get(\"headline\", \"\"),\n",
    "                    \"summary\": item.get(\"summary\", \"\"),\n",
    "                    \"link\": item.get(\"url\", \"\"),\n",
    "                    \"publisher\": item.get(\"publisher\", \"\"),\n",
    "                    \"category\": item.get(\"category\", \"\"),\n",
    "                    \"pubDate\": pub_date,\n",
    "                    \"image\": item.get(\"image\", \"\"),\n",
    "                    \"related\": item.get(\"related\", \"\"),\n",
    "                    \"source\": item.get(\"source\", \"\"),\n",
    "                    \"collection_period\": f\"{current_date.isoformat()}_{period_end.isoformat()}\"\n",
    "                }\n",
    "                period_articles.append(article)\n",
    "            \n",
    "            all_articles.extend(period_articles)\n",
    "            print(f\"[SUCCESS] 기간별 수집: {len(period_articles)}개 기사 (총 {len(all_articles)}개)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] API 호출 오류 (기간: {current_date} ~ {period_end}): {e}\")\n",
    "            if \"limit\" in str(e).lower() or \"403\" in str(e):\n",
    "                print(f\"[INFO] Free Tier 제한 도달 가능성. 현재까지 수집: {len(all_articles)}개\")\n",
    "                break\n",
    "        \n",
    "        # 다음 기간으로 이동\n",
    "        current_date = period_end + datetime.timedelta(days=1)\n",
    "        \n",
    "        # API 호출 제한 방지를 위한 추가 대기\n",
    "        if request_count % 10 == 0:  # 10번 호출마다 추가 대기\n",
    "            print(f\"[INFO] API 제한 방지를 위한 대기... (현재까지 {len(all_articles)}개 수집)\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # 전체 결과 처리\n",
    "    if all_articles:\n",
    "        df = pd.DataFrame(all_articles)\n",
    "        \n",
    "        # 중복 제거 (ID, 제목, 링크 기준)\n",
    "        before_dedup = len(df)\n",
    "        df = df.drop_duplicates(subset=['id', 'title', 'link'])\n",
    "        after_dedup = len(df)\n",
    "        \n",
    "        if before_dedup != after_dedup:\n",
    "            print(f\"[INFO] 중복 기사 제거: {before_dedup - after_dedup}개\")\n",
    "        \n",
    "        # 날짜순 정렬 (최신순) - None 값 처리\n",
    "        df = df.sort_values('pubDate', ascending=False, na_position='last')\n",
    "        \n",
    "        print(f\"[SUCCESS] {symbol} 총 {len(df)}개의 뉴스 기사 수집 완료!\")\n",
    "        print(f\"[INFO] 총 API 호출 횟수: {request_count}회\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f\"[WARNING] {symbol}에 대한 뉴스를 찾을 수 없습니다.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# yfinance 주가 데이터\n",
    "def fetch_stock_data(ticker_symbol: str, period: str = '1y', interval: str = '1d') -> pd.DataFrame:\n",
    "    try:\n",
    "        df = yf.download(\n",
    "            tickers=ticker_symbol,\n",
    "            period=period,\n",
    "            interval=interval,\n",
    "            auto_adjust=False,\n",
    "            progress=False\n",
    "        )\n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "        df = df.reset_index()\n",
    "        df = df.rename(columns={'Adj Close': 'Adj_Close'})\n",
    "        return df[['Date', 'Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']]\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 주식 데이터 오류: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 대량 뉴스 수집할 단일 심볼 설정\n",
    "    target_symbol = \"GOOGL\"  # 원하는 심볼로 변경 가능 (예: MSFT, GOOGL, AMZN, TSLA, META, NVDA 등)\n",
    "    start_date = \"2025-06-13\"  # 참고용 (실제로는 1년 전부터 자동 수집)\n",
    "    days_per_request = 7  # 한 번의 API 호출당 7일씩 수집 (더 많은 API 호출로 최대 수집)\n",
    "    \n",
    "    print(f\"{target_symbol} 뉴스 대량 수집 시작!\")\n",
    "    print(f\"참고 날짜: {start_date} (실제로는 3년 전부터 수집 시도)\")\n",
    "    print(f\"수집 방식: {days_per_request}일씩 구간별 수집\")\n",
    "    print(f\"API 제한: {API_CALLS_PER_MINUTE}회/분\")\n",
    "    print(f\"목표:Free Tier 제한 돌파 시도 - 최대 3년간 뉴스 수집!\")\n",
    "    print(f\"주의: API가 제한을 걸 수 있으니 실험적 수집입니다.\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 확장된 뉴스 수집 함수 사용\n",
    "    df_extended_news = fetch_finnhub_news_extended(\n",
    "        symbol=target_symbol, \n",
    "        start_date=start_date,\n",
    "        days_per_request=days_per_request\n",
    "    )\n",
    "    \n",
    "    if not df_extended_news.empty:\n",
    "        print(f\"\\n🎉 {target_symbol} 뉴스 수집 완료!\")\n",
    "        print(f\"📰 총 수집 기사 수: {len(df_extended_news)}개\")\n",
    "        \n",
    "        # 날짜별 기사 분포 분석 (유효한 날짜만)\n",
    "        if 'pubDate' in df_extended_news.columns:\n",
    "            valid_dates = df_extended_news[df_extended_news['pubDate'].notna()].copy()\n",
    "            if not valid_dates.empty:\n",
    "                valid_dates['date_only'] = valid_dates['pubDate'].dt.date\n",
    "                date_counts = valid_dates['date_only'].value_counts().sort_index()\n",
    "                print(f\"📈 수집 기간: {date_counts.index.min()} ~ {date_counts.index.max()}\")\n",
    "                print(f\"📊 평균 일일 기사 수: {date_counts.mean():.1f}개\")\n",
    "                print(f\"📅 유효한 날짜 기사: {len(valid_dates)}개 / 전체 {len(df_extended_news)}개\")\n",
    "        \n",
    "        # 최신 기사 10개 미리보기\n",
    "        print(f\"\\n📰 최신 뉴스 미리보기 (상위 10개)\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, row in df_extended_news.head(10).iterrows():\n",
    "            pub_date = row['pubDate'].strftime('%Y-%m-%d %H:%M') if pd.notna(row['pubDate']) else 'N/A'\n",
    "            title = row['title'][:60] + \"...\" if len(row['title']) > 60 else row['title']\n",
    "            publisher = row['publisher'] if row['publisher'] else row['source']\n",
    "            print(f\"{i+1:2d}. [{publisher}] {title}\")\n",
    "            print(f\"    📅 {pub_date} | 🔗 {row['link'][:50]}...\")\n",
    "            print()\n",
    "        \n",
    "        # CSV 파일로 저장\n",
    "        today = datetime.date.today().isoformat()\n",
    "        filename = f\"{target_symbol}_extended_news_{today}.csv\"\n",
    "        df_extended_news.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"💾 파일 저장 완료: {filename}\")\n",
    "        \n",
    "        # 발행처별 통계\n",
    "        print(f\"\\n📊 발행처별 기사 수 통계 (상위 10개)\")\n",
    "        print(\"-\" * 40)\n",
    "        publisher_col = 'publisher' if df_extended_news['publisher'].notna().any() else 'source'\n",
    "        publisher_counts = df_extended_news[publisher_col].value_counts()\n",
    "        for publisher, count in publisher_counts.head(10).items():\n",
    "            if publisher:  # 빈 값이 아닌 경우만\n",
    "                print(f\"{publisher:25s}: {count:3d}개\")\n",
    "        \n",
    "        # 카테고리별 통계 (있는 경우)\n",
    "        if 'category' in df_extended_news.columns and df_extended_news['category'].notna().any():\n",
    "            print(f\"\\n🏷️  카테고리별 기사 수\")\n",
    "            print(\"-\" * 30)\n",
    "            category_counts = df_extended_news['category'].value_counts()\n",
    "            for category, count in category_counts.items():\n",
    "                if category:\n",
    "                    print(f\"{category:20s}: {count:3d}개\")\n",
    "                    \n",
    "    else:\n",
    "        print(f\"❌ {target_symbol} 뉴스 수집에 실패했습니다.\")\n",
    "        print(\"🔧 가능한 해결 방법:\")\n",
    "        print(\"   1. API 키 확인 (.env 파일의 'finhub' 변수)\")\n",
    "        print(\"   2. 인터넷 연결 확인\")  \n",
    "        print(\"   3. 심볼명 확인 (미국 상장 기업만 지원)\")\n",
    "        print(\"   4. 날짜 범위 조정\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📋 수집 완료 요약\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"🏢 대상 기업: {target_symbol}\")\n",
    "    print(f\"📅 수집 날짜: {start_date}\")\n",
    "    print(f\"📊 총 기사 수: {len(df_extended_news) if not df_extended_news.empty else 0}개\")\n",
    "    print(f\"💾 저장 파일: {filename if not df_extended_news.empty else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_hourly_stock_data(ticker, days=365, save_to_csv=True):\n",
    "    \"\"\"\n",
    "    티커를 입력받아 최근 N일간의 1시간 간격 주식 데이터를 가져오는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    ticker (str): 주식 티커 심볼 (예: 'AAPL', 'TSLA', 'AMZN')\n",
    "    days (int): 수집할 일수 (최대 730일, yfinance 제약)\n",
    "    save_to_csv (bool): CSV 파일로 저장할지 여부\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: 1시간 간격 주식 데이터\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # yfinance 1시간 간격 제약사항 확인\n",
    "        if days > 730:\n",
    "            print(f\"⚠️ yfinance 1시간 간격 데이터는 최대 730일까지만 지원됩니다.\")\n",
    "            print(f\"요청한 {days}일 → 730일로 조정합니다.\")\n",
    "            days = 730\n",
    "        \n",
    "        # 날짜 설정 (현재 날짜 기준)\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        \n",
    "        print(f\"📊 {ticker} 주식 데이터 수집 중...\")\n",
    "        print(f\"기간: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')} ({days}일)\")\n",
    "        print(f\"간격: 1시간\")\n",
    "        \n",
    "        # yfinance로 데이터 수집 (24시간 데이터 포함)\n",
    "        stock_data = yf.download(\n",
    "            ticker,\n",
    "            start=start_date.strftime('%Y-%m-%d'),\n",
    "            end=end_date.strftime('%Y-%m-%d'),\n",
    "            interval='1h',\n",
    "            prepost=True,  # 시장 외 시간 데이터 포함\n",
    "            progress=False\n",
    "        )\n",
    "        \n",
    "        if stock_data.empty:\n",
    "            print(f\"❌ {ticker}에 대한 데이터를 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        # 인덱스를 컬럼으로 변환\n",
    "        stock_data = stock_data.reset_index()\n",
    "        \n",
    "        # 컬럼명 확인 및 정리\n",
    "        print(f\"🔍 원본 컬럼명: {list(stock_data.columns)}\")\n",
    "        \n",
    "        # 인덱스 컬럼명 통일 (Datetime으로)\n",
    "        if 'Date' in stock_data.columns:\n",
    "            stock_data = stock_data.rename(columns={'Date': 'Datetime'})\n",
    "        elif stock_data.columns[0] not in ['Datetime', 'Date']:\n",
    "            # 첫 번째 컬럼이 시간 데이터인 경우\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # 멀티레벨 컬럼인 경우 처리\n",
    "        if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "            new_columns = []\n",
    "            for col in stock_data.columns:\n",
    "                if isinstance(col, tuple):\n",
    "                    if col[0] == 'Datetime' or 'Date' in str(col[0]):\n",
    "                        new_columns.append('Datetime')\n",
    "                    else:\n",
    "                        new_columns.append(col[0])\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "            stock_data.columns = new_columns\n",
    "        \n",
    "        # 기본 정보 출력\n",
    "        print(f\"✅ 데이터 수집 완료!\")\n",
    "        print(f\"총 데이터 포인트: {len(stock_data):,}개\")\n",
    "        print(f\"정리된 컬럼명: {list(stock_data.columns)}\")\n",
    "        \n",
    "        # Datetime 컬럼 확인\n",
    "        if 'Datetime' in stock_data.columns:\n",
    "            print(f\"데이터 기간: {stock_data['Datetime'].min()} ~ {stock_data['Datetime'].max()}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Datetime 컬럼을 찾을 수 없습니다. 첫 번째 컬럼 사용: {stock_data.columns[0]}\")\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # 기본 통계\n",
    "        print(f\"\\n📈 기본 통계:\")\n",
    "        print(f\"시작 가격: ${stock_data['Open'].iloc[0]:.2f}\")\n",
    "        print(f\"종료 가격: ${stock_data['Close'].iloc[-1]:.2f}\")\n",
    "        print(f\"최고가: ${stock_data['High'].max():.2f}\")\n",
    "        print(f\"최저가: ${stock_data['Low'].min():.2f}\")\n",
    "        print(f\"평균 거래량: {stock_data['Volume'].mean():,.0f}\")\n",
    "        \n",
    "        # 시간을 정시로 조정 (예: 13:30 -> 13:00)\n",
    "        stock_data = adjust_time_to_hour(stock_data)\n",
    "        \n",
    "        # 추가 특성 계산\n",
    "        stock_data = add_technical_features(stock_data)\n",
    "        \n",
    "        # CSV 저장\n",
    "        if save_to_csv:\n",
    "            filename = f\"{ticker}_1hour_data_{days}days.csv\"\n",
    "            stock_data.to_csv(filename, index=False)\n",
    "            print(f\"💾 데이터가 '{filename}'에 저장되었습니다.\")\n",
    "        \n",
    "        return stock_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        print(f\"오류 상세: {type(e).__name__}\")\n",
    "        return None\n",
    "\n",
    "def get_30min_stock_data(ticker, days=60, save_to_csv=True):\n",
    "    \"\"\"\n",
    "    티커를 입력받아 최근 N일간의 30분 간격 주식 데이터를 가져오는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    ticker (str): 주식 티커 심볼 (예: 'AAPL', 'TSLA', 'AMZN')\n",
    "    days (int): 수집할 일수 (최대 60일, yfinance 제약)\n",
    "    save_to_csv (bool): CSV 파일로 저장할지 여부\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: 30분 간격 주식 데이터\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # yfinance 30분 간격 제약사항 확인\n",
    "        if days > 60:\n",
    "            print(f\"⚠️ yfinance 30분 간격 데이터는 최대 60일까지만 지원됩니다.\")\n",
    "            print(f\"요청한 {days}일 → 60일로 조정합니다.\")\n",
    "            days = 60\n",
    "        \n",
    "        # 날짜 설정 (현재 날짜 기준)\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        \n",
    "        print(f\"📊 {ticker} 주식 데이터 수집 중...\")\n",
    "        print(f\"기간: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')} ({days}일)\")\n",
    "        print(f\"간격: 30분\")\n",
    "        \n",
    "        # yfinance로 데이터 수집 (24시간 데이터 포함)\n",
    "        stock_data = yf.download(\n",
    "            ticker,\n",
    "            start=start_date.strftime('%Y-%m-%d'),\n",
    "            end=end_date.strftime('%Y-%m-%d'),\n",
    "            interval='30m',\n",
    "            prepost=True,  # 시장 외 시간 데이터 포함\n",
    "            progress=False\n",
    "        )\n",
    "        \n",
    "        if stock_data.empty:\n",
    "            print(f\"❌ {ticker}에 대한 데이터를 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        # 인덱스를 컬럼으로 변환\n",
    "        stock_data = stock_data.reset_index()\n",
    "        \n",
    "        # 컬럼명 확인 및 정리\n",
    "        print(f\"🔍 원본 컬럼명: {list(stock_data.columns)}\")\n",
    "        \n",
    "        # 인덱스 컬럼명 통일 (Datetime으로)\n",
    "        if 'Date' in stock_data.columns:\n",
    "            stock_data = stock_data.rename(columns={'Date': 'Datetime'})\n",
    "        elif stock_data.columns[0] not in ['Datetime', 'Date']:\n",
    "            # 첫 번째 컬럼이 시간 데이터인 경우\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # 멀티레벨 컬럼인 경우 처리\n",
    "        if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "            new_columns = []\n",
    "            for col in stock_data.columns:\n",
    "                if isinstance(col, tuple):\n",
    "                    if col[0] == 'Datetime' or 'Date' in str(col[0]):\n",
    "                        new_columns.append('Datetime')\n",
    "                    else:\n",
    "                        new_columns.append(col[0])\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "            stock_data.columns = new_columns\n",
    "        \n",
    "        # 기본 정보 출력\n",
    "        print(f\"✅ 데이터 수집 완료!\")\n",
    "        print(f\"총 데이터 포인트: {len(stock_data):,}개\")\n",
    "        print(f\"정리된 컬럼명: {list(stock_data.columns)}\")\n",
    "        \n",
    "        # Datetime 컬럼 확인\n",
    "        if 'Datetime' in stock_data.columns:\n",
    "            print(f\"데이터 기간: {stock_data['Datetime'].min()} ~ {stock_data['Datetime'].max()}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Datetime 컬럼을 찾을 수 없습니다. 첫 번째 컬럼 사용: {stock_data.columns[0]}\")\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # 기본 통계\n",
    "        print(f\"\\n📈 기본 통계:\")\n",
    "        print(f\"시작 가격: ${stock_data['Open'].iloc[0]:.2f}\")\n",
    "        print(f\"종료 가격: ${stock_data['Close'].iloc[-1]:.2f}\")\n",
    "        print(f\"최고가: ${stock_data['High'].max():.2f}\")\n",
    "        print(f\"최저가: ${stock_data['Low'].min():.2f}\")\n",
    "        print(f\"평균 거래량: {stock_data['Volume'].mean():,.0f}\")\n",
    "        \n",
    "        # 시간을 정시로 조정 (예: 13:30 -> 13:00)\n",
    "        stock_data = adjust_time_to_hour(stock_data)\n",
    "        \n",
    "        # 추가 특성 계산\n",
    "        stock_data = add_technical_features(stock_data)\n",
    "        \n",
    "        # CSV 저장\n",
    "        if save_to_csv:\n",
    "            filename = f\"{ticker}_30min_data_{days}days.csv\"\n",
    "            stock_data.to_csv(filename, index=False)\n",
    "            print(f\"💾 데이터가 '{filename}'에 저장되었습니다.\")\n",
    "        \n",
    "        return stock_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        print(f\"오류 상세: {type(e).__name__}\")\n",
    "        return None\n",
    "\n",
    "def get_max_period_data(ticker, save_to_csv=True):\n",
    "    \"\"\"\n",
    "    yfinance 제약사항에 맞춰 가능한 최대 기간의 데이터를 수집\n",
    "    - 1시간: 730일 (약 2년)\n",
    "    - 30분: 60일\n",
    "    - 15분: 60일\n",
    "    - 5분: 60일\n",
    "    - 1분: 7일\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 {ticker} 최대 기간 데이터 수집...\")\n",
    "    \n",
    "    intervals_and_periods = [\n",
    "        ('1h', 730, '2년'),\n",
    "        ('30m', 60, '60일'),\n",
    "        ('15m', 60, '60일'),\n",
    "        ('5m', 60, '60일'),\n",
    "        ('1m', 7, '7일')\n",
    "    ]\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for interval, max_days, description in intervals_and_periods:\n",
    "        try:\n",
    "            print(f\"\\n📊 {interval} 간격 데이터 수집 중... (최대 {description})\")\n",
    "            \n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=max_days)\n",
    "            \n",
    "            data = yf.download(\n",
    "                ticker,\n",
    "                start=start_date.strftime('%Y-%m-%d'),\n",
    "                end=end_date.strftime('%Y-%m-%d'),\n",
    "                interval=interval,\n",
    "                prepost=True,  # 시장 외 시간 데이터 포함\n",
    "                progress=False\n",
    "            )\n",
    "            \n",
    "            if not data.empty:\n",
    "                data = data.reset_index()\n",
    "                all_data[interval] = data\n",
    "                print(f\"✅ {interval} 데이터: {len(data):,}개 포인트\")\n",
    "                \n",
    "                if save_to_csv:\n",
    "                    filename = f\"{ticker}_{interval}_data_{max_days}days.csv\"\n",
    "                    data.to_csv(filename, index=False)\n",
    "                    print(f\"💾 저장: {filename}\")\n",
    "            else:\n",
    "                print(f\"❌ {interval} 데이터 없음\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {interval} 오류: {e}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def get_longer_period_with_daily(ticker, days=365, save_to_csv=True):\n",
    "    \"\"\"\n",
    "    1년 데이터가 필요한 경우 일별 데이터로 수집\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"📊 {ticker} 일별 데이터 수집 중... ({days}일)\")\n",
    "        \n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        \n",
    "        # 일별 데이터는 제약이 거의 없음\n",
    "        daily_data = yf.download(\n",
    "            ticker,\n",
    "            start=start_date.strftime('%Y-%m-%d'),\n",
    "            end=end_date.strftime('%Y-%m-%d'),\n",
    "            interval='1d',\n",
    "            prepost=True,  # 시장 외 시간 데이터 포함\n",
    "            progress=False\n",
    "        )\n",
    "        \n",
    "        if daily_data.empty:\n",
    "            print(f\"❌ {ticker} 일별 데이터를 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        daily_data = daily_data.reset_index()\n",
    "        \n",
    "        print(f\"✅ 일별 데이터 수집 완료!\")\n",
    "        print(f\"총 데이터 포인트: {len(daily_data):,}개\")\n",
    "        print(f\"데이터 기간: {daily_data['Date'].min()} ~ {daily_data['Date'].max()}\")\n",
    "        \n",
    "        # 기술적 지표 추가\n",
    "        daily_data = add_technical_features_daily(daily_data)\n",
    "        \n",
    "        if save_to_csv:\n",
    "            filename = f\"{ticker}_daily_data_{days}days.csv\"\n",
    "            daily_data.to_csv(filename, index=False)\n",
    "            print(f\"💾 데이터가 '{filename}'에 저장되었습니다.\")\n",
    "        \n",
    "        return daily_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_technical_features_daily(df):\n",
    "    \"\"\"일별 데이터용 기술적 지표 추가\"\"\"\n",
    "    \n",
    "    print(\"🔧 기술적 지표 계산 중...\")\n",
    "    \n",
    "    # 수익률 계산\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # 이동평균\n",
    "    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "    \n",
    "    # 지수이동평균\n",
    "    df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
    "    df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
    "    \n",
    "    # MACD\n",
    "    df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
    "    \n",
    "    # RSI\n",
    "    df['RSI'] = calculate_rsi(df['Close'])\n",
    "    \n",
    "    # 볼린저 밴드\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
    "    bb_std = df['Close'].rolling(window=20).std()\n",
    "    df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
    "    df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
    "    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
    "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "    \n",
    "    # 변동성\n",
    "    df['Volatility_5'] = df['Returns'].rolling(window=5).std()\n",
    "    df['Volatility_10'] = df['Returns'].rolling(window=10).std()\n",
    "    df['Volatility_20'] = df['Returns'].rolling(window=20).std()\n",
    "    \n",
    "    # 가격 변화\n",
    "    df['Price_Change'] = df['Close'] - df['Open']\n",
    "    df['Price_Change_Pct'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "    \n",
    "    # High-Low 스프레드\n",
    "    df['HL_Spread'] = df['High'] - df['Low']\n",
    "    df['HL_Spread_Pct'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
    "    \n",
    "    # 시간 특성 (일별 데이터용)\n",
    "    df['DayOfWeek'] = pd.to_datetime(df['Date']).dt.dayofweek\n",
    "    df['Month'] = pd.to_datetime(df['Date']).dt.month\n",
    "    df['Quarter'] = pd.to_datetime(df['Date']).dt.quarter\n",
    "    df['DayOfMonth'] = pd.to_datetime(df['Date']).dt.day\n",
    "    df['WeekOfYear'] = pd.to_datetime(df['Date']).dt.isocalendar().week\n",
    "    \n",
    "    # 거래일 특성\n",
    "    df['Is_Monday'] = (df['DayOfWeek'] == 0).astype(int)\n",
    "    df['Is_Friday'] = (df['DayOfWeek'] == 4).astype(int)\n",
    "    df['Is_MonthEnd'] = pd.to_datetime(df['Date']).dt.is_month_end.astype(int)\n",
    "    df['Is_MonthStart'] = pd.to_datetime(df['Date']).dt.is_month_start.astype(int)\n",
    "    \n",
    "    print(f\"✅ 기술적 지표 추가 완료! 총 컬럼 수: {len(df.columns)}개\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_technical_features(df):\n",
    "    \"\"\"기술적 지표 추가\"\"\"\n",
    "    \n",
    "    print(\"🔧 기술적 지표 계산 중...\")\n",
    "    \n",
    "    # 수익률 계산\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # 이동평균\n",
    "    df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "    \n",
    "    # 지수이동평균\n",
    "    df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
    "    df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
    "    \n",
    "    # MACD\n",
    "    df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
    "    \n",
    "    # RSI\n",
    "    df['RSI'] = calculate_rsi(df['Close'])\n",
    "    \n",
    "    # 볼린저 밴드\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
    "    bb_std = df['Close'].rolling(window=20).std()\n",
    "    df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
    "    df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
    "    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
    "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "    \n",
    "    # 변동성\n",
    "    df['Volatility_10'] = df['Returns'].rolling(window=10).std()\n",
    "    df['Volatility_20'] = df['Returns'].rolling(window=20).std()\n",
    "    \n",
    "    # 가격 변화\n",
    "    df['Price_Change'] = df['Close'] - df['Open']\n",
    "    df['Price_Change_Pct'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "    \n",
    "    # High-Low 스프레드\n",
    "    df['HL_Spread'] = df['High'] - df['Low']\n",
    "    df['HL_Spread_Pct'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
    "    \n",
    "    # 시간 특성\n",
    "    df['Hour'] = df['Datetime'].dt.hour\n",
    "    df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
    "    df['Month'] = df['Datetime'].dt.month\n",
    "    df['Quarter'] = df['Datetime'].dt.quarter\n",
    "    \n",
    "    # 거래시간 여부 (미국 주식시장: 9:30-16:00 EST, 24시간 포함으로 확대)\n",
    "    df['Is_Trading_Hours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 16)).astype(int)  # 정규 거래시간\n",
    "    df['Is_Market_Open'] = ((df['Hour'] >= 9) & (df['Hour'] < 16)).astype(int)     # 시장 개장시간\n",
    "    df['Is_Premarket'] = ((df['Hour'] >= 4) & (df['Hour'] < 9)).astype(int)       # 프리마켓 (4:00-9:30)\n",
    "    df['Is_Aftermarket'] = ((df['Hour'] >= 16) & (df['Hour'] <= 20)).astype(int)  # 애프터마켓 (16:00-20:00)\n",
    "    df['Is_Extended_Hours'] = (df['Is_Premarket'] | df['Is_Aftermarket']).astype(int)  # 연장거래시간\n",
    "    \n",
    "    print(f\"✅ 기술적 지표 추가 완료! 총 컬럼 수: {len(df.columns)}개\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_rsi(prices, window=14):\n",
    "    \"\"\"RSI (Relative Strength Index) 계산\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def adjust_time_to_hour(df):\n",
    "    \"\"\"시간을 정시로 조정하는 함수 (예: 13:30 -> 13:00)\"\"\"\n",
    "    \n",
    "    print(\"🕐 시간을 정시로 조정 중...\")\n",
    "    \n",
    "    # Datetime 컬럼이 있는지 확인\n",
    "    if 'Datetime' in df.columns:\n",
    "        # 시간을 정시로 조정 (분, 초를 0으로 설정)\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        df['Datetime'] = df['Datetime'].dt.floor('H')  # 시간 단위로 내림\n",
    "        \n",
    "        print(f\"✅ 시간 조정 완료: {df['Datetime'].min()} ~ {df['Datetime'].max()}\")\n",
    "        \n",
    "        # 중복된 시간이 있는 경우 마지막 값 유지\n",
    "        df = df.drop_duplicates(subset=['Datetime'], keep='last')\n",
    "        print(f\"중복 제거 후 데이터 포인트: {len(df):,}개\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "def get_multiple_tickers_hourly(tickers, days=365, save_individual=True, save_combined=True):\n",
    "    \"\"\"여러 티커의 1시간 간격 데이터를 한번에 수집\"\"\"\n",
    "    \n",
    "    print(f\"🚀 {len(tickers)}개 티커 1시간 간격 데이터 수집 시작...\")\n",
    "    print(f\"티커 목록: {', '.join(tickers)}\")\n",
    "    print(f\"수집 기간: 최근 {days}일\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"\\n[{i}/{len(tickers)}] {ticker} 처리 중...\")\n",
    "        \n",
    "        data = get_hourly_stock_data(ticker, days=days, save_to_csv=save_individual)\n",
    "        \n",
    "        if data is not None:\n",
    "            all_data[ticker] = data\n",
    "            print(f\"✅ {ticker} 완료\")\n",
    "        else:\n",
    "            print(f\"❌ {ticker} 실패\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # 통합 데이터 저장\n",
    "    if save_combined and all_data:\n",
    "        print(f\"\\n💾 통합 데이터 저장 중...\")\n",
    "        \n",
    "        # 각 티커별로 컬럼에 티커명 추가\n",
    "        combined_data = pd.DataFrame()\n",
    "        \n",
    "        for ticker, data in all_data.items():\n",
    "            ticker_data = data.copy()\n",
    "            ticker_data['Ticker'] = ticker\n",
    "            combined_data = pd.concat([combined_data, ticker_data], ignore_index=True)\n",
    "        \n",
    "        combined_filename = f\"multiple_stocks_1hour_data_{days}days.csv\"\n",
    "        combined_data.to_csv(combined_filename, index=False)\n",
    "        print(f\"✅ 통합 데이터가 '{combined_filename}'에 저장되었습니다.\")\n",
    "        print(f\"총 데이터 포인트: {len(combined_data):,}개\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def get_multiple_tickers(tickers, days=60, save_individual=True, save_combined=True):\n",
    "    \"\"\"여러 티커의 30분 간격 데이터를 한번에 수집\"\"\"\n",
    "    \n",
    "    print(f\"🚀 {len(tickers)}개 티커 30분 간격 데이터 수집 시작...\")\n",
    "    print(f\"티커 목록: {', '.join(tickers)}\")\n",
    "    print(f\"수집 기간: 최근 {days}일\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"\\n[{i}/{len(tickers)}] {ticker} 처리 중...\")\n",
    "        \n",
    "        data = get_30min_stock_data(ticker, days=days, save_to_csv=save_individual)\n",
    "        \n",
    "        if data is not None:\n",
    "            all_data[ticker] = data\n",
    "            print(f\"✅ {ticker} 완료\")\n",
    "        else:\n",
    "            print(f\"❌ {ticker} 실패\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # 통합 데이터 저장\n",
    "    if save_combined and all_data:\n",
    "        print(f\"\\n💾 통합 데이터 저장 중...\")\n",
    "        \n",
    "        # 각 티커별로 컬럼에 티커명 추가\n",
    "        combined_data = pd.DataFrame()\n",
    "        \n",
    "        for ticker, data in all_data.items():\n",
    "            ticker_data = data.copy()\n",
    "            ticker_data['Ticker'] = ticker\n",
    "            combined_data = pd.concat([combined_data, ticker_data], ignore_index=True)\n",
    "        \n",
    "        combined_filename = f\"multiple_stocks_30min_data_{days}days.csv\"\n",
    "        combined_data.to_csv(combined_filename, index=False)\n",
    "        print(f\"✅ 통합 데이터가 '{combined_filename}'에 저장되었습니다.\")\n",
    "        print(f\"총 데이터 포인트: {len(combined_data):,}개\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def analyze_data_summary(data_dict):\n",
    "    \"\"\"수집된 데이터 요약 분석\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📊 데이터 수집 요약\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for ticker, data in data_dict.items():\n",
    "        if data is not None:\n",
    "            print(f\"\\n{ticker}:\")\n",
    "            print(f\"  데이터 포인트: {len(data):,}개\")\n",
    "            print(f\"  기간: {data['Datetime'].min().strftime('%Y-%m-%d %H:%M')} ~ {data['Datetime'].max().strftime('%Y-%m-%d %H:%M')}\")\n",
    "            print(f\"  가격 범위: ${data['Low'].min():.2f} ~ ${data['High'].max():.2f}\")\n",
    "            print(f\"  평균 거래량: {data['Volume'].mean():,.0f}\")\n",
    "            \n",
    "            # 결측치 확인\n",
    "            missing_count = data.isnull().sum().sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"  ⚠️ 결측치: {missing_count}개\")\n",
    "            else:\n",
    "                print(f\"  ✅ 결측치 없음\")\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"🎯 yfinance 제약사항 안내:\")\n",
    "    print(\"- 1시간 간격: 최대 730일 (약 2년) ⭐ 추천!\")\n",
    "    print(\"- 30분 간격: 최대 60일\")\n",
    "    print(\"- 일별 간격: 제한 없음\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 1시간 간격 데이터 (1년) - 메인 추천!\n",
    "    print(\"\\n🎯 1시간 간격 데이터 수집 (1년) - 추천!\")\n",
    "    aapl_1h = get_hourly_stock_data('AAPL', days=365)\n",
    "    \n",
    "    if aapl_1h is not None:\n",
    "        print(f\"\\n📋 AAPL 1시간 데이터 미리보기:\")\n",
    "        print(aapl_1h[['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']].head())\n",
    "        \n",
    "        # 데이터 양 분석\n",
    "        trading_hours = aapl_1h[aapl_1h['Is_Trading_Hours'] == 1]\n",
    "        print(f\"\\n📊 LSTM 학습용 데이터 분석:\")\n",
    "        print(f\"전체 시간: {len(aapl_1h):,}개\")\n",
    "        print(f\"거래시간만: {len(trading_hours):,}개\")\n",
    "        print(f\"LSTM 시퀀스 길이 30 가정 시 학습 샘플: {len(trading_hours) - 30:,}개\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # 2. 여러 티커 1시간 데이터 (1년)\n",
    "    print(\"\\n🎯 여러 티커 1시간 데이터 수집 (1년)\")\n",
    "    tickers = ['AAPL', 'AMZN', 'TSLA', 'GOOGL', 'MSFT']\n",
    "    \n",
    "    all_stock_data = get_multiple_tickers_hourly(tickers, days=365)\n",
    "    \n",
    "    # 3. 요약 분석\n",
    "    analyze_data_summary(all_stock_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # 4. 30분 간격 비교용 (60일)\n",
    "    print(\"\\n🎯 30분 간격 데이터 비교 (60일)\")\n",
    "    print(\"⚠️ 30분 간격은 최대 60일 제한이 있습니다.\")\n",
    "    \n",
    "    # 현재 날짜 확인\n",
    "    current_date = datetime.now()\n",
    "    print(f\"현재 날짜: {current_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    aapl_30m = get_30min_stock_data('AAPL', days=30)  # 30일로 줄여서 안전하게 테스트\n",
    "    \n",
    "    if aapl_30m is not None:\n",
    "        print(f\"\\n📋 AAPL 30분 데이터 미리보기:\")\n",
    "        print(aapl_30m[['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']].head())\n",
    "    \n",
    "    print(\"\\n🎉 모든 데이터 수집 완료!\")\n",
    "    print(\"\\n💡 권장사항:\")\n",
    "    print(\"✅ 1시간 간격 1년 데이터 - LSTM 학습에 최적!\")\n",
    "    print(f\"   → 약 {365 * 6.5:.0f}개 거래시간 데이터 포인트\")\n",
    "    print(\"   → 충분한 데이터 양 + 적절한 시간 해상도\")\n",
    "    print(\"⚠️ 30분 간격은 60일 제한으로 데이터 부족\")\n",
    "    print(\"⚠️ 일별 데이터는 시간 해상도 부족\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rapid api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RapidAPI - twitter241 엔드포인트를 사용한 트윗 크롤러\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('rapidapi_crawler.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RapidAPITweetCrawler:\n",
    "    \"\"\"\n",
    "    RapidAPI의 twitter241 엔드포인트를 사용하여 트윗을 수집하고 CSV로 저장하는 크롤러.\n",
    "    페이지네이션(cursor)을 처리하여 지정된 개수만큼 트윗을 수집합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"\n",
    "        크롤러를 초기화합니다.\n",
    "        \n",
    "        Args:\n",
    "            api_key: RapidAPI에서 발급받은 API 키\n",
    "        \"\"\"\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API 키가 제공되지 않았습니다.\")\n",
    "            \n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://twitter241.p.rapidapi.com/user-tweets\"\n",
    "        self.headers = {\n",
    "            \"x-rapidapi-key\": self.api_key,\n",
    "            \"x-rapidapi-host\": \"twitter241.p.rapidapi.com\"\n",
    "        }\n",
    "        # count를 증가시켜 한번에 더 많은 트윗 요청 (최대 200까지 시도)\n",
    "        self.count_per_request = 200\n",
    "        \n",
    "        # cursor 캐시 및 중복 방지\n",
    "        self.used_cursors = set()\n",
    "\n",
    "    def _parse_tweets_from_response(self, response_json: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        API 응답 JSON에서 트윗 데이터를 파싱합니다.\n",
    "        \n",
    "        Args:\n",
    "            response_json: API로부터 받은 JSON 응답\n",
    "        \n",
    "        Returns:\n",
    "            추출된 트윗 데이터 리스트 ({'created_at': ..., 'full_text': ...})\n",
    "        \"\"\"\n",
    "        tweets_data = []\n",
    "        \n",
    "        try:\n",
    "            # 'instructions' 리스트에서 'TimelineAddEntries' 타입의 항목을 찾습니다.\n",
    "            instructions = response_json.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "            \n",
    "            timeline_entries = []\n",
    "            for instruction in instructions:\n",
    "                if instruction.get('type') == 'TimelineAddEntries':\n",
    "                    timeline_entries = instruction.get('entries', [])\n",
    "                    break\n",
    "            \n",
    "            if not timeline_entries:\n",
    "                logger.warning(\"응답에서 'entries'를 찾을 수 없습니다.\")\n",
    "                return []\n",
    "\n",
    "            for entry in timeline_entries:\n",
    "                # 'TimelineTweet' 타입의 콘텐츠만 처리\n",
    "                item_content = entry.get('content', {}).get('itemContent', {})\n",
    "                if item_content and item_content.get('itemType') == 'TimelineTweet':\n",
    "                    tweet_results = item_content.get('tweet_results', {})\n",
    "                    result = tweet_results.get('result', {})\n",
    "                    \n",
    "                    # legacy 필드에 실제 데이터가 있습니다.\n",
    "                    legacy_data = result.get('legacy', {})\n",
    "                    \n",
    "                    if legacy_data:\n",
    "                        created_at = legacy_data.get('created_at', 'N/A')\n",
    "                        full_text = \"\"\n",
    "                        \n",
    "                        # 리트윗(RT)인 경우 원본 트윗의 full_text를 가져옵니다.\n",
    "                        # 'retweeted_status_result' 키가 있는지 확인합니다.\n",
    "                        if 'retweeted_status_result' in legacy_data:\n",
    "                            # 원본 트윗의 legacy 데이터를 찾습니다.\n",
    "                            original_tweet_legacy = legacy_data.get('retweeted_status_result', {}).get('result', {}).get('legacy', {})\n",
    "                            full_text = original_tweet_legacy.get('full_text', '')\n",
    "                        else:\n",
    "                            # 일반 트윗은 기존 방식대로 full_text를 가져옵니다.\n",
    "                            full_text = legacy_data.get('full_text', '')\n",
    "\n",
    "                        # 줄바꿈 문자를 공백으로 변환하고 양 끝 공백 제거\n",
    "                        full_text = full_text.replace('\\n', ' ').strip()\n",
    "                        \n",
    "                        tweets_data.append({\n",
    "                            'created_at': created_at,\n",
    "                            'full_text': full_text\n",
    "                        })\n",
    "        except (AttributeError, KeyError, IndexError) as e:\n",
    "            logger.error(f\"트윗 데이터 파싱 중 오류 발생: {e}\")\n",
    "            logger.debug(f\"오류 발생 지점의 JSON 구조: {json.dumps(response_json, indent=2, ensure_ascii=False)}\")\n",
    "            \n",
    "        return tweets_data\n",
    "\n",
    "    def _find_next_cursor(self, response_json: Dict[str, Any]) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        API 응답에서 다음 페이지를 위한 cursor 값을 찾습니다.\n",
    "        개선된 cursor 파싱으로 더 많은 cursor 타입을 처리합니다.\n",
    "        \n",
    "        Args:\n",
    "            response_json: API로부터 받은 JSON 응답\n",
    "            \n",
    "        Returns:\n",
    "            다음 페이지 cursor 문자열 또는 None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            instructions = response_json.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "            \n",
    "            # 모든 instruction 타입에서 cursor 찾기\n",
    "            all_cursors = []\n",
    "            \n",
    "            for instruction in instructions:\n",
    "                # TimelineAddEntries에서 cursor 찾기\n",
    "                if instruction.get('type') == 'TimelineAddEntries':\n",
    "                    entries = instruction.get('entries', [])\n",
    "                    for entry in entries:\n",
    "                        content = entry.get('content', {})\n",
    "                        if content.get('entryType') == 'TimelineTimelineCursor':\n",
    "                            cursor_value = content.get('value')\n",
    "                            cursor_type = content.get('cursorType', '')\n",
    "                            \n",
    "                            if cursor_value and cursor_value not in self.used_cursors:\n",
    "                                all_cursors.append({\n",
    "                                    'value': cursor_value,\n",
    "                                    'type': cursor_type,\n",
    "                                    'priority': 1 if cursor_type == 'Bottom' else 2\n",
    "                                })\n",
    "                \n",
    "                # TimelineReplaceEntry에서도 cursor 찾기\n",
    "                elif instruction.get('type') == 'TimelineReplaceEntry':\n",
    "                    entry = instruction.get('entry', {})\n",
    "                    content = entry.get('content', {})\n",
    "                    if content.get('entryType') == 'TimelineTimelineCursor':\n",
    "                        cursor_value = content.get('value')\n",
    "                        cursor_type = content.get('cursorType', '')\n",
    "                        \n",
    "                        if cursor_value and cursor_value not in self.used_cursors:\n",
    "                            all_cursors.append({\n",
    "                                'value': cursor_value,\n",
    "                                'type': cursor_type,\n",
    "                                'priority': 1 if cursor_type == 'Bottom' else 2\n",
    "                            })\n",
    "            \n",
    "            # cursor를 우선순위에 따라 정렬 (Bottom이 우선)\n",
    "            if all_cursors:\n",
    "                all_cursors.sort(key=lambda x: x['priority'])\n",
    "                selected_cursor = all_cursors[0]['value']\n",
    "                self.used_cursors.add(selected_cursor)\n",
    "                logger.debug(f\"선택된 cursor: {selected_cursor[:50]}... (타입: {all_cursors[0]['type']})\")\n",
    "                return selected_cursor\n",
    "                \n",
    "        except (AttributeError, KeyError, IndexError) as e:\n",
    "            logger.error(f\"Cursor 파싱 중 오류 발생: {e}\")\n",
    "            \n",
    "        return None\n",
    "\n",
    "    def fetch_user_tweets(self, user_id: str, max_tweets: int = 1000):\n",
    "        \"\"\"\n",
    "        특정 사용자의 트윗을 수집하여 CSV 파일로 저장합니다.\n",
    "        개선된 페이지네이션으로 더 많은 트윗을 효율적으로 수집합니다.\n",
    "        \n",
    "        Args:\n",
    "            user_id: 트윗을 수집할 사용자의 ID\n",
    "            max_tweets: 수집할 최대 트윗 수\n",
    "        \"\"\"\n",
    "        logger.info(f\"사용자 ID {user_id}의 트윗 수집을 시작합니다. 목표: {max_tweets}개\")\n",
    "        logger.info(f\"한 번의 요청당 {self.count_per_request}개 트윗 요청\")\n",
    "        \n",
    "        all_tweets = []\n",
    "        cursor = None\n",
    "        request_count = 0\n",
    "        max_requests = 100  # 무한 루프 방지\n",
    "        consecutive_empty_responses = 0\n",
    "        \n",
    "        # cursor 캐시 초기화\n",
    "        self.used_cursors.clear()\n",
    "        \n",
    "        while len(all_tweets) < max_tweets and request_count < max_requests:\n",
    "            # count를 동적으로 조정 (남은 트윗 수에 따라)\n",
    "            remaining_tweets = max_tweets - len(all_tweets)\n",
    "            current_count = min(self.count_per_request, remaining_tweets)\n",
    "            \n",
    "            querystring = {\n",
    "                \"user\": user_id,\n",
    "                \"count\": str(current_count)\n",
    "            }\n",
    "            if cursor:\n",
    "                querystring[\"cursor\"] = cursor\n",
    "            \n",
    "            logger.info(f\"API 요청 #{request_count + 1}: {len(all_tweets)} / {max_tweets} 수집됨. Count: {current_count}\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.base_url, headers=self.headers, params=querystring, timeout=45)\n",
    "                request_count += 1\n",
    "                \n",
    "                if response.status_code == 429:  # Rate limit\n",
    "                    logger.warning(\"Rate limit에 도달했습니다. 60초 대기...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                elif response.status_code != 200:\n",
    "                    logger.error(f\"API 에러: {response.status_code} - {response.text}\")\n",
    "                    if response.status_code >= 500:  # 서버 에러인 경우 재시도\n",
    "                        logger.info(\"서버 에러로 인한 10초 후 재시도...\")\n",
    "                        time.sleep(10)\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                    \n",
    "                data = response.json()\n",
    "                \n",
    "                newly_fetched_tweets = self._parse_tweets_from_response(data)\n",
    "                \n",
    "                if not newly_fetched_tweets:\n",
    "                    consecutive_empty_responses += 1\n",
    "                    logger.warning(f\"이번 응답에서 트윗을 찾을 수 없습니다. ({consecutive_empty_responses}/3)\")\n",
    "                    \n",
    "                    if consecutive_empty_responses >= 3:\n",
    "                        logger.info(\"연속 3회 빈 응답으로 수집을 종료합니다.\")\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_empty_responses = 0\n",
    "                    logger.info(f\"이번 요청에서 {len(newly_fetched_tweets)}개 트윗 수집\")\n",
    "                \n",
    "                all_tweets.extend(newly_fetched_tweets)\n",
    "                \n",
    "                # 중복 제거 (created_at + full_text 기준)\n",
    "                seen = set()\n",
    "                unique_tweets = []\n",
    "                for tweet in all_tweets:\n",
    "                    tweet_key = (tweet['created_at'], tweet['full_text'])\n",
    "                    if tweet_key not in seen:\n",
    "                        seen.add(tweet_key)\n",
    "                        unique_tweets.append(tweet)\n",
    "                \n",
    "                all_tweets = unique_tweets\n",
    "                logger.info(f\"중복 제거 후: {len(all_tweets)}개 트윗\")\n",
    "                \n",
    "                # 다음 cursor 찾기\n",
    "                next_cursor = self._find_next_cursor(data)\n",
    "                if not next_cursor or next_cursor == cursor:\n",
    "                    logger.info(\"더 이상 사용 가능한 cursor가 없습니다. 수집을 종료합니다.\")\n",
    "                    break\n",
    "                \n",
    "                cursor = next_cursor\n",
    "\n",
    "                # API rate limit를 고려한 대기 시간 (요청 수에 따라 조정)\n",
    "                if request_count % 10 == 0:  # 10번째마다 긴 대기\n",
    "                    wait_time = 5\n",
    "                else:\n",
    "                    wait_time = 1\n",
    "                    \n",
    "                logger.debug(f\"{wait_time}초 대기 중...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "            except requests.exceptions.Timeout:\n",
    "                logger.warning(\"요청 타임아웃. 5초 후 재시도...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"네트워크 오류 발생: {e}\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            except json.JSONDecodeError:\n",
    "                logger.error(\"JSON 디코딩 오류. 응답이 올바른 JSON 형식이 아닙니다.\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"총 {len(all_tweets)}개의 트윗을 {request_count}번의 요청으로 수집했습니다.\")\n",
    "        logger.info(f\"평균 요청당 트윗 수: {len(all_tweets) / request_count if request_count > 0 else 0:.1f}개\")\n",
    "        \n",
    "        if all_tweets:\n",
    "            filename = f\"user_{user_id}_tweets_ReTweet.csv\"\n",
    "            self._save_to_csv(all_tweets, filename)\n",
    "            \n",
    "    def _save_to_csv(self, tweets_list: List[Dict[str, str]], filename: str):\n",
    "        \"\"\"\n",
    "        수집된 트윗 데이터를 CSV 파일로 저장합니다.\n",
    "        \n",
    "        Args:\n",
    "            tweets_list: 저장할 트윗 데이터 리스트\n",
    "            filename: 저장할 파일 이름\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "                # 'utf-8-sig'는 Excel에서 한글이 깨지지 않도록 BOM을 추가합니다.\n",
    "                writer = csv.DictWriter(f, fieldnames=['created_at', 'full_text'])\n",
    "                writer.writeheader()\n",
    "                writer.writerows(tweets_list)\n",
    "            logger.info(f\"CSV 파일 저장 완료: {filename}\")\n",
    "        except IOError as e:\n",
    "            logger.error(f\"파일 저장 중 오류 발생: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    스크립트 실행을 위한 메인 함수\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"  RapidAPI(twitter241) 기반 트윗 크롤러 (개선된 버전)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # --- 설정 ---\n",
    "    # 보안을 위해 API 키는 환경 변수에서 가져오는 것을 권장합니다.\n",
    "    # 예: api_key = os.getenv(\"RAPIDAPI_KEY\")\n",
    "    API_KEY = \"5fac920861msh988e449f8d91b60p10459bjsnba691d3d2d81\" # 사용자 요청에 따라 하드코딩\n",
    "    USER_ID = \"86437069\"\n",
    "    # @WhiteHouse 1879644163769335808\n",
    "    # @SecScottBessent 1889019333960998912\n",
    "    # @JDVance 1542228578\n",
    "    # @marcorubio 15745368\n",
    "    # @elonmusk 44196397\n",
    "    MAX_TWEETS = 1000\n",
    "    \n",
    "    if not API_KEY:\n",
    "        print(\"[ERROR] API 키가 설정되지 않았습니다. 스크립트를 종료합니다.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"대상 사용자 ID: {USER_ID}\")\n",
    "    print(f\"수집 목표 트윗 수: {MAX_TWEETS}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    crawler = RapidAPITweetCrawler(api_key=API_KEY)\n",
    "    crawler.fetch_user_tweets(user_id=USER_ID, max_tweets=MAX_TWEETS)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"크롤링 작업이 완료되었습니다.\")\n",
    "    print(f\"결과는 user_{USER_ID}_tweets_ReTweet.csv 파일에 저장되었습니다.\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + DNN 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "병합 완료: news_stock_classification.csv 저장됨\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로\n",
    "stock_path = \"./AAPL_1hour_data_365days.csv\"\n",
    "news_path = \"./apple_finbert_finnhub.csv\"\n",
    "\n",
    "# 데이터 불러오기\n",
    "stock_df = pd.read_csv(stock_path, parse_dates=[\"Datetime\"])\n",
    "news_df = pd.read_csv(news_path, parse_dates=[\"pubDate\"])\n",
    "\n",
    "# 타임존 제거\n",
    "stock_df[\"Datetime\"] = stock_df[\"Datetime\"].dt.tz_localize(None)\n",
    "news_df[\"pubDate\"] = news_df[\"pubDate\"].dt.tz_localize(None)\n",
    "\n",
    "# 정렬\n",
    "stock_df = stock_df.sort_values(\"Datetime\").reset_index(drop=True)\n",
    "\n",
    "# 제외할 열\n",
    "exclude_cols = ['Is_Trading_Hours', 'Is_Market_Open', 'Is_Premarket', 'Is_Aftermarket', 'Is_Extended_Hours']\n",
    "stock_df = stock_df.drop(columns=[col for col in exclude_cols if col in stock_df.columns])\n",
    "\n",
    "# 병합 결과\n",
    "rows = []\n",
    "\n",
    "for _, news_row in news_df.iterrows():\n",
    "    news_time = news_row['pubDate']\n",
    "\n",
    "    # 뉴스 이후 가장 가까운 주가\n",
    "    future_stock = stock_df[stock_df['Datetime'] > news_time].head(1)\n",
    "    if future_stock.empty:\n",
    "        continue\n",
    "\n",
    "    target_row = future_stock.iloc[0]\n",
    "    target_time = target_row['Datetime']\n",
    "    target_close = target_row['Close']\n",
    "\n",
    "    # 과거 3개 주가\n",
    "    past_rows = stock_df[stock_df['Datetime'] < target_time].tail(3)\n",
    "    if len(past_rows) < 3:\n",
    "        continue\n",
    "\n",
    "    past_last_close = past_rows.iloc[-1]['Close']\n",
    "\n",
    "    # 상승률\n",
    "    return_pct = (target_close - past_last_close) / past_last_close * 100\n",
    "    label = 1 if return_pct >= 0.4 else (-1 if return_pct <= -0.4 else 0)\n",
    "\n",
    "    # 병합 row 생성\n",
    "    row = {\n",
    "        \"news_id\": news_row['id'],\n",
    "        \"news_time\": news_time,\n",
    "        \"target_close\": target_close,\n",
    "        \"target_return_pct\": return_pct,\n",
    "        \"target_multi_raw\": label,\n",
    "        \"finbert_positive\": news_row['finbert_positive'],\n",
    "        \"finbert_neutral\": news_row['finbert_neutral'],\n",
    "        \"finbert_negative\": news_row['finbert_negative'],\n",
    "    }\n",
    "\n",
    "    # 과거 3개 flatten\n",
    "    for i, (_, stock_row) in enumerate(past_rows.iterrows(), 1):\n",
    "        for col in stock_df.columns:\n",
    "            if col == \"Datetime\":\n",
    "                continue\n",
    "            row[f\"x{i}_{col}\"] = stock_row[col]\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "# 최종 DataFrame\n",
    "merged_df = pd.DataFrame(rows)\n",
    "\n",
    "# 클래스 0/1/2로 매핑 (XGBoost용)\n",
    "label_map = {-1: 0, 0: 1, 1: 2}\n",
    "merged_df[\"target_multi\"] = merged_df[\"target_multi_raw\"].map(label_map)\n",
    "\n",
    "# 저장\n",
    "merged_df.to_csv(\"news_stock_classification.csv\", index=False)\n",
    "print(\"병합 완료: news_stock_classification.csv 저장됨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 165.6778\n",
      "Epoch 2 | Loss: 139.9572\n",
      "Epoch 3 | Loss: 124.0224\n",
      "Epoch 4 | Loss: 111.9599\n",
      "Epoch 5 | Loss: 102.0316\n",
      "Epoch 6 | Loss: 91.2283\n",
      "Epoch 7 | Loss: 82.4791\n",
      "Epoch 8 | Loss: 74.9245\n",
      "Epoch 9 | Loss: 68.3347\n",
      "Epoch 10 | Loss: 62.1140\n",
      "Epoch 11 | Loss: 56.4879\n",
      "Epoch 12 | Loss: 51.9471\n",
      "Epoch 13 | Loss: 47.4376\n",
      "Epoch 14 | Loss: 43.8949\n",
      "Epoch 15 | Loss: 39.6787\n",
      "Epoch 16 | Loss: 36.4289\n",
      "Epoch 17 | Loss: 33.2387\n",
      "Epoch 18 | Loss: 30.9987\n",
      "Epoch 19 | Loss: 28.0263\n",
      "Epoch 20 | Loss: 25.6803\n",
      "Epoch 21 | Loss: 23.1786\n",
      "Epoch 22 | Loss: 21.3123\n",
      "Epoch 23 | Loss: 19.6293\n",
      "Epoch 24 | Loss: 17.5688\n",
      "Epoch 25 | Loss: 15.6692\n",
      "Epoch 26 | Loss: 14.3345\n",
      "Epoch 27 | Loss: 12.6775\n",
      "Epoch 28 | Loss: 11.6631\n",
      "Epoch 29 | Loss: 10.7014\n",
      "Epoch 30 | Loss: 9.4993\n",
      "Epoch 31 | Loss: 8.4388\n",
      "Epoch 32 | Loss: 7.4168\n",
      "Epoch 33 | Loss: 7.0659\n",
      "Epoch 34 | Loss: 5.9982\n",
      "Epoch 35 | Loss: 5.2894\n",
      "Epoch 36 | Loss: 4.8408\n",
      "Epoch 37 | Loss: 4.3750\n",
      "Epoch 38 | Loss: 3.9858\n",
      "Epoch 39 | Loss: 4.1333\n",
      "Epoch 40 | Loss: 3.2084\n",
      "Epoch 41 | Loss: 2.7471\n",
      "Epoch 42 | Loss: 3.0587\n",
      "Epoch 43 | Loss: 4.0325\n",
      "Epoch 44 | Loss: 2.0774\n",
      "Epoch 45 | Loss: 2.0874\n",
      "Epoch 46 | Loss: 1.6183\n",
      "Epoch 47 | Loss: 2.0426\n",
      "Epoch 48 | Loss: 1.9357\n",
      "Epoch 49 | Loss: 1.2066\n",
      "Epoch 50 | Loss: 1.1955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "df = pd.read_csv(\"news_stock_classification.csv\", parse_dates=[\"news_time\"])\n",
    "\n",
    "# 2. Feature 및 Label 준비\n",
    "feature_cols = [col for col in df.columns if col.startswith(\"x\") or col.startswith(\"finbert_\")]\n",
    "X = df[feature_cols].fillna(0)\n",
    "y = df[\"target_multi\"]\n",
    "\n",
    "# 3. 시계열 데이터 3-step 생성 (x1_, x2_, x3_)\n",
    "X_seq = []\n",
    "for i in range(len(X)):\n",
    "    X_seq.append([\n",
    "        X.iloc[i][[col for col in X.columns if col.startswith(\"x1_\")]].values,\n",
    "        X.iloc[i][[col for col in X.columns if col.startswith(\"x2_\")]].values,\n",
    "        X.iloc[i][[col for col in X.columns if col.startswith(\"x3_\")]].values\n",
    "    ])\n",
    "X_seq = np.array(X_seq)\n",
    "\n",
    "# 4. FinBERT 피처 추가 (Broadcast across time steps)\n",
    "finbert_feats = X[[c for c in X.columns if c.startswith(\"finbert_\")]].values\n",
    "finbert_feats = np.repeat(finbert_feats[:, np.newaxis, :], 3, axis=1)\n",
    "X_seq = np.concatenate([X_seq, finbert_feats], axis=-1)\n",
    "\n",
    "# 5. 정규화\n",
    "n_samples, time_steps, n_features = X_seq.shape\n",
    "X_reshaped = X_seq.reshape(-1, n_features)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_reshaped)\n",
    "X_seq = X_scaled.reshape(n_samples, time_steps, n_features)\n",
    "\n",
    "# 6. Tensor로 변환\n",
    "X_tensor = torch.tensor(X_seq, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# 7. Train/Test 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, shuffle=False)\n",
    "train_dl = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(TensorDataset(X_test, y_test), batch_size=32)\n",
    "\n",
    "# 8. LSTM 모델 정의\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return self.fc(hn[-1])\n",
    "\n",
    "# 9. 학습 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(input_dim=n_features).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 10. 학습 루프\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy: 0.5917297612114153\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.13      0.11       246\n",
      "           1       0.77      0.78      0.77      1224\n",
      "           2       0.25      0.11      0.15       247\n",
      "\n",
      "    accuracy                           0.59      1717\n",
      "   macro avg       0.37      0.34      0.34      1717\n",
      "weighted avg       0.59      0.59      0.59      1717\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      " [[ 32 161  53]\n",
      " [237 957  30]\n",
      " [ 90 130  27]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 11. 평가\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb).argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "print(\"\\n Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(\"\\n Classification Report:\\n\", classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
