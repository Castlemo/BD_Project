{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finnhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "# .env ì„¤ì • ë¡œë“œ\n",
    "load_dotenv()\n",
    "FINHUB_API_KEY = os.getenv(\"finhub\")  # ë³€ìˆ˜ëª…ì€ 'finnhub' ì•„ë‹Œ 'finhub'\n",
    "\n",
    "# API í˜¸ì¶œ ì œí•œ (60 calls/minute)\n",
    "API_CALLS_PER_MINUTE = 60\n",
    "DELAY_BETWEEN_CALLS = 60.0 / API_CALLS_PER_MINUTE  # 1ì´ˆ ê°„ê²©\n",
    "\n",
    "\n",
    "def safe_datetime_conversion(timestamp):\n",
    "    \"\"\"\n",
    "    íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì•ˆì „í•˜ê²Œ datetimeìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    Out of bounds nanosecond timestamp ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if not timestamp or timestamp == 0:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # ìœ ë‹‰ìŠ¤ íƒ€ì„ìŠ¤íƒ¬í”„ ë²”ìœ„ í™•ì¸ (1970-01-01 ì´í›„)\n",
    "        if timestamp < 0:\n",
    "            return None\n",
    "            \n",
    "        # ë„ˆë¬´ í° ê°’ í™•ì¸ (2262ë…„ ì´í›„ëŠ” pandasì—ì„œ ì²˜ë¦¬ ë¶ˆê°€)\n",
    "        if timestamp > 9223372036:  # 2262-04-11 ì •ë„\n",
    "            return None\n",
    "            \n",
    "        # ì •ìƒì ì¸ ë³€í™˜ ì‹œë„\n",
    "        return pd.to_datetime(timestamp, unit='s')\n",
    "        \n",
    "    except (ValueError, OutOfBoundsDatetime, OverflowError):\n",
    "        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜\n",
    "        return None\n",
    "    except Exception:\n",
    "        # ê¸°íƒ€ ì˜ˆì™¸ ì‹œ None ë°˜í™˜\n",
    "        return None\n",
    "\n",
    "\n",
    "# Finnhubì—ì„œ íšŒì‚¬ ë‰´ìŠ¤ ìˆ˜ì§‘ (ìµœì í™”ëœ ë²„ì „ - ëŒ€ëŸ‰ ìˆ˜ì§‘)\n",
    "def fetch_finnhub_news_extended(symbol: str = \"GOOGL\", start_date: str = \"2025-06-14\", days_per_request: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    í•˜ë‚˜ì˜ ì‹¬ë³¼ì— ëŒ€í•´ ìµœëŒ€í•œ ë§ì€ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    ë‚ ì§œ êµ¬ê°„ì„ ë‚˜ëˆ„ì–´ì„œ ì—¬ëŸ¬ ë²ˆ API í˜¸ì¶œí•˜ì—¬ ë” ë§ì€ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        symbol: ì£¼ì‹ ì‹¬ë³¼ (ì˜ˆ: \"AAPL\")\n",
    "        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)\n",
    "        days_per_request: í•œ ë²ˆì˜ API í˜¸ì¶œë‹¹ ìˆ˜ì§‘í•  ì¼ìˆ˜ (ê¸°ë³¸: 30ì¼)\n",
    "    \"\"\"\n",
    "    if not FINHUB_API_KEY:\n",
    "        print(\"[ERROR] Finnhub API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    url = \"https://finnhub.io/api/v1/company-news\"\n",
    "    \n",
    "    try:\n",
    "        start_date_obj = datetime.datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "        today = datetime.date.today()\n",
    "        \n",
    "        print(f\"[INFO] ìš”ì²­ ì‹œì‘ ë‚ ì§œ: {start_date_obj}\")\n",
    "        print(f\"[INFO] í˜„ì¬ ë‚ ì§œ: {today}\")\n",
    "        \n",
    "        # Free Tier ì œí•œì„ ë„˜ì–´ì„œ ìµœëŒ€í•œ ë§ì´ ìˆ˜ì§‘ ì‹œë„\n",
    "        two_years_ago = today - datetime.timedelta(days=730)  # 2ë…„ ì „\n",
    "        three_years_ago = today - datetime.timedelta(days=1095)  # 3ë…„ ì „\n",
    "        \n",
    "        print(f\"[INFO] ğŸš€ Free Tier ì œí•œ ëŒíŒŒ ì‹œë„: 2-3ë…„ê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œë„í•©ë‹ˆë‹¤!\")\n",
    "        print(f\"[INFO] 2ë…„ ì „ ë‚ ì§œ: {two_years_ago}\")\n",
    "        print(f\"[INFO] 3ë…„ ì „ ë‚ ì§œ: {three_years_ago}\")\n",
    "        \n",
    "        # 3ë…„ ì „ë¶€í„° í˜„ì¬ê¹Œì§€ë¡œ ì‹œë„ (APIê°€ ì–´ë””ê¹Œì§€ í—ˆìš©í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸)\n",
    "        actual_start = three_years_ago\n",
    "        actual_end = today\n",
    "        \n",
    "        print(f\"[INFO] {symbol} ë‰´ìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹œì‘ (ì œí•œ ëŒíŒŒ ì‹œë„)\")\n",
    "        print(f\"[INFO] ì‹¤ì œ ìˆ˜ì§‘ ê¸°ê°„: {actual_start.isoformat()} ~ {actual_end.isoformat()}\")\n",
    "        print(f\"[INFO] ìˆ˜ì§‘ ê¸°ê°„: {(actual_end - actual_start).days}ì¼ (ì•½ 3ë…„)\")\n",
    "        print(f\"[INFO] ì˜ˆìƒ API í˜¸ì¶œ íšŸìˆ˜: {(actual_end - actual_start).days // days_per_request + 1}íšŒ\")\n",
    "        print(f\"[WARNING] APIê°€ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "        \n",
    "    except ValueError:\n",
    "        print(f\"[ERROR] ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {start_date}. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_articles = []\n",
    "    current_date = actual_start\n",
    "    request_count = 0\n",
    "    \n",
    "    while current_date < actual_end:\n",
    "        # ê° ìš”ì²­ì˜ ì¢…ë£Œ ë‚ ì§œ ê³„ì‚°\n",
    "        period_end = min(current_date + datetime.timedelta(days=days_per_request), actual_end)\n",
    "        \n",
    "        params = {\n",
    "            \"symbol\": symbol,\n",
    "            \"from\": current_date.isoformat(),\n",
    "            \"to\": period_end.isoformat(),\n",
    "            \"token\": FINHUB_API_KEY\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            request_count += 1\n",
    "            print(f\"[INFO] API í˜¸ì¶œ {request_count}: {current_date.isoformat()} ~ {period_end.isoformat()}\")\n",
    "            \n",
    "            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ë”œë ˆì´\n",
    "            time.sleep(DELAY_BETWEEN_CALLS)\n",
    "            \n",
    "            res = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if res.status_code == 429:\n",
    "                print(f\"[WARNING] API í˜¸ì¶œ ì œí•œ ë„ë‹¬. ë” ê¸´ ëŒ€ê¸° í›„ ì¬ì‹œë„...\")\n",
    "                time.sleep(10)\n",
    "                res = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if res.status_code == 403:\n",
    "                print(f\"[ERROR] API ì ‘ê·¼ ê±°ë¶€ (ê¸°ê°„: {current_date} ~ {period_end}): Free Tier ì œí•œ ë„ë‹¬ ê°€ëŠ¥ì„±\")\n",
    "                print(f\"[INFO] í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(all_articles)}ê°œ\")\n",
    "                break\n",
    "            \n",
    "            if res.status_code != 200:\n",
    "                print(f\"[WARNING] API ìš”ì²­ ì‹¤íŒ¨ (ê¸°ê°„: {current_date} ~ {period_end}): HTTP {res.status_code}\")\n",
    "                print(f\"[INFO] ì‘ë‹µ ë‚´ìš©: {res.text[:200]}...\")\n",
    "                \n",
    "                # 429 (Too Many Requests)ê°€ ì•„ë‹ˆë¼ë©´ ê³„ì† ì§„í–‰\n",
    "                if res.status_code != 429:\n",
    "                    current_date = period_end + datetime.timedelta(days=1)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"[INFO] í˜¸ì¶œ ì œí•œìœ¼ë¡œ ì¸í•œ ëŒ€ê¸°...\")\n",
    "                    time.sleep(30)\n",
    "                    continue\n",
    "\n",
    "            data = res.json()\n",
    "            \n",
    "            if not isinstance(data, list):\n",
    "                print(f\"[WARNING] ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì‘ë‹µ í˜•ì‹ (ê¸°ê°„: {current_date} ~ {period_end})\")\n",
    "                if isinstance(data, dict) and 'error' in data:\n",
    "                    print(f\"[ERROR] API ì˜¤ë¥˜: {data['error']}\")\n",
    "                    if 'limit' in data['error'].lower():\n",
    "                        print(f\"[INFO] Free Tier ì œí•œ ë„ë‹¬. í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘: {len(all_articles)}ê°œ\")\n",
    "                        break\n",
    "                current_date = period_end + datetime.timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            # í•´ë‹¹ ê¸°ê°„ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘\n",
    "            period_articles = []\n",
    "            for item in data:\n",
    "                # ì•ˆì „í•œ datetime ë³€í™˜ ì‚¬ìš©\n",
    "                pub_date = safe_datetime_conversion(item.get(\"datetime\"))\n",
    "                \n",
    "                article = {\n",
    "                    \"id\": item.get(\"id\"),\n",
    "                    \"title\": item.get(\"headline\", \"\"),\n",
    "                    \"summary\": item.get(\"summary\", \"\"),\n",
    "                    \"link\": item.get(\"url\", \"\"),\n",
    "                    \"publisher\": item.get(\"publisher\", \"\"),\n",
    "                    \"category\": item.get(\"category\", \"\"),\n",
    "                    \"pubDate\": pub_date,\n",
    "                    \"image\": item.get(\"image\", \"\"),\n",
    "                    \"related\": item.get(\"related\", \"\"),\n",
    "                    \"source\": item.get(\"source\", \"\"),\n",
    "                    \"collection_period\": f\"{current_date.isoformat()}_{period_end.isoformat()}\"\n",
    "                }\n",
    "                period_articles.append(article)\n",
    "            \n",
    "            all_articles.extend(period_articles)\n",
    "            print(f\"[SUCCESS] ê¸°ê°„ë³„ ìˆ˜ì§‘: {len(period_articles)}ê°œ ê¸°ì‚¬ (ì´ {len(all_articles)}ê°œ)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] API í˜¸ì¶œ ì˜¤ë¥˜ (ê¸°ê°„: {current_date} ~ {period_end}): {e}\")\n",
    "            if \"limit\" in str(e).lower() or \"403\" in str(e):\n",
    "                print(f\"[INFO] Free Tier ì œí•œ ë„ë‹¬ ê°€ëŠ¥ì„±. í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘: {len(all_articles)}ê°œ\")\n",
    "                break\n",
    "        \n",
    "        # ë‹¤ìŒ ê¸°ê°„ìœ¼ë¡œ ì´ë™\n",
    "        current_date = period_end + datetime.timedelta(days=1)\n",
    "        \n",
    "        # API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ì¶”ê°€ ëŒ€ê¸°\n",
    "        if request_count % 10 == 0:  # 10ë²ˆ í˜¸ì¶œë§ˆë‹¤ ì¶”ê°€ ëŒ€ê¸°\n",
    "            print(f\"[INFO] API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°... (í˜„ì¬ê¹Œì§€ {len(all_articles)}ê°œ ìˆ˜ì§‘)\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # ì „ì²´ ê²°ê³¼ ì²˜ë¦¬\n",
    "    if all_articles:\n",
    "        df = pd.DataFrame(all_articles)\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° (ID, ì œëª©, ë§í¬ ê¸°ì¤€)\n",
    "        before_dedup = len(df)\n",
    "        df = df.drop_duplicates(subset=['id', 'title', 'link'])\n",
    "        after_dedup = len(df)\n",
    "        \n",
    "        if before_dedup != after_dedup:\n",
    "            print(f\"[INFO] ì¤‘ë³µ ê¸°ì‚¬ ì œê±°: {before_dedup - after_dedup}ê°œ\")\n",
    "        \n",
    "        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ) - None ê°’ ì²˜ë¦¬\n",
    "        df = df.sort_values('pubDate', ascending=False, na_position='last')\n",
    "        \n",
    "        print(f\"[SUCCESS] {symbol} ì´ {len(df)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "        print(f\"[INFO] ì´ API í˜¸ì¶œ íšŸìˆ˜: {request_count}íšŒ\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f\"[WARNING] {symbol}ì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# yfinance ì£¼ê°€ ë°ì´í„°\n",
    "def fetch_stock_data(ticker_symbol: str, period: str = '1y', interval: str = '1d') -> pd.DataFrame:\n",
    "    try:\n",
    "        df = yf.download(\n",
    "            tickers=ticker_symbol,\n",
    "            period=period,\n",
    "            interval=interval,\n",
    "            auto_adjust=False,\n",
    "            progress=False\n",
    "        )\n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "        df = df.reset_index()\n",
    "        df = df.rename(columns={'Adj Close': 'Adj_Close'})\n",
    "        return df[['Date', 'Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']]\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] ì£¼ì‹ ë°ì´í„° ì˜¤ë¥˜: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ëŒ€ëŸ‰ ë‰´ìŠ¤ ìˆ˜ì§‘í•  ë‹¨ì¼ ì‹¬ë³¼ ì„¤ì •\n",
    "    target_symbol = \"GOOGL\"  # ì›í•˜ëŠ” ì‹¬ë³¼ë¡œ ë³€ê²½ ê°€ëŠ¥ (ì˜ˆ: MSFT, GOOGL, AMZN, TSLA, META, NVDA ë“±)\n",
    "    start_date = \"2025-06-13\"  # ì°¸ê³ ìš© (ì‹¤ì œë¡œëŠ” 1ë…„ ì „ë¶€í„° ìë™ ìˆ˜ì§‘)\n",
    "    days_per_request = 7  # í•œ ë²ˆì˜ API í˜¸ì¶œë‹¹ 7ì¼ì”© ìˆ˜ì§‘ (ë” ë§ì€ API í˜¸ì¶œë¡œ ìµœëŒ€ ìˆ˜ì§‘)\n",
    "    \n",
    "    print(f\"{target_symbol} ë‰´ìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹œì‘!\")\n",
    "    print(f\"ì°¸ê³  ë‚ ì§œ: {start_date} (ì‹¤ì œë¡œëŠ” 3ë…„ ì „ë¶€í„° ìˆ˜ì§‘ ì‹œë„)\")\n",
    "    print(f\"ìˆ˜ì§‘ ë°©ì‹: {days_per_request}ì¼ì”© êµ¬ê°„ë³„ ìˆ˜ì§‘\")\n",
    "    print(f\"API ì œí•œ: {API_CALLS_PER_MINUTE}íšŒ/ë¶„\")\n",
    "    print(f\"ëª©í‘œ:Free Tier ì œí•œ ëŒíŒŒ ì‹œë„ - ìµœëŒ€ 3ë…„ê°„ ë‰´ìŠ¤ ìˆ˜ì§‘!\")\n",
    "    print(f\"ì£¼ì˜: APIê°€ ì œí•œì„ ê±¸ ìˆ˜ ìˆìœ¼ë‹ˆ ì‹¤í—˜ì  ìˆ˜ì§‘ì…ë‹ˆë‹¤.\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # í™•ì¥ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ ì‚¬ìš©\n",
    "    df_extended_news = fetch_finnhub_news_extended(\n",
    "        symbol=target_symbol, \n",
    "        start_date=start_date,\n",
    "        days_per_request=days_per_request\n",
    "    )\n",
    "    \n",
    "    if not df_extended_news.empty:\n",
    "        print(f\"\\nğŸ‰ {target_symbol} ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ“° ì´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {len(df_extended_news)}ê°œ\")\n",
    "        \n",
    "        # ë‚ ì§œë³„ ê¸°ì‚¬ ë¶„í¬ ë¶„ì„ (ìœ íš¨í•œ ë‚ ì§œë§Œ)\n",
    "        if 'pubDate' in df_extended_news.columns:\n",
    "            valid_dates = df_extended_news[df_extended_news['pubDate'].notna()].copy()\n",
    "            if not valid_dates.empty:\n",
    "                valid_dates['date_only'] = valid_dates['pubDate'].dt.date\n",
    "                date_counts = valid_dates['date_only'].value_counts().sort_index()\n",
    "                print(f\"ğŸ“ˆ ìˆ˜ì§‘ ê¸°ê°„: {date_counts.index.min()} ~ {date_counts.index.max()}\")\n",
    "                print(f\"ğŸ“Š í‰ê·  ì¼ì¼ ê¸°ì‚¬ ìˆ˜: {date_counts.mean():.1f}ê°œ\")\n",
    "                print(f\"ğŸ“… ìœ íš¨í•œ ë‚ ì§œ ê¸°ì‚¬: {len(valid_dates)}ê°œ / ì „ì²´ {len(df_extended_news)}ê°œ\")\n",
    "        \n",
    "        # ìµœì‹  ê¸°ì‚¬ 10ê°œ ë¯¸ë¦¬ë³´ê¸°\n",
    "        print(f\"\\nğŸ“° ìµœì‹  ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 10ê°œ)\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, row in df_extended_news.head(10).iterrows():\n",
    "            pub_date = row['pubDate'].strftime('%Y-%m-%d %H:%M') if pd.notna(row['pubDate']) else 'N/A'\n",
    "            title = row['title'][:60] + \"...\" if len(row['title']) > 60 else row['title']\n",
    "            publisher = row['publisher'] if row['publisher'] else row['source']\n",
    "            print(f\"{i+1:2d}. [{publisher}] {title}\")\n",
    "            print(f\"    ğŸ“… {pub_date} | ğŸ”— {row['link'][:50]}...\")\n",
    "            print()\n",
    "        \n",
    "        # CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "        today = datetime.date.today().isoformat()\n",
    "        filename = f\"{target_symbol}_extended_news_{today}.csv\"\n",
    "        df_extended_news.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "        \n",
    "        # ë°œí–‰ì²˜ë³„ í†µê³„\n",
    "        print(f\"\\nğŸ“Š ë°œí–‰ì²˜ë³„ ê¸°ì‚¬ ìˆ˜ í†µê³„ (ìƒìœ„ 10ê°œ)\")\n",
    "        print(\"-\" * 40)\n",
    "        publisher_col = 'publisher' if df_extended_news['publisher'].notna().any() else 'source'\n",
    "        publisher_counts = df_extended_news[publisher_col].value_counts()\n",
    "        for publisher, count in publisher_counts.head(10).items():\n",
    "            if publisher:  # ë¹ˆ ê°’ì´ ì•„ë‹Œ ê²½ìš°ë§Œ\n",
    "                print(f\"{publisher:25s}: {count:3d}ê°œ\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ (ìˆëŠ” ê²½ìš°)\n",
    "        if 'category' in df_extended_news.columns and df_extended_news['category'].notna().any():\n",
    "            print(f\"\\nğŸ·ï¸  ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜\")\n",
    "            print(\"-\" * 30)\n",
    "            category_counts = df_extended_news['category'].value_counts()\n",
    "            for category, count in category_counts.items():\n",
    "                if category:\n",
    "                    print(f\"{category:20s}: {count:3d}ê°œ\")\n",
    "                    \n",
    "    else:\n",
    "        print(f\"âŒ {target_symbol} ë‰´ìŠ¤ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ğŸ”§ ê°€ëŠ¥í•œ í•´ê²° ë°©ë²•:\")\n",
    "        print(\"   1. API í‚¤ í™•ì¸ (.env íŒŒì¼ì˜ 'finhub' ë³€ìˆ˜)\")\n",
    "        print(\"   2. ì¸í„°ë„· ì—°ê²° í™•ì¸\")  \n",
    "        print(\"   3. ì‹¬ë³¼ëª… í™•ì¸ (ë¯¸êµ­ ìƒì¥ ê¸°ì—…ë§Œ ì§€ì›)\")\n",
    "        print(\"   4. ë‚ ì§œ ë²”ìœ„ ì¡°ì •\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“‹ ìˆ˜ì§‘ ì™„ë£Œ ìš”ì•½\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ğŸ¢ ëŒ€ìƒ ê¸°ì—…: {target_symbol}\")\n",
    "    print(f\"ğŸ“… ìˆ˜ì§‘ ë‚ ì§œ: {start_date}\")\n",
    "    print(f\"ğŸ“Š ì´ ê¸°ì‚¬ ìˆ˜: {len(df_extended_news) if not df_extended_news.empty else 0}ê°œ\")\n",
    "    print(f\"ğŸ’¾ ì €ì¥ íŒŒì¼: {filename if not df_extended_news.empty else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_hourly_stock_data(ticker, days=365, save_to_csv=True):\n",
    "    \"\"\"\n",
    "    í‹°ì»¤ë¥¼ ì…ë ¥ë°›ì•„ ìµœê·¼ Nì¼ê°„ì˜ 1ì‹œê°„ ê°„ê²© ì£¼ì‹ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Parameters:\n",
    "    ticker (str): ì£¼ì‹ í‹°ì»¤ ì‹¬ë³¼ (ì˜ˆ: 'AAPL', 'TSLA', 'AMZN')\n",
    "    days (int): ìˆ˜ì§‘í•  ì¼ìˆ˜ (ìµœëŒ€ 730ì¼, yfinance ì œì•½)\n",
    "    save_to_csv (bool): CSV íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: 1ì‹œê°„ ê°„ê²© ì£¼ì‹ ë°ì´í„°\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # yfinance 1ì‹œê°„ ê°„ê²© ì œì•½ì‚¬í•­ í™•ì¸\n",
    "        if days > 730:\n",
    "            print(f\"âš ï¸ yfinance 1ì‹œê°„ ê°„ê²© ë°ì´í„°ëŠ” ìµœëŒ€ 730ì¼ê¹Œì§€ë§Œ ì§€ì›ë©ë‹ˆë‹¤.\")\n",
    "            print(f\"ìš”ì²­í•œ {days}ì¼ â†’ 730ì¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.\")\n",
    "            days = 730\n",
    "        \n",
    "        # ë‚ ì§œ ì„¤ì • (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€)\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        \n",
    "        print(f\"ğŸ“Š {ticker} ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...\")\n",
    "        print(f\"ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')} ({days}ì¼)\")\n",
    "        print(f\"ê°„ê²©: 1ì‹œê°„\")\n",
    "        \n",
    "        # yfinanceë¡œ ë°ì´í„° ìˆ˜ì§‘ (24ì‹œê°„ ë°ì´í„° í¬í•¨)\n",
    "        stock_data = yf.download(\n",
    "            ticker,\n",
    "            start=start_date.strftime('%Y-%m-%d'),\n",
    "            end=end_date.strftime('%Y-%m-%d'),\n",
    "            interval='1h',\n",
    "            prepost=True,  # ì‹œì¥ ì™¸ ì‹œê°„ ë°ì´í„° í¬í•¨\n",
    "            progress=False\n",
    "        )\n",
    "        \n",
    "        if stock_data.empty:\n",
    "            print(f\"âŒ {ticker}ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        # ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜\n",
    "        stock_data = stock_data.reset_index()\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… í™•ì¸ ë° ì •ë¦¬\n",
    "        print(f\"ğŸ” ì›ë³¸ ì»¬ëŸ¼ëª…: {list(stock_data.columns)}\")\n",
    "        \n",
    "        # ì¸ë±ìŠ¤ ì»¬ëŸ¼ëª… í†µì¼ (Datetimeìœ¼ë¡œ)\n",
    "        if 'Date' in stock_data.columns:\n",
    "            stock_data = stock_data.rename(columns={'Date': 'Datetime'})\n",
    "        elif stock_data.columns[0] not in ['Datetime', 'Date']:\n",
    "            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ì‹œê°„ ë°ì´í„°ì¸ ê²½ìš°\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # ë©€í‹°ë ˆë²¨ ì»¬ëŸ¼ì¸ ê²½ìš° ì²˜ë¦¬\n",
    "        if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "            new_columns = []\n",
    "            for col in stock_data.columns:\n",
    "                if isinstance(col, tuple):\n",
    "                    if col[0] == 'Datetime' or 'Date' in str(col[0]):\n",
    "                        new_columns.append('Datetime')\n",
    "                    else:\n",
    "                        new_columns.append(col[0])\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "            stock_data.columns = new_columns\n",
    "        \n",
    "        # ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "        print(f\"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "        print(f\"ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(stock_data):,}ê°œ\")\n",
    "        print(f\"ì •ë¦¬ëœ ì»¬ëŸ¼ëª…: {list(stock_data.columns)}\")\n",
    "        \n",
    "        # Datetime ì»¬ëŸ¼ í™•ì¸\n",
    "        if 'Datetime' in stock_data.columns:\n",
    "            print(f\"ë°ì´í„° ê¸°ê°„: {stock_data['Datetime'].min()} ~ {stock_data['Datetime'].max()}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Datetime ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©: {stock_data.columns[0]}\")\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        print(f\"\\nğŸ“ˆ ê¸°ë³¸ í†µê³„:\")\n",
    "        print(f\"ì‹œì‘ ê°€ê²©: ${stock_data['Open'].iloc[0]:.2f}\")\n",
    "        print(f\"ì¢…ë£Œ ê°€ê²©: ${stock_data['Close'].iloc[-1]:.2f}\")\n",
    "        print(f\"ìµœê³ ê°€: ${stock_data['High'].max():.2f}\")\n",
    "        print(f\"ìµœì €ê°€: ${stock_data['Low'].min():.2f}\")\n",
    "        print(f\"í‰ê·  ê±°ë˜ëŸ‰: {stock_data['Volume'].mean():,.0f}\")\n",
    "        \n",
    "        # ì‹œê°„ì„ ì •ì‹œë¡œ ì¡°ì • (ì˜ˆ: 13:30 -> 13:00)\n",
    "        stock_data = adjust_time_to_hour(stock_data)\n",
    "        \n",
    "        # ì¶”ê°€ íŠ¹ì„± ê³„ì‚°\n",
    "        stock_data = add_technical_features(stock_data)\n",
    "        \n",
    "        # CSV ì €ì¥\n",
    "        if save_to_csv:\n",
    "            filename = f\"{ticker}_1hour_data_{days}days.csv\"\n",
    "            stock_data.to_csv(filename, index=False)\n",
    "            print(f\"ğŸ’¾ ë°ì´í„°ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return stock_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(f\"ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}\")\n",
    "        return None\n",
    "\n",
    "def get_30min_stock_data(ticker, days=60, save_to_csv=True):\n",
    "    \"\"\"\n",
    "    í‹°ì»¤ë¥¼ ì…ë ¥ë°›ì•„ ìµœê·¼ Nì¼ê°„ì˜ 30ë¶„ ê°„ê²© ì£¼ì‹ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Parameters:\n",
    "    ticker (str): ì£¼ì‹ í‹°ì»¤ ì‹¬ë³¼ (ì˜ˆ: 'AAPL', 'TSLA', 'AMZN')\n",
    "    days (int): ìˆ˜ì§‘í•  ì¼ìˆ˜ (ìµœëŒ€ 60ì¼, yfinance ì œì•½)\n",
    "    save_to_csv (bool): CSV íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: 30ë¶„ ê°„ê²© ì£¼ì‹ ë°ì´í„°\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # yfinance 30ë¶„ ê°„ê²© ì œì•½ì‚¬í•­ í™•ì¸\n",
    "        if days > 60:\n",
    "            print(f\"âš ï¸ yfinance 30ë¶„ ê°„ê²© ë°ì´í„°ëŠ” ìµœëŒ€ 60ì¼ê¹Œì§€ë§Œ ì§€ì›ë©ë‹ˆë‹¤.\")\n",
    "            print(f\"ìš”ì²­í•œ {days}ì¼ â†’ 60ì¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.\")\n",
    "            days = 60\n",
    "        \n",
    "        # ë‚ ì§œ ì„¤ì • (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€)\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        \n",
    "        print(f\"ğŸ“Š {ticker} ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...\")\n",
    "        print(f\"ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')} ({days}ì¼)\")\n",
    "        print(f\"ê°„ê²©: 30ë¶„\")\n",
    "        \n",
    "        # yfinanceë¡œ ë°ì´í„° ìˆ˜ì§‘ (24ì‹œê°„ ë°ì´í„° í¬í•¨)\n",
    "        stock_data = yf.download(\n",
    "            ticker,\n",
    "            start=start_date.strftime('%Y-%m-%d'),\n",
    "            end=end_date.strftime('%Y-%m-%d'),\n",
    "            interval='30m',\n",
    "            prepost=True,  # ì‹œì¥ ì™¸ ì‹œê°„ ë°ì´í„° í¬í•¨\n",
    "            progress=False\n",
    "        )\n",
    "        \n",
    "        if stock_data.empty:\n",
    "            print(f\"âŒ {ticker}ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        # ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜\n",
    "        stock_data = stock_data.reset_index()\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… í™•ì¸ ë° ì •ë¦¬\n",
    "        print(f\"ğŸ” ì›ë³¸ ì»¬ëŸ¼ëª…: {list(stock_data.columns)}\")\n",
    "        \n",
    "        # ì¸ë±ìŠ¤ ì»¬ëŸ¼ëª… í†µì¼ (Datetimeìœ¼ë¡œ)\n",
    "        if 'Date' in stock_data.columns:\n",
    "            stock_data = stock_data.rename(columns={'Date': 'Datetime'})\n",
    "        elif stock_data.columns[0] not in ['Datetime', 'Date']:\n",
    "            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ì‹œê°„ ë°ì´í„°ì¸ ê²½ìš°\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # ë©€í‹°ë ˆë²¨ ì»¬ëŸ¼ì¸ ê²½ìš° ì²˜ë¦¬\n",
    "        if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "            new_columns = []\n",
    "            for col in stock_data.columns:\n",
    "                if isinstance(col, tuple):\n",
    "                    if col[0] == 'Datetime' or 'Date' in str(col[0]):\n",
    "                        new_columns.append('Datetime')\n",
    "                    else:\n",
    "                        new_columns.append(col[0])\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "            stock_data.columns = new_columns\n",
    "        \n",
    "        # ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "        print(f\"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "        print(f\"ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(stock_data):,}ê°œ\")\n",
    "        print(f\"ì •ë¦¬ëœ ì»¬ëŸ¼ëª…: {list(stock_data.columns)}\")\n",
    "        \n",
    "        # Datetime ì»¬ëŸ¼ í™•ì¸\n",
    "        if 'Datetime' in stock_data.columns:\n",
    "            print(f\"ë°ì´í„° ê¸°ê°„: {stock_data['Datetime'].min()} ~ {stock_data['Datetime'].max()}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Datetime ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©: {stock_data.columns[0]}\")\n",
    "            stock_data = stock_data.rename(columns={stock_data.columns[0]: 'Datetime'})\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        print(f\"\\nğŸ“ˆ ê¸°ë³¸ í†µê³„:\")\n",
    "        print(f\"ì‹œì‘ ê°€ê²©: ${stock_data['Open'].iloc[0]:.2f}\")\n",
    "        print(f\"ì¢…ë£Œ ê°€ê²©: ${stock_data['Close'].iloc[-1]:.2f}\")\n",
    "        print(f\"ìµœê³ ê°€: ${stock_data['High'].max():.2f}\")\n",
    "        print(f\"ìµœì €ê°€: ${stock_data['Low'].min():.2f}\")\n",
    "        print(f\"í‰ê·  ê±°ë˜ëŸ‰: {stock_data['Volume'].mean():,.0f}\")\n",
    "        \n",
    "        # ì‹œê°„ì„ ì •ì‹œë¡œ ì¡°ì • (ì˜ˆ: 13:30 -> 13:00)\n",
    "        stock_data = adjust_time_to_hour(stock_data)\n",
    "        \n",
    "        # ì¶”ê°€ íŠ¹ì„± ê³„ì‚°\n",
    "        stock_data = add_technical_features(stock_data)\n",
    "        \n",
    "        # CSV ì €ì¥\n",
    "        if save_to_csv:\n",
    "            filename = f\"{ticker}_30min_data_{days}days.csv\"\n",
    "            stock_data.to_csv(filename, index=False)\n",
    "            print(f\"ğŸ’¾ ë°ì´í„°ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return stock_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(f\"ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}\")\n",
    "        return None\n",
    "\n",
    "def get_max_period_data(ticker, save_to_csv=True):\n",
    "    \"\"\"\n",
    "    yfinance ì œì•½ì‚¬í•­ì— ë§ì¶° ê°€ëŠ¥í•œ ìµœëŒ€ ê¸°ê°„ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘\n",
    "    - 1ì‹œê°„: 730ì¼ (ì•½ 2ë…„)\n",
    "    - 30ë¶„: 60ì¼\n",
    "    - 15ë¶„: 60ì¼\n",
    "    - 5ë¶„: 60ì¼\n",
    "    - 1ë¶„: 7ì¼\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸš€ {ticker} ìµœëŒ€ ê¸°ê°„ ë°ì´í„° ìˆ˜ì§‘...\")\n",
    "    \n",
    "    intervals_and_periods = [\n",
    "        ('1h', 730, '2ë…„'),\n",
    "        ('30m', 60, '60ì¼'),\n",
    "        ('15m', 60, '60ì¼'),\n",
    "        ('5m', 60, '60ì¼'),\n",
    "        ('1m', 7, '7ì¼')\n",
    "    ]\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for interval, max_days, description in intervals_and_periods:\n",
    "        try:\n",
    "            print(f\"\\nğŸ“Š {interval} ê°„ê²© ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ìµœëŒ€ {description})\")\n",
    "            \n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=max_days)\n",
    "            \n",
    "            data = yf.download(\n",
    "                ticker,\n",
    "                start=start_date.strftime('%Y-%m-%d'),\n",
    "                end=end_date.strftime('%Y-%m-%d'),\n",
    "                interval=interval,\n",
    "                prepost=True,  # ì‹œì¥ ì™¸ ì‹œê°„ ë°ì´í„° í¬í•¨\n",
    "                progress=False\n",
    "            )\n",
    "            \n",
    "            if not data.empty:\n",
    "                data = data.reset_index()\n",
    "                all_data[interval] = data\n",
    "                print(f\"âœ… {interval} ë°ì´í„°: {len(data):,}ê°œ í¬ì¸íŠ¸\")\n",
    "                \n",
    "                if save_to_csv:\n",
    "                    filename = f\"{ticker}_{interval}_data_{max_days}days.csv\"\n",
    "                    data.to_csv(filename, index=False)\n",
    "                    print(f\"ğŸ’¾ ì €ì¥: {filename}\")\n",
    "            else:\n",
    "                print(f\"âŒ {interval} ë°ì´í„° ì—†ìŒ\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {interval} ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def get_longer_period_with_daily(ticker, days=365, save_to_csv=True):\n",
    "    \"\"\"\n",
    "    1ë…„ ë°ì´í„°ê°€ í•„ìš”í•œ ê²½ìš° ì¼ë³„ ë°ì´í„°ë¡œ ìˆ˜ì§‘\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ“Š {ticker} ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({days}ì¼)\")\n",
    "        \n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        \n",
    "        # ì¼ë³„ ë°ì´í„°ëŠ” ì œì•½ì´ ê±°ì˜ ì—†ìŒ\n",
    "        daily_data = yf.download(\n",
    "            ticker,\n",
    "            start=start_date.strftime('%Y-%m-%d'),\n",
    "            end=end_date.strftime('%Y-%m-%d'),\n",
    "            interval='1d',\n",
    "            prepost=True,  # ì‹œì¥ ì™¸ ì‹œê°„ ë°ì´í„° í¬í•¨\n",
    "            progress=False\n",
    "        )\n",
    "        \n",
    "        if daily_data.empty:\n",
    "            print(f\"âŒ {ticker} ì¼ë³„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        daily_data = daily_data.reset_index()\n",
    "        \n",
    "        print(f\"âœ… ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "        print(f\"ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(daily_data):,}ê°œ\")\n",
    "        print(f\"ë°ì´í„° ê¸°ê°„: {daily_data['Date'].min()} ~ {daily_data['Date'].max()}\")\n",
    "        \n",
    "        # ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€\n",
    "        daily_data = add_technical_features_daily(daily_data)\n",
    "        \n",
    "        if save_to_csv:\n",
    "            filename = f\"{ticker}_daily_data_{days}days.csv\"\n",
    "            daily_data.to_csv(filename, index=False)\n",
    "            print(f\"ğŸ’¾ ë°ì´í„°ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return daily_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_technical_features_daily(df):\n",
    "    \"\"\"ì¼ë³„ ë°ì´í„°ìš© ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”§ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì¤‘...\")\n",
    "    \n",
    "    # ìˆ˜ìµë¥  ê³„ì‚°\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # ì´ë™í‰ê· \n",
    "    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "    \n",
    "    # ì§€ìˆ˜ì´ë™í‰ê· \n",
    "    df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
    "    df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
    "    \n",
    "    # MACD\n",
    "    df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
    "    \n",
    "    # RSI\n",
    "    df['RSI'] = calculate_rsi(df['Close'])\n",
    "    \n",
    "    # ë³¼ë¦°ì € ë°´ë“œ\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
    "    bb_std = df['Close'].rolling(window=20).std()\n",
    "    df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
    "    df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
    "    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
    "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "    \n",
    "    # ë³€ë™ì„±\n",
    "    df['Volatility_5'] = df['Returns'].rolling(window=5).std()\n",
    "    df['Volatility_10'] = df['Returns'].rolling(window=10).std()\n",
    "    df['Volatility_20'] = df['Returns'].rolling(window=20).std()\n",
    "    \n",
    "    # ê°€ê²© ë³€í™”\n",
    "    df['Price_Change'] = df['Close'] - df['Open']\n",
    "    df['Price_Change_Pct'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "    \n",
    "    # High-Low ìŠ¤í”„ë ˆë“œ\n",
    "    df['HL_Spread'] = df['High'] - df['Low']\n",
    "    df['HL_Spread_Pct'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
    "    \n",
    "    # ì‹œê°„ íŠ¹ì„± (ì¼ë³„ ë°ì´í„°ìš©)\n",
    "    df['DayOfWeek'] = pd.to_datetime(df['Date']).dt.dayofweek\n",
    "    df['Month'] = pd.to_datetime(df['Date']).dt.month\n",
    "    df['Quarter'] = pd.to_datetime(df['Date']).dt.quarter\n",
    "    df['DayOfMonth'] = pd.to_datetime(df['Date']).dt.day\n",
    "    df['WeekOfYear'] = pd.to_datetime(df['Date']).dt.isocalendar().week\n",
    "    \n",
    "    # ê±°ë˜ì¼ íŠ¹ì„±\n",
    "    df['Is_Monday'] = (df['DayOfWeek'] == 0).astype(int)\n",
    "    df['Is_Friday'] = (df['DayOfWeek'] == 4).astype(int)\n",
    "    df['Is_MonthEnd'] = pd.to_datetime(df['Date']).dt.is_month_end.astype(int)\n",
    "    df['Is_MonthStart'] = pd.to_datetime(df['Date']).dt.is_month_start.astype(int)\n",
    "    \n",
    "    print(f\"âœ… ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€ ì™„ë£Œ! ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_technical_features(df):\n",
    "    \"\"\"ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”§ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì¤‘...\")\n",
    "    \n",
    "    # ìˆ˜ìµë¥  ê³„ì‚°\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # ì´ë™í‰ê· \n",
    "    df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "    \n",
    "    # ì§€ìˆ˜ì´ë™í‰ê· \n",
    "    df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
    "    df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
    "    \n",
    "    # MACD\n",
    "    df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
    "    \n",
    "    # RSI\n",
    "    df['RSI'] = calculate_rsi(df['Close'])\n",
    "    \n",
    "    # ë³¼ë¦°ì € ë°´ë“œ\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
    "    bb_std = df['Close'].rolling(window=20).std()\n",
    "    df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
    "    df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
    "    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
    "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "    \n",
    "    # ë³€ë™ì„±\n",
    "    df['Volatility_10'] = df['Returns'].rolling(window=10).std()\n",
    "    df['Volatility_20'] = df['Returns'].rolling(window=20).std()\n",
    "    \n",
    "    # ê°€ê²© ë³€í™”\n",
    "    df['Price_Change'] = df['Close'] - df['Open']\n",
    "    df['Price_Change_Pct'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "    \n",
    "    # High-Low ìŠ¤í”„ë ˆë“œ\n",
    "    df['HL_Spread'] = df['High'] - df['Low']\n",
    "    df['HL_Spread_Pct'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
    "    \n",
    "    # ì‹œê°„ íŠ¹ì„±\n",
    "    df['Hour'] = df['Datetime'].dt.hour\n",
    "    df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
    "    df['Month'] = df['Datetime'].dt.month\n",
    "    df['Quarter'] = df['Datetime'].dt.quarter\n",
    "    \n",
    "    # ê±°ë˜ì‹œê°„ ì—¬ë¶€ (ë¯¸êµ­ ì£¼ì‹ì‹œì¥: 9:30-16:00 EST, 24ì‹œê°„ í¬í•¨ìœ¼ë¡œ í™•ëŒ€)\n",
    "    df['Is_Trading_Hours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 16)).astype(int)  # ì •ê·œ ê±°ë˜ì‹œê°„\n",
    "    df['Is_Market_Open'] = ((df['Hour'] >= 9) & (df['Hour'] < 16)).astype(int)     # ì‹œì¥ ê°œì¥ì‹œê°„\n",
    "    df['Is_Premarket'] = ((df['Hour'] >= 4) & (df['Hour'] < 9)).astype(int)       # í”„ë¦¬ë§ˆì¼“ (4:00-9:30)\n",
    "    df['Is_Aftermarket'] = ((df['Hour'] >= 16) & (df['Hour'] <= 20)).astype(int)  # ì• í”„í„°ë§ˆì¼“ (16:00-20:00)\n",
    "    df['Is_Extended_Hours'] = (df['Is_Premarket'] | df['Is_Aftermarket']).astype(int)  # ì—°ì¥ê±°ë˜ì‹œê°„\n",
    "    \n",
    "    print(f\"âœ… ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€ ì™„ë£Œ! ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_rsi(prices, window=14):\n",
    "    \"\"\"RSI (Relative Strength Index) ê³„ì‚°\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def adjust_time_to_hour(df):\n",
    "    \"\"\"ì‹œê°„ì„ ì •ì‹œë¡œ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜ (ì˜ˆ: 13:30 -> 13:00)\"\"\"\n",
    "    \n",
    "    print(\"ğŸ• ì‹œê°„ì„ ì •ì‹œë¡œ ì¡°ì • ì¤‘...\")\n",
    "    \n",
    "    # Datetime ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "    if 'Datetime' in df.columns:\n",
    "        # ì‹œê°„ì„ ì •ì‹œë¡œ ì¡°ì • (ë¶„, ì´ˆë¥¼ 0ìœ¼ë¡œ ì„¤ì •)\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        df['Datetime'] = df['Datetime'].dt.floor('H')  # ì‹œê°„ ë‹¨ìœ„ë¡œ ë‚´ë¦¼\n",
    "        \n",
    "        print(f\"âœ… ì‹œê°„ ì¡°ì • ì™„ë£Œ: {df['Datetime'].min()} ~ {df['Datetime'].max()}\")\n",
    "        \n",
    "        # ì¤‘ë³µëœ ì‹œê°„ì´ ìˆëŠ” ê²½ìš° ë§ˆì§€ë§‰ ê°’ ìœ ì§€\n",
    "        df = df.drop_duplicates(subset=['Datetime'], keep='last')\n",
    "        print(f\"ì¤‘ë³µ ì œê±° í›„ ë°ì´í„° í¬ì¸íŠ¸: {len(df):,}ê°œ\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "def get_multiple_tickers_hourly(tickers, days=365, save_individual=True, save_combined=True):\n",
    "    \"\"\"ì—¬ëŸ¬ í‹°ì»¤ì˜ 1ì‹œê°„ ê°„ê²© ë°ì´í„°ë¥¼ í•œë²ˆì— ìˆ˜ì§‘\"\"\"\n",
    "    \n",
    "    print(f\"ğŸš€ {len(tickers)}ê°œ í‹°ì»¤ 1ì‹œê°„ ê°„ê²© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "    print(f\"í‹°ì»¤ ëª©ë¡: {', '.join(tickers)}\")\n",
    "    print(f\"ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {days}ì¼\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"\\n[{i}/{len(tickers)}] {ticker} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        data = get_hourly_stock_data(ticker, days=days, save_to_csv=save_individual)\n",
    "        \n",
    "        if data is not None:\n",
    "            all_data[ticker] = data\n",
    "            print(f\"âœ… {ticker} ì™„ë£Œ\")\n",
    "        else:\n",
    "            print(f\"âŒ {ticker} ì‹¤íŒ¨\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # í†µí•© ë°ì´í„° ì €ì¥\n",
    "    if save_combined and all_data:\n",
    "        print(f\"\\nğŸ’¾ í†µí•© ë°ì´í„° ì €ì¥ ì¤‘...\")\n",
    "        \n",
    "        # ê° í‹°ì»¤ë³„ë¡œ ì»¬ëŸ¼ì— í‹°ì»¤ëª… ì¶”ê°€\n",
    "        combined_data = pd.DataFrame()\n",
    "        \n",
    "        for ticker, data in all_data.items():\n",
    "            ticker_data = data.copy()\n",
    "            ticker_data['Ticker'] = ticker\n",
    "            combined_data = pd.concat([combined_data, ticker_data], ignore_index=True)\n",
    "        \n",
    "        combined_filename = f\"multiple_stocks_1hour_data_{days}days.csv\"\n",
    "        combined_data.to_csv(combined_filename, index=False)\n",
    "        print(f\"âœ… í†µí•© ë°ì´í„°ê°€ '{combined_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(combined_data):,}ê°œ\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def get_multiple_tickers(tickers, days=60, save_individual=True, save_combined=True):\n",
    "    \"\"\"ì—¬ëŸ¬ í‹°ì»¤ì˜ 30ë¶„ ê°„ê²© ë°ì´í„°ë¥¼ í•œë²ˆì— ìˆ˜ì§‘\"\"\"\n",
    "    \n",
    "    print(f\"ğŸš€ {len(tickers)}ê°œ í‹°ì»¤ 30ë¶„ ê°„ê²© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "    print(f\"í‹°ì»¤ ëª©ë¡: {', '.join(tickers)}\")\n",
    "    print(f\"ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {days}ì¼\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"\\n[{i}/{len(tickers)}] {ticker} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        data = get_30min_stock_data(ticker, days=days, save_to_csv=save_individual)\n",
    "        \n",
    "        if data is not None:\n",
    "            all_data[ticker] = data\n",
    "            print(f\"âœ… {ticker} ì™„ë£Œ\")\n",
    "        else:\n",
    "            print(f\"âŒ {ticker} ì‹¤íŒ¨\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # í†µí•© ë°ì´í„° ì €ì¥\n",
    "    if save_combined and all_data:\n",
    "        print(f\"\\nğŸ’¾ í†µí•© ë°ì´í„° ì €ì¥ ì¤‘...\")\n",
    "        \n",
    "        # ê° í‹°ì»¤ë³„ë¡œ ì»¬ëŸ¼ì— í‹°ì»¤ëª… ì¶”ê°€\n",
    "        combined_data = pd.DataFrame()\n",
    "        \n",
    "        for ticker, data in all_data.items():\n",
    "            ticker_data = data.copy()\n",
    "            ticker_data['Ticker'] = ticker\n",
    "            combined_data = pd.concat([combined_data, ticker_data], ignore_index=True)\n",
    "        \n",
    "        combined_filename = f\"multiple_stocks_30min_data_{days}days.csv\"\n",
    "        combined_data.to_csv(combined_filename, index=False)\n",
    "        print(f\"âœ… í†µí•© ë°ì´í„°ê°€ '{combined_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(combined_data):,}ê°œ\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def analyze_data_summary(data_dict):\n",
    "    \"\"\"ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½ ë¶„ì„\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ìš”ì•½\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for ticker, data in data_dict.items():\n",
    "        if data is not None:\n",
    "            print(f\"\\n{ticker}:\")\n",
    "            print(f\"  ë°ì´í„° í¬ì¸íŠ¸: {len(data):,}ê°œ\")\n",
    "            print(f\"  ê¸°ê°„: {data['Datetime'].min().strftime('%Y-%m-%d %H:%M')} ~ {data['Datetime'].max().strftime('%Y-%m-%d %H:%M')}\")\n",
    "            print(f\"  ê°€ê²© ë²”ìœ„: ${data['Low'].min():.2f} ~ ${data['High'].max():.2f}\")\n",
    "            print(f\"  í‰ê·  ê±°ë˜ëŸ‰: {data['Volume'].mean():,.0f}\")\n",
    "            \n",
    "            # ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "            missing_count = data.isnull().sum().sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"  âš ï¸ ê²°ì¸¡ì¹˜: {missing_count}ê°œ\")\n",
    "            else:\n",
    "                print(f\"  âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"ğŸ¯ yfinance ì œì•½ì‚¬í•­ ì•ˆë‚´:\")\n",
    "    print(\"- 1ì‹œê°„ ê°„ê²©: ìµœëŒ€ 730ì¼ (ì•½ 2ë…„) â­ ì¶”ì²œ!\")\n",
    "    print(\"- 30ë¶„ ê°„ê²©: ìµœëŒ€ 60ì¼\")\n",
    "    print(\"- ì¼ë³„ ê°„ê²©: ì œí•œ ì—†ìŒ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 1ì‹œê°„ ê°„ê²© ë°ì´í„° (1ë…„) - ë©”ì¸ ì¶”ì²œ!\n",
    "    print(\"\\nğŸ¯ 1ì‹œê°„ ê°„ê²© ë°ì´í„° ìˆ˜ì§‘ (1ë…„) - ì¶”ì²œ!\")\n",
    "    aapl_1h = get_hourly_stock_data('AAPL', days=365)\n",
    "    \n",
    "    if aapl_1h is not None:\n",
    "        print(f\"\\nğŸ“‹ AAPL 1ì‹œê°„ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "        print(aapl_1h[['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']].head())\n",
    "        \n",
    "        # ë°ì´í„° ì–‘ ë¶„ì„\n",
    "        trading_hours = aapl_1h[aapl_1h['Is_Trading_Hours'] == 1]\n",
    "        print(f\"\\nğŸ“Š LSTM í•™ìŠµìš© ë°ì´í„° ë¶„ì„:\")\n",
    "        print(f\"ì „ì²´ ì‹œê°„: {len(aapl_1h):,}ê°œ\")\n",
    "        print(f\"ê±°ë˜ì‹œê°„ë§Œ: {len(trading_hours):,}ê°œ\")\n",
    "        print(f\"LSTM ì‹œí€€ìŠ¤ ê¸¸ì´ 30 ê°€ì • ì‹œ í•™ìŠµ ìƒ˜í”Œ: {len(trading_hours) - 30:,}ê°œ\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # 2. ì—¬ëŸ¬ í‹°ì»¤ 1ì‹œê°„ ë°ì´í„° (1ë…„)\n",
    "    print(\"\\nğŸ¯ ì—¬ëŸ¬ í‹°ì»¤ 1ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ (1ë…„)\")\n",
    "    tickers = ['AAPL', 'AMZN', 'TSLA', 'GOOGL', 'MSFT']\n",
    "    \n",
    "    all_stock_data = get_multiple_tickers_hourly(tickers, days=365)\n",
    "    \n",
    "    # 3. ìš”ì•½ ë¶„ì„\n",
    "    analyze_data_summary(all_stock_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # 4. 30ë¶„ ê°„ê²© ë¹„êµìš© (60ì¼)\n",
    "    print(\"\\nğŸ¯ 30ë¶„ ê°„ê²© ë°ì´í„° ë¹„êµ (60ì¼)\")\n",
    "    print(\"âš ï¸ 30ë¶„ ê°„ê²©ì€ ìµœëŒ€ 60ì¼ ì œí•œì´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # í˜„ì¬ ë‚ ì§œ í™•ì¸\n",
    "    current_date = datetime.now()\n",
    "    print(f\"í˜„ì¬ ë‚ ì§œ: {current_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    aapl_30m = get_30min_stock_data('AAPL', days=30)  # 30ì¼ë¡œ ì¤„ì—¬ì„œ ì•ˆì „í•˜ê²Œ í…ŒìŠ¤íŠ¸\n",
    "    \n",
    "    if aapl_30m is not None:\n",
    "        print(f\"\\nğŸ“‹ AAPL 30ë¶„ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "        print(aapl_30m[['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']].head())\n",
    "    \n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "    print(\"\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\")\n",
    "    print(\"âœ… 1ì‹œê°„ ê°„ê²© 1ë…„ ë°ì´í„° - LSTM í•™ìŠµì— ìµœì !\")\n",
    "    print(f\"   â†’ ì•½ {365 * 6.5:.0f}ê°œ ê±°ë˜ì‹œê°„ ë°ì´í„° í¬ì¸íŠ¸\")\n",
    "    print(\"   â†’ ì¶©ë¶„í•œ ë°ì´í„° ì–‘ + ì ì ˆí•œ ì‹œê°„ í•´ìƒë„\")\n",
    "    print(\"âš ï¸ 30ë¶„ ê°„ê²©ì€ 60ì¼ ì œí•œìœ¼ë¡œ ë°ì´í„° ë¶€ì¡±\")\n",
    "    print(\"âš ï¸ ì¼ë³„ ë°ì´í„°ëŠ” ì‹œê°„ í•´ìƒë„ ë¶€ì¡±\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rapid api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RapidAPI - twitter241 ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•œ íŠ¸ìœ— í¬ë¡¤ëŸ¬\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('rapidapi_crawler.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RapidAPITweetCrawler:\n",
    "    \"\"\"\n",
    "    RapidAPIì˜ twitter241 ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ìœ—ì„ ìˆ˜ì§‘í•˜ê³  CSVë¡œ ì €ì¥í•˜ëŠ” í¬ë¡¤ëŸ¬.\n",
    "    í˜ì´ì§€ë„¤ì´ì…˜(cursor)ì„ ì²˜ë¦¬í•˜ì—¬ ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ íŠ¸ìœ—ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"\n",
    "        í¬ë¡¤ëŸ¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            api_key: RapidAPIì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤\n",
    "        \"\"\"\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API í‚¤ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://twitter241.p.rapidapi.com/user-tweets\"\n",
    "        self.headers = {\n",
    "            \"x-rapidapi-key\": self.api_key,\n",
    "            \"x-rapidapi-host\": \"twitter241.p.rapidapi.com\"\n",
    "        }\n",
    "        # countë¥¼ ì¦ê°€ì‹œì¼œ í•œë²ˆì— ë” ë§ì€ íŠ¸ìœ— ìš”ì²­ (ìµœëŒ€ 200ê¹Œì§€ ì‹œë„)\n",
    "        self.count_per_request = 200\n",
    "        \n",
    "        # cursor ìºì‹œ ë° ì¤‘ë³µ ë°©ì§€\n",
    "        self.used_cursors = set()\n",
    "\n",
    "    def _parse_tweets_from_response(self, response_json: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        API ì‘ë‹µ JSONì—ì„œ íŠ¸ìœ— ë°ì´í„°ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            response_json: APIë¡œë¶€í„° ë°›ì€ JSON ì‘ë‹µ\n",
    "        \n",
    "        Returns:\n",
    "            ì¶”ì¶œëœ íŠ¸ìœ— ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ({'created_at': ..., 'full_text': ...})\n",
    "        \"\"\"\n",
    "        tweets_data = []\n",
    "        \n",
    "        try:\n",
    "            # 'instructions' ë¦¬ìŠ¤íŠ¸ì—ì„œ 'TimelineAddEntries' íƒ€ì…ì˜ í•­ëª©ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "            instructions = response_json.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "            \n",
    "            timeline_entries = []\n",
    "            for instruction in instructions:\n",
    "                if instruction.get('type') == 'TimelineAddEntries':\n",
    "                    timeline_entries = instruction.get('entries', [])\n",
    "                    break\n",
    "            \n",
    "            if not timeline_entries:\n",
    "                logger.warning(\"ì‘ë‹µì—ì„œ 'entries'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return []\n",
    "\n",
    "            for entry in timeline_entries:\n",
    "                # 'TimelineTweet' íƒ€ì…ì˜ ì½˜í…ì¸ ë§Œ ì²˜ë¦¬\n",
    "                item_content = entry.get('content', {}).get('itemContent', {})\n",
    "                if item_content and item_content.get('itemType') == 'TimelineTweet':\n",
    "                    tweet_results = item_content.get('tweet_results', {})\n",
    "                    result = tweet_results.get('result', {})\n",
    "                    \n",
    "                    # legacy í•„ë“œì— ì‹¤ì œ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "                    legacy_data = result.get('legacy', {})\n",
    "                    \n",
    "                    if legacy_data:\n",
    "                        created_at = legacy_data.get('created_at', 'N/A')\n",
    "                        full_text = \"\"\n",
    "                        \n",
    "                        # ë¦¬íŠ¸ìœ—(RT)ì¸ ê²½ìš° ì›ë³¸ íŠ¸ìœ—ì˜ full_textë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "                        # 'retweeted_status_result' í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "                        if 'retweeted_status_result' in legacy_data:\n",
    "                            # ì›ë³¸ íŠ¸ìœ—ì˜ legacy ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "                            original_tweet_legacy = legacy_data.get('retweeted_status_result', {}).get('result', {}).get('legacy', {})\n",
    "                            full_text = original_tweet_legacy.get('full_text', '')\n",
    "                        else:\n",
    "                            # ì¼ë°˜ íŠ¸ìœ—ì€ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ full_textë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "                            full_text = legacy_data.get('full_text', '')\n",
    "\n",
    "                        # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì–‘ ë ê³µë°± ì œê±°\n",
    "                        full_text = full_text.replace('\\n', ' ').strip()\n",
    "                        \n",
    "                        tweets_data.append({\n",
    "                            'created_at': created_at,\n",
    "                            'full_text': full_text\n",
    "                        })\n",
    "        except (AttributeError, KeyError, IndexError) as e:\n",
    "            logger.error(f\"íŠ¸ìœ— ë°ì´í„° íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            logger.debug(f\"ì˜¤ë¥˜ ë°œìƒ ì§€ì ì˜ JSON êµ¬ì¡°: {json.dumps(response_json, indent=2, ensure_ascii=False)}\")\n",
    "            \n",
    "        return tweets_data\n",
    "\n",
    "    def _find_next_cursor(self, response_json: Dict[str, Any]) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        API ì‘ë‹µì—ì„œ ë‹¤ìŒ í˜ì´ì§€ë¥¼ ìœ„í•œ cursor ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "        ê°œì„ ëœ cursor íŒŒì‹±ìœ¼ë¡œ ë” ë§ì€ cursor íƒ€ì…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            response_json: APIë¡œë¶€í„° ë°›ì€ JSON ì‘ë‹µ\n",
    "            \n",
    "        Returns:\n",
    "            ë‹¤ìŒ í˜ì´ì§€ cursor ë¬¸ìì—´ ë˜ëŠ” None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            instructions = response_json.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "            \n",
    "            # ëª¨ë“  instruction íƒ€ì…ì—ì„œ cursor ì°¾ê¸°\n",
    "            all_cursors = []\n",
    "            \n",
    "            for instruction in instructions:\n",
    "                # TimelineAddEntriesì—ì„œ cursor ì°¾ê¸°\n",
    "                if instruction.get('type') == 'TimelineAddEntries':\n",
    "                    entries = instruction.get('entries', [])\n",
    "                    for entry in entries:\n",
    "                        content = entry.get('content', {})\n",
    "                        if content.get('entryType') == 'TimelineTimelineCursor':\n",
    "                            cursor_value = content.get('value')\n",
    "                            cursor_type = content.get('cursorType', '')\n",
    "                            \n",
    "                            if cursor_value and cursor_value not in self.used_cursors:\n",
    "                                all_cursors.append({\n",
    "                                    'value': cursor_value,\n",
    "                                    'type': cursor_type,\n",
    "                                    'priority': 1 if cursor_type == 'Bottom' else 2\n",
    "                                })\n",
    "                \n",
    "                # TimelineReplaceEntryì—ì„œë„ cursor ì°¾ê¸°\n",
    "                elif instruction.get('type') == 'TimelineReplaceEntry':\n",
    "                    entry = instruction.get('entry', {})\n",
    "                    content = entry.get('content', {})\n",
    "                    if content.get('entryType') == 'TimelineTimelineCursor':\n",
    "                        cursor_value = content.get('value')\n",
    "                        cursor_type = content.get('cursorType', '')\n",
    "                        \n",
    "                        if cursor_value and cursor_value not in self.used_cursors:\n",
    "                            all_cursors.append({\n",
    "                                'value': cursor_value,\n",
    "                                'type': cursor_type,\n",
    "                                'priority': 1 if cursor_type == 'Bottom' else 2\n",
    "                            })\n",
    "            \n",
    "            # cursorë¥¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬ (Bottomì´ ìš°ì„ )\n",
    "            if all_cursors:\n",
    "                all_cursors.sort(key=lambda x: x['priority'])\n",
    "                selected_cursor = all_cursors[0]['value']\n",
    "                self.used_cursors.add(selected_cursor)\n",
    "                logger.debug(f\"ì„ íƒëœ cursor: {selected_cursor[:50]}... (íƒ€ì…: {all_cursors[0]['type']})\")\n",
    "                return selected_cursor\n",
    "                \n",
    "        except (AttributeError, KeyError, IndexError) as e:\n",
    "            logger.error(f\"Cursor íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            \n",
    "        return None\n",
    "\n",
    "    def fetch_user_tweets(self, user_id: str, max_tweets: int = 1000):\n",
    "        \"\"\"\n",
    "        íŠ¹ì • ì‚¬ìš©ìì˜ íŠ¸ìœ—ì„ ìˆ˜ì§‘í•˜ì—¬ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        ê°œì„ ëœ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ë” ë§ì€ íŠ¸ìœ—ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            user_id: íŠ¸ìœ—ì„ ìˆ˜ì§‘í•  ì‚¬ìš©ìì˜ ID\n",
    "            max_tweets: ìˆ˜ì§‘í•  ìµœëŒ€ íŠ¸ìœ— ìˆ˜\n",
    "        \"\"\"\n",
    "        logger.info(f\"ì‚¬ìš©ì ID {user_id}ì˜ íŠ¸ìœ— ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤. ëª©í‘œ: {max_tweets}ê°œ\")\n",
    "        logger.info(f\"í•œ ë²ˆì˜ ìš”ì²­ë‹¹ {self.count_per_request}ê°œ íŠ¸ìœ— ìš”ì²­\")\n",
    "        \n",
    "        all_tweets = []\n",
    "        cursor = None\n",
    "        request_count = 0\n",
    "        max_requests = 100  # ë¬´í•œ ë£¨í”„ ë°©ì§€\n",
    "        consecutive_empty_responses = 0\n",
    "        \n",
    "        # cursor ìºì‹œ ì´ˆê¸°í™”\n",
    "        self.used_cursors.clear()\n",
    "        \n",
    "        while len(all_tweets) < max_tweets and request_count < max_requests:\n",
    "            # countë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì • (ë‚¨ì€ íŠ¸ìœ— ìˆ˜ì— ë”°ë¼)\n",
    "            remaining_tweets = max_tweets - len(all_tweets)\n",
    "            current_count = min(self.count_per_request, remaining_tweets)\n",
    "            \n",
    "            querystring = {\n",
    "                \"user\": user_id,\n",
    "                \"count\": str(current_count)\n",
    "            }\n",
    "            if cursor:\n",
    "                querystring[\"cursor\"] = cursor\n",
    "            \n",
    "            logger.info(f\"API ìš”ì²­ #{request_count + 1}: {len(all_tweets)} / {max_tweets} ìˆ˜ì§‘ë¨. Count: {current_count}\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.base_url, headers=self.headers, params=querystring, timeout=45)\n",
    "                request_count += 1\n",
    "                \n",
    "                if response.status_code == 429:  # Rate limit\n",
    "                    logger.warning(\"Rate limitì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. 60ì´ˆ ëŒ€ê¸°...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                elif response.status_code != 200:\n",
    "                    logger.error(f\"API ì—ëŸ¬: {response.status_code} - {response.text}\")\n",
    "                    if response.status_code >= 500:  # ì„œë²„ ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„\n",
    "                        logger.info(\"ì„œë²„ ì—ëŸ¬ë¡œ ì¸í•œ 10ì´ˆ í›„ ì¬ì‹œë„...\")\n",
    "                        time.sleep(10)\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                    \n",
    "                data = response.json()\n",
    "                \n",
    "                newly_fetched_tweets = self._parse_tweets_from_response(data)\n",
    "                \n",
    "                if not newly_fetched_tweets:\n",
    "                    consecutive_empty_responses += 1\n",
    "                    logger.warning(f\"ì´ë²ˆ ì‘ë‹µì—ì„œ íŠ¸ìœ—ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({consecutive_empty_responses}/3)\")\n",
    "                    \n",
    "                    if consecutive_empty_responses >= 3:\n",
    "                        logger.info(\"ì—°ì† 3íšŒ ë¹ˆ ì‘ë‹µìœ¼ë¡œ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_empty_responses = 0\n",
    "                    logger.info(f\"ì´ë²ˆ ìš”ì²­ì—ì„œ {len(newly_fetched_tweets)}ê°œ íŠ¸ìœ— ìˆ˜ì§‘\")\n",
    "                \n",
    "                all_tweets.extend(newly_fetched_tweets)\n",
    "                \n",
    "                # ì¤‘ë³µ ì œê±° (created_at + full_text ê¸°ì¤€)\n",
    "                seen = set()\n",
    "                unique_tweets = []\n",
    "                for tweet in all_tweets:\n",
    "                    tweet_key = (tweet['created_at'], tweet['full_text'])\n",
    "                    if tweet_key not in seen:\n",
    "                        seen.add(tweet_key)\n",
    "                        unique_tweets.append(tweet)\n",
    "                \n",
    "                all_tweets = unique_tweets\n",
    "                logger.info(f\"ì¤‘ë³µ ì œê±° í›„: {len(all_tweets)}ê°œ íŠ¸ìœ—\")\n",
    "                \n",
    "                # ë‹¤ìŒ cursor ì°¾ê¸°\n",
    "                next_cursor = self._find_next_cursor(data)\n",
    "                if not next_cursor or next_cursor == cursor:\n",
    "                    logger.info(\"ë” ì´ìƒ ì‚¬ìš© ê°€ëŠ¥í•œ cursorê°€ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "                    break\n",
    "                \n",
    "                cursor = next_cursor\n",
    "\n",
    "                # API rate limitë¥¼ ê³ ë ¤í•œ ëŒ€ê¸° ì‹œê°„ (ìš”ì²­ ìˆ˜ì— ë”°ë¼ ì¡°ì •)\n",
    "                if request_count % 10 == 0:  # 10ë²ˆì§¸ë§ˆë‹¤ ê¸´ ëŒ€ê¸°\n",
    "                    wait_time = 5\n",
    "                else:\n",
    "                    wait_time = 1\n",
    "                    \n",
    "                logger.debug(f\"{wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "            except requests.exceptions.Timeout:\n",
    "                logger.warning(\"ìš”ì²­ íƒ€ì„ì•„ì›ƒ. 5ì´ˆ í›„ ì¬ì‹œë„...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            except json.JSONDecodeError:\n",
    "                logger.error(\"JSON ë””ì½”ë”© ì˜¤ë¥˜. ì‘ë‹µì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"ì´ {len(all_tweets)}ê°œì˜ íŠ¸ìœ—ì„ {request_count}ë²ˆì˜ ìš”ì²­ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        logger.info(f\"í‰ê·  ìš”ì²­ë‹¹ íŠ¸ìœ— ìˆ˜: {len(all_tweets) / request_count if request_count > 0 else 0:.1f}ê°œ\")\n",
    "        \n",
    "        if all_tweets:\n",
    "            filename = f\"user_{user_id}_tweets_ReTweet.csv\"\n",
    "            self._save_to_csv(all_tweets, filename)\n",
    "            \n",
    "    def _save_to_csv(self, tweets_list: List[Dict[str, str]], filename: str):\n",
    "        \"\"\"\n",
    "        ìˆ˜ì§‘ëœ íŠ¸ìœ— ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            tweets_list: ì €ì¥í•  íŠ¸ìœ— ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "            filename: ì €ì¥í•  íŒŒì¼ ì´ë¦„\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "                # 'utf-8-sig'ëŠ” Excelì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ BOMì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "                writer = csv.DictWriter(f, fieldnames=['created_at', 'full_text'])\n",
    "                writer.writeheader()\n",
    "                writer.writerows(tweets_list)\n",
    "            logger.info(f\"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "        except IOError as e:\n",
    "            logger.error(f\"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ ë©”ì¸ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"  RapidAPI(twitter241) ê¸°ë°˜ íŠ¸ìœ— í¬ë¡¤ëŸ¬ (ê°œì„ ëœ ë²„ì „)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # --- ì„¤ì • ---\n",
    "    # ë³´ì•ˆì„ ìœ„í•´ API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
    "    # ì˜ˆ: api_key = os.getenv(\"RAPIDAPI_KEY\")\n",
    "    API_KEY = \"5fac920861msh988e449f8d91b60p10459bjsnba691d3d2d81\" # ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ í•˜ë“œì½”ë”©\n",
    "    USER_ID = \"86437069\"\n",
    "    # @WhiteHouse 1879644163769335808\n",
    "    # @SecScottBessent 1889019333960998912\n",
    "    # @JDVance 1542228578\n",
    "    # @marcorubio 15745368\n",
    "    # @elonmusk 44196397\n",
    "    MAX_TWEETS = 1000\n",
    "    \n",
    "    if not API_KEY:\n",
    "        print(\"[ERROR] API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"ëŒ€ìƒ ì‚¬ìš©ì ID: {USER_ID}\")\n",
    "    print(f\"ìˆ˜ì§‘ ëª©í‘œ íŠ¸ìœ— ìˆ˜: {MAX_TWEETS}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    crawler = RapidAPITweetCrawler(api_key=API_KEY)\n",
    "    crawler.fetch_user_tweets(user_id=USER_ID, max_tweets=MAX_TWEETS)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ê²°ê³¼ëŠ” user_{USER_ID}_tweets_ReTweet.csv íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + DNN ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë³‘í•© ì™„ë£Œ: news_stock_classification.csv ì €ì¥ë¨\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "stock_path = \"./AAPL_1hour_data_365days.csv\"\n",
    "news_path = \"./apple_finbert_finnhub.csv\"\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "stock_df = pd.read_csv(stock_path, parse_dates=[\"Datetime\"])\n",
    "news_df = pd.read_csv(news_path, parse_dates=[\"pubDate\"])\n",
    "\n",
    "# íƒ€ì„ì¡´ ì œê±°\n",
    "stock_df[\"Datetime\"] = stock_df[\"Datetime\"].dt.tz_localize(None)\n",
    "news_df[\"pubDate\"] = news_df[\"pubDate\"].dt.tz_localize(None)\n",
    "\n",
    "# ì •ë ¬\n",
    "stock_df = stock_df.sort_values(\"Datetime\").reset_index(drop=True)\n",
    "\n",
    "# ì œì™¸í•  ì—´\n",
    "exclude_cols = ['Is_Trading_Hours', 'Is_Market_Open', 'Is_Premarket', 'Is_Aftermarket', 'Is_Extended_Hours']\n",
    "stock_df = stock_df.drop(columns=[col for col in exclude_cols if col in stock_df.columns])\n",
    "\n",
    "# ë³‘í•© ê²°ê³¼\n",
    "rows = []\n",
    "\n",
    "for _, news_row in news_df.iterrows():\n",
    "    news_time = news_row['pubDate']\n",
    "\n",
    "    # ë‰´ìŠ¤ ì´í›„ ê°€ì¥ ê°€ê¹Œìš´ ì£¼ê°€\n",
    "    future_stock = stock_df[stock_df['Datetime'] > news_time].head(1)\n",
    "    if future_stock.empty:\n",
    "        continue\n",
    "\n",
    "    target_row = future_stock.iloc[0]\n",
    "    target_time = target_row['Datetime']\n",
    "    target_close = target_row['Close']\n",
    "\n",
    "    # ê³¼ê±° 3ê°œ ì£¼ê°€\n",
    "    past_rows = stock_df[stock_df['Datetime'] < target_time].tail(3)\n",
    "    if len(past_rows) < 3:\n",
    "        continue\n",
    "\n",
    "    past_last_close = past_rows.iloc[-1]['Close']\n",
    "\n",
    "    # ìƒìŠ¹ë¥ \n",
    "    return_pct = (target_close - past_last_close) / past_last_close * 100\n",
    "    label = 1 if return_pct >= 0.4 else (-1 if return_pct <= -0.4 else 0)\n",
    "\n",
    "    # ë³‘í•© row ìƒì„±\n",
    "    row = {\n",
    "        \"news_id\": news_row['id'],\n",
    "        \"news_time\": news_time,\n",
    "        \"target_close\": target_close,\n",
    "        \"target_return_pct\": return_pct,\n",
    "        \"target_multi_raw\": label,\n",
    "        \"finbert_positive\": news_row['finbert_positive'],\n",
    "        \"finbert_neutral\": news_row['finbert_neutral'],\n",
    "        \"finbert_negative\": news_row['finbert_negative'],\n",
    "    }\n",
    "\n",
    "    # ê³¼ê±° 3ê°œ flatten\n",
    "    for i, (_, stock_row) in enumerate(past_rows.iterrows(), 1):\n",
    "        for col in stock_df.columns:\n",
    "            if col == \"Datetime\":\n",
    "                continue\n",
    "            row[f\"x{i}_{col}\"] = stock_row[col]\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "# ìµœì¢… DataFrame\n",
    "merged_df = pd.DataFrame(rows)\n",
    "\n",
    "# í´ë˜ìŠ¤ 0/1/2ë¡œ ë§¤í•‘ (XGBoostìš©)\n",
    "label_map = {-1: 0, 0: 1, 1: 2}\n",
    "merged_df[\"target_multi\"] = merged_df[\"target_multi_raw\"].map(label_map)\n",
    "\n",
    "# ì €ì¥\n",
    "merged_df.to_csv(\"news_stock_classification.csv\", index=False)\n",
    "print(\"ë³‘í•© ì™„ë£Œ: news_stock_classification.csv ì €ì¥ë¨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 165.6778\n",
      "Epoch 2 | Loss: 139.9572\n",
      "Epoch 3 | Loss: 124.0224\n",
      "Epoch 4 | Loss: 111.9599\n",
      "Epoch 5 | Loss: 102.0316\n",
      "Epoch 6 | Loss: 91.2283\n",
      "Epoch 7 | Loss: 82.4791\n",
      "Epoch 8 | Loss: 74.9245\n",
      "Epoch 9 | Loss: 68.3347\n",
      "Epoch 10 | Loss: 62.1140\n",
      "Epoch 11 | Loss: 56.4879\n",
      "Epoch 12 | Loss: 51.9471\n",
      "Epoch 13 | Loss: 47.4376\n",
      "Epoch 14 | Loss: 43.8949\n",
      "Epoch 15 | Loss: 39.6787\n",
      "Epoch 16 | Loss: 36.4289\n",
      "Epoch 17 | Loss: 33.2387\n",
      "Epoch 18 | Loss: 30.9987\n",
      "Epoch 19 | Loss: 28.0263\n",
      "Epoch 20 | Loss: 25.6803\n",
      "Epoch 21 | Loss: 23.1786\n",
      "Epoch 22 | Loss: 21.3123\n",
      "Epoch 23 | Loss: 19.6293\n",
      "Epoch 24 | Loss: 17.5688\n",
      "Epoch 25 | Loss: 15.6692\n",
      "Epoch 26 | Loss: 14.3345\n",
      "Epoch 27 | Loss: 12.6775\n",
      "Epoch 28 | Loss: 11.6631\n",
      "Epoch 29 | Loss: 10.7014\n",
      "Epoch 30 | Loss: 9.4993\n",
      "Epoch 31 | Loss: 8.4388\n",
      "Epoch 32 | Loss: 7.4168\n",
      "Epoch 33 | Loss: 7.0659\n",
      "Epoch 34 | Loss: 5.9982\n",
      "Epoch 35 | Loss: 5.2894\n",
      "Epoch 36 | Loss: 4.8408\n",
      "Epoch 37 | Loss: 4.3750\n",
      "Epoch 38 | Loss: 3.9858\n",
      "Epoch 39 | Loss: 4.1333\n",
      "Epoch 40 | Loss: 3.2084\n",
      "Epoch 41 | Loss: 2.7471\n",
      "Epoch 42 | Loss: 3.0587\n",
      "Epoch 43 | Loss: 4.0325\n",
      "Epoch 44 | Loss: 2.0774\n",
      "Epoch 45 | Loss: 2.0874\n",
      "Epoch 46 | Loss: 1.6183\n",
      "Epoch 47 | Loss: 2.0426\n",
      "Epoch 48 | Loss: 1.9357\n",
      "Epoch 49 | Loss: 1.2066\n",
      "Epoch 50 | Loss: 1.1955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"news_stock_classification.csv\", parse_dates=[\"news_time\"])\n",
    "\n",
    "# 2. Feature ë° Label ì¤€ë¹„\n",
    "feature_cols = [col for col in df.columns if col.startswith(\"x\") or col.startswith(\"finbert_\")]\n",
    "X = df[feature_cols].fillna(0)\n",
    "y = df[\"target_multi\"]\n",
    "\n",
    "# 3. ì‹œê³„ì—´ ë°ì´í„° 3-step ìƒì„± (x1_, x2_, x3_)\n",
    "X_seq = []\n",
    "for i in range(len(X)):\n",
    "    X_seq.append([\n",
    "        X.iloc[i][[col for col in X.columns if col.startswith(\"x1_\")]].values,\n",
    "        X.iloc[i][[col for col in X.columns if col.startswith(\"x2_\")]].values,\n",
    "        X.iloc[i][[col for col in X.columns if col.startswith(\"x3_\")]].values\n",
    "    ])\n",
    "X_seq = np.array(X_seq)\n",
    "\n",
    "# 4. FinBERT í”¼ì²˜ ì¶”ê°€ (Broadcast across time steps)\n",
    "finbert_feats = X[[c for c in X.columns if c.startswith(\"finbert_\")]].values\n",
    "finbert_feats = np.repeat(finbert_feats[:, np.newaxis, :], 3, axis=1)\n",
    "X_seq = np.concatenate([X_seq, finbert_feats], axis=-1)\n",
    "\n",
    "# 5. ì •ê·œí™”\n",
    "n_samples, time_steps, n_features = X_seq.shape\n",
    "X_reshaped = X_seq.reshape(-1, n_features)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_reshaped)\n",
    "X_seq = X_scaled.reshape(n_samples, time_steps, n_features)\n",
    "\n",
    "# 6. Tensorë¡œ ë³€í™˜\n",
    "X_tensor = torch.tensor(X_seq, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# 7. Train/Test ë¶„ë¦¬\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, shuffle=False)\n",
    "train_dl = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(TensorDataset(X_test, y_test), batch_size=32)\n",
    "\n",
    "# 8. LSTM ëª¨ë¸ ì •ì˜\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return self.fc(hn[-1])\n",
    "\n",
    "# 9. í•™ìŠµ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(input_dim=n_features).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 10. í•™ìŠµ ë£¨í”„\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy: 0.5917297612114153\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.13      0.11       246\n",
      "           1       0.77      0.78      0.77      1224\n",
      "           2       0.25      0.11      0.15       247\n",
      "\n",
      "    accuracy                           0.59      1717\n",
      "   macro avg       0.37      0.34      0.34      1717\n",
      "weighted avg       0.59      0.59      0.59      1717\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      " [[ 32 161  53]\n",
      " [237 957  30]\n",
      " [ 90 130  27]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 11. í‰ê°€\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb).argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "print(\"\\n Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(\"\\n Classification Report:\\n\", classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
