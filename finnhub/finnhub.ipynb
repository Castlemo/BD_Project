{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finnhub ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ ë° FinBERT ê°ì •ë¶„ì„\n",
    "\n",
    "1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "2. FinBERT ê°ì •ë¶„ì„ ìˆ˜í–‰\n",
    "3. ìµœì¢… ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ë°ì´í„° í–‰ ìˆ˜: 8769\n",
      "ì»¬ëŸ¼: ['id', 'title', 'summary', 'link', 'publisher', 'category', 'pubDate', 'image', 'related', 'source', 'collection_period']\n",
      "Summary NULL ê°œìˆ˜: 166\n",
      "Title NULL ê°œìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(\"./AAPL_extended_news_2025-06-14.csv\")\n",
    "print(f\"ì›ë³¸ ë°ì´í„° í–‰ ìˆ˜: {len(df)}\")\n",
    "print(f\"ì»¬ëŸ¼: {list(df.columns)}\")\n",
    "print(f\"Summary NULL ê°œìˆ˜: {df['summary'].isnull().sum()}\")\n",
    "print(f\"Title NULL ê°œìˆ˜: {df['title'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ì»¬ëŸ¼ ì „ì²˜ë¦¬ ===\n",
      "titleê³¼ summary ëª¨ë‘ NULLì¸ í–‰ ì œê±°: 0ê°œ\n",
      "í…ìŠ¤íŠ¸ ì •ë¦¬ ì™„ë£Œ\n",
      "ìµœì¢… ë°ì´í„° í–‰ ìˆ˜: 8769\n"
     ]
    }
   ],
   "source": [
    "# 1ë‹¨ê³„: Summary ì»¬ëŸ¼ ì „ì²˜ë¦¬\n",
    "print(\"=== Summary ì»¬ëŸ¼ ì „ì²˜ë¦¬ ===\")\n",
    "\n",
    "# summaryê°€ NULLì¸ ê²½ìš° titleë¡œ ëŒ€ì²´\n",
    "df['summary'] = df['summary'].fillna(df['title'])\n",
    "\n",
    "# titleê³¼ summary ëª¨ë‘ NULLì¸ ê²½ìš° ì œê±°\n",
    "before_drop = len(df)\n",
    "df = df.dropna(subset=['title', 'summary'], how='all')\n",
    "after_drop = len(df)\n",
    "print(f\"titleê³¼ summary ëª¨ë‘ NULLì¸ í–‰ ì œê±°: {before_drop - after_drop}ê°œ\")\n",
    "\n",
    "# summary í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    text = str(text)\n",
    "    \n",
    "    # URL ì œê±°\n",
    "    text = re.sub(r'https?://[^\\s]+', '', text)\n",
    "    text = re.sub(r'www\\.[^\\s]+', '', text)\n",
    "    \n",
    "    # ì¤„ë°”ê¿ˆ, íƒ­ ì œê±°\n",
    "    text = re.sub(r'[\\n\\t\\r]', ' ', text)\n",
    "    \n",
    "    # ì—°ì†ëœ íŠ¹ìˆ˜ë¬¸ì ì œê±° ($$$, ###, ---, === ë“±)\n",
    "    text = re.sub(r'[#$=\\-_*]{3,}', '', text)\n",
    "    text = re.sub(r'[!@#$%^&*()_+=\\[\\]{}|;:\",.<>?/~`]{5,}', '', text)\n",
    "    \n",
    "    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # ì•ë’¤ ê³µë°± ì œê±°\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# summaryì™€ title ì •ë¦¬\n",
    "df['summary'] = df['summary'].apply(clean_text)\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "\n",
    "print(\"í…ìŠ¤íŠ¸ ì •ë¦¬ ì™„ë£Œ\")\n",
    "print(f\"ìµœì¢… ë°ì´í„° í–‰ ìˆ˜: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ë‚ ì§œ ì²˜ë¦¬ ë° ì •ë ¬ ===\n",
      "ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨í•œ í–‰ ì œê±°: 2ê°œ\n",
      "ë‚ ì§œ ë²”ìœ„: 2023-03-22 18:22:51 ~ 2025-06-14 09:55:00\n",
      "ë¹ˆ í…ìŠ¤íŠ¸ ì œê±° í›„ ìµœì¢… í–‰ ìˆ˜: 8767\n"
     ]
    }
   ],
   "source": [
    "# 2ë‹¨ê³„: ë‚ ì§œ ì²˜ë¦¬ ë° ì •ë ¬\n",
    "print(\"\\n=== ë‚ ì§œ ì²˜ë¦¬ ë° ì •ë ¬ ===\")\n",
    "\n",
    "# pubDateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜\n",
    "df['pubDate'] = pd.to_datetime(df['pubDate'], errors='coerce')\n",
    "\n",
    "# ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨í•œ í–‰ ì œê±°\n",
    "before_date_drop = len(df)\n",
    "df = df.dropna(subset=['pubDate'])\n",
    "after_date_drop = len(df)\n",
    "print(f\"ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨í•œ í–‰ ì œê±°: {before_date_drop - after_date_drop}ê°œ\")\n",
    "\n",
    "# ë‚ ì§œìˆœ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)\n",
    "df = df.sort_values('pubDate', ascending=False).reset_index(drop=True)\n",
    "print(f\"ë‚ ì§œ ë²”ìœ„: {df['pubDate'].min()} ~ {df['pubDate'].max()}\")\n",
    "\n",
    "# full_text ìƒì„± (title + summary)\n",
    "df['full_text'] = df['title'] + \". \" + df['summary']\n",
    "# ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°\n",
    "df = df[df['full_text'].str.strip() != \"\"].reset_index(drop=True)\n",
    "print(f\"ë¹ˆ í…ìŠ¤íŠ¸ ì œê±° í›„ ìµœì¢… í–‰ ìˆ˜: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\82102\\anaconda3\\lib\\site-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\82102\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FinBERT ê°ì •ë¶„ì„ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ì •ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸ ìˆ˜: 8767\n",
      "í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„ ì¤‘...\n",
      "í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„ (í† í° ìˆ˜, ìƒ˜í”Œ 100ê°œ):\n",
      "  í‰ê· : 66.9\n",
      "  ìµœëŒ€: 189\n",
      "  ìµœì†Œ: 19\n",
      "  512 í† í° ì´ˆê³¼í•˜ëŠ” í…ìŠ¤íŠ¸: 0ê°œ\n",
      "\n",
      "ğŸ”„ ê°ì •ë¶„ì„ ì‹œì‘... (ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
      "âœ… ê°ì •ë¶„ì„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# 3ë‹¨ê³„: FinBERT ê°ì •ë¶„ì„ (ì˜¤ë¥˜ ìˆ˜ì •)\n",
    "print(\"\\n=== FinBERT ê°ì •ë¶„ì„ ===\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "MODEL = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# â­ í•´ê²°ì±…: truncation=True, max_length=512 ì¶”ê°€í•˜ì—¬ ê¸´ í…ìŠ¤íŠ¸ ìë¥´ê¸°\n",
    "finbert = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    top_k=None,\n",
    "    truncation=True,      # í…ìŠ¤íŠ¸ ìë¥´ê¸° í™œì„±í™” âœ‚ï¸\n",
    "    max_length=512,       # ìµœëŒ€ 512 í† í°ìœ¼ë¡œ ì œí•œ\n",
    "    padding=True          # íŒ¨ë”© ì¶”ê°€\n",
    ")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸\n",
    "texts = df['full_text'].tolist()\n",
    "print(f\"ê°ì •ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸ ìˆ˜: {len(texts)}\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„ í™•ì¸ (ì²˜ìŒ 100ê°œ ìƒ˜í”Œ)\n",
    "print(\"í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„ ì¤‘...\")\n",
    "sample_lengths = []\n",
    "for i, text in enumerate(texts[:100]):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    sample_lengths.append(len(tokens))\n",
    "\n",
    "print(f\"í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„ (í† í° ìˆ˜, ìƒ˜í”Œ 100ê°œ):\")\n",
    "print(f\"  í‰ê· : {sum(sample_lengths)/len(sample_lengths):.1f}\")\n",
    "print(f\"  ìµœëŒ€: {max(sample_lengths)}\")\n",
    "print(f\"  ìµœì†Œ: {min(sample_lengths)}\")\n",
    "print(f\"  512 í† í° ì´ˆê³¼í•˜ëŠ” í…ìŠ¤íŠ¸: {sum(1 for x in sample_lengths if x > 512)}ê°œ\")\n",
    "\n",
    "# ê°ì •ë¶„ì„ ì‹¤í–‰\n",
    "print(\"\\nğŸ”„ ê°ì •ë¶„ì„ ì‹œì‘... (ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "results = finbert(texts, batch_size=4)  # ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "print(\"âœ… ê°ì •ë¶„ì„ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ê°ì •ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ===\n",
      "ê°ì •ë¶„ì„ ê²°ê³¼:\n",
      "- Positive: 2384ê°œ\n",
      "- Neutral: 4731ê°œ\n",
      "- Negative: 1652ê°œ\n"
     ]
    }
   ],
   "source": [
    "# 4ë‹¨ê³„: ê°ì •ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ë° ìµœì¢… ë°ì´í„°í”„ë ˆì„ êµ¬ì„±\n",
    "print(\"\\n=== ê°ì •ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ===\")\n",
    "\n",
    "# ê°ì •ë¶„ì„ ì ìˆ˜ ì¶”ì¶œ\n",
    "pos_scores, neu_scores, neg_scores = [], [], []\n",
    "sentiment_labels = []\n",
    "\n",
    "for res in results:\n",
    "    # ê° ê°ì •ì˜ ì ìˆ˜ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬\n",
    "    d = {r[\"label\"].lower(): r[\"score\"] for r in res}\n",
    "    pos_score = d.get(\"positive\", 0.0)\n",
    "    neu_score = d.get(\"neutral\", 0.0)\n",
    "    neg_score = d.get(\"negative\", 0.0)\n",
    "    \n",
    "    pos_scores.append(pos_score)\n",
    "    neu_scores.append(neu_score)\n",
    "    neg_scores.append(neg_score)\n",
    "    \n",
    "    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ê°ì •ì„ ì£¼ ê°ì •ìœ¼ë¡œ ê²°ì •\n",
    "    max_score = max(pos_score, neu_score, neg_score)\n",
    "    if max_score == pos_score:\n",
    "        sentiment_labels.append(\"positive\")\n",
    "    elif max_score == neg_score:\n",
    "        sentiment_labels.append(\"negative\")\n",
    "    else:\n",
    "        sentiment_labels.append(\"neutral\")\n",
    "\n",
    "# ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
    "df['pos'] = pos_scores\n",
    "df['neu'] = neu_scores\n",
    "df['neg'] = neg_scores\n",
    "df['sentiment'] = sentiment_labels\n",
    "\n",
    "print(f\"ê°ì •ë¶„ì„ ê²°ê³¼:\")\n",
    "print(f\"- Positive: {sentiment_labels.count('positive')}ê°œ\")\n",
    "print(f\"- Neutral: {sentiment_labels.count('neutral')}ê°œ\") \n",
    "print(f\"- Negative: {sentiment_labels.count('negative')}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìµœì¢… ê²°ê³¼ ì €ì¥ ===\n",
      "âœ… ìµœì¢… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: AAPL_finnhub_processed_final.csv\n",
      "ğŸ“Š ìµœì¢… ë°ì´í„° í–‰ ìˆ˜: 8767\n",
      "ğŸ“‹ ìµœì¢… ì»¬ëŸ¼: ['Date', 'full_text', 'sentiment', 'neg', 'neu', 'pos']\n",
      "\n",
      "ê°ì •ë¶„ì„ ê²°ê³¼ ë¶„í¬:\n",
      "sentiment\n",
      "neutral     4731\n",
      "positive    2384\n",
      "negative    1652\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ì²˜ìŒ 3ê°œ ìƒ˜í”Œ:\n",
      "                  Date sentiment       pos       neu       neg\n",
      "0  2025-06-14 09:55:00  negative  0.000066  0.000041  0.999893\n",
      "1  2025-06-14 01:30:33   neutral  0.027770  0.944107  0.028124\n",
      "2  2025-06-14 01:00:00  negative  0.000960  0.456238  0.542802\n"
     ]
    }
   ],
   "source": [
    "# 5ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "print(\"\\n=== ìµœì¢… ê²°ê³¼ ì €ì¥ ===\")\n",
    "\n",
    "# ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ìµœì¢… ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "final_df = pd.DataFrame({\n",
    "    'Date': df['pubDate'].dt.strftime('%Y-%m-%d %H:%M:%S'),  # ë‚ ì§œ í¬ë§· ì§€ì •\n",
    "    'full_text': df['full_text'],\n",
    "    'sentiment': df['sentiment'],  # positive/negative/neutral ì¤‘ í•˜ë‚˜\n",
    "    'neg': df['neg'],\n",
    "    'neu': df['neu'], \n",
    "    'pos': df['pos']\n",
    "})\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "output_filename = \"AAPL_finnhub_processed_final.csv\"\n",
    "final_df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"âœ… ìµœì¢… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_filename}\")\n",
    "print(f\"ğŸ“Š ìµœì¢… ë°ì´í„° í–‰ ìˆ˜: {len(final_df)}\")\n",
    "print(f\"ğŸ“‹ ìµœì¢… ì»¬ëŸ¼: {list(final_df.columns)}\")\n",
    "print(f\"\\nê°ì •ë¶„ì„ ê²°ê³¼ ë¶„í¬:\")\n",
    "print(final_df['sentiment'].value_counts())\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "print(f\"\\nì²˜ìŒ 3ê°œ ìƒ˜í”Œ:\")\n",
    "print(final_df[['Date', 'sentiment', 'pos', 'neu', 'neg']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
