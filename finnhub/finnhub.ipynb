{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finnhub 뉴스 데이터 전처리 및 FinBERT 감정분석\n",
    "\n",
    "1. 데이터 로드 및 전처리\n",
    "2. FinBERT 감정분석 수행\n",
    "3. 최종 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터 행 수: 8769\n",
      "컬럼: ['id', 'title', 'summary', 'link', 'publisher', 'category', 'pubDate', 'image', 'related', 'source', 'collection_period']\n",
      "Summary NULL 개수: 166\n",
      "Title NULL 개수: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(\"./AAPL_extended_news_2025-06-14.csv\")\n",
    "print(f\"원본 데이터 행 수: {len(df)}\")\n",
    "print(f\"컬럼: {list(df.columns)}\")\n",
    "print(f\"Summary NULL 개수: {df['summary'].isnull().sum()}\")\n",
    "print(f\"Title NULL 개수: {df['title'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary 컬럼 전처리 ===\n",
      "title과 summary 모두 NULL인 행 제거: 0개\n",
      "텍스트 정리 완료\n",
      "최종 데이터 행 수: 8769\n"
     ]
    }
   ],
   "source": [
    "# 1단계: Summary 컬럼 전처리\n",
    "print(\"=== Summary 컬럼 전처리 ===\")\n",
    "\n",
    "# summary가 NULL인 경우 title로 대체\n",
    "df['summary'] = df['summary'].fillna(df['title'])\n",
    "\n",
    "# title과 summary 모두 NULL인 경우 제거\n",
    "before_drop = len(df)\n",
    "df = df.dropna(subset=['title', 'summary'], how='all')\n",
    "after_drop = len(df)\n",
    "print(f\"title과 summary 모두 NULL인 행 제거: {before_drop - after_drop}개\")\n",
    "\n",
    "# summary 텍스트 정리 함수\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # 문자열로 변환\n",
    "    text = str(text)\n",
    "    \n",
    "    # URL 제거\n",
    "    text = re.sub(r'https?://[^\\s]+', '', text)\n",
    "    text = re.sub(r'www\\.[^\\s]+', '', text)\n",
    "    \n",
    "    # 줄바꿈, 탭 제거\n",
    "    text = re.sub(r'[\\n\\t\\r]', ' ', text)\n",
    "    \n",
    "    # 연속된 특수문자 제거 ($$$, ###, ---, === 등)\n",
    "    text = re.sub(r'[#$=\\-_*]{3,}', '', text)\n",
    "    text = re.sub(r'[!@#$%^&*()_+=\\[\\]{}|;:\",.<>?/~`]{5,}', '', text)\n",
    "    \n",
    "    # 연속된 공백을 하나로\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 앞뒤 공백 제거\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# summary와 title 정리\n",
    "df['summary'] = df['summary'].apply(clean_text)\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "\n",
    "print(\"텍스트 정리 완료\")\n",
    "print(f\"최종 데이터 행 수: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 날짜 처리 및 정렬 ===\n",
      "날짜 변환 실패한 행 제거: 2개\n",
      "날짜 범위: 2023-03-22 18:22:51 ~ 2025-06-14 09:55:00\n",
      "빈 텍스트 제거 후 최종 행 수: 8767\n"
     ]
    }
   ],
   "source": [
    "# 2단계: 날짜 처리 및 정렬\n",
    "print(\"\\n=== 날짜 처리 및 정렬 ===\")\n",
    "\n",
    "# pubDate를 datetime으로 변환\n",
    "df['pubDate'] = pd.to_datetime(df['pubDate'], errors='coerce')\n",
    "\n",
    "# 날짜 변환 실패한 행 제거\n",
    "before_date_drop = len(df)\n",
    "df = df.dropna(subset=['pubDate'])\n",
    "after_date_drop = len(df)\n",
    "print(f\"날짜 변환 실패한 행 제거: {before_date_drop - after_date_drop}개\")\n",
    "\n",
    "# 날짜순 내림차순 정렬 (최신순)\n",
    "df = df.sort_values('pubDate', ascending=False).reset_index(drop=True)\n",
    "print(f\"날짜 범위: {df['pubDate'].min()} ~ {df['pubDate'].max()}\")\n",
    "\n",
    "# full_text 생성 (title + summary)\n",
    "df['full_text'] = df['title'] + \". \" + df['summary']\n",
    "# 빈 텍스트 제거\n",
    "df = df[df['full_text'].str.strip() != \"\"].reset_index(drop=True)\n",
    "print(f\"빈 텍스트 제거 후 최종 행 수: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\82102\\anaconda3\\lib\\site-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\82102\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FinBERT 감정분석 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정분석 대상 텍스트 수: 8767\n",
      "텍스트 길이 분석 중...\n",
      "텍스트 길이 통계 (토큰 수, 샘플 100개):\n",
      "  평균: 66.9\n",
      "  최대: 189\n",
      "  최소: 19\n",
      "  512 토큰 초과하는 텍스트: 0개\n",
      "\n",
      "🔄 감정분석 시작... (시간이 다소 걸릴 수 있습니다)\n",
      "✅ 감정분석 완료!\n"
     ]
    }
   ],
   "source": [
    "# 3단계: FinBERT 감정분석 (오류 수정)\n",
    "print(\"\\n=== FinBERT 감정분석 ===\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "MODEL = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# ⭐ 해결책: truncation=True, max_length=512 추가하여 긴 텍스트 자르기\n",
    "finbert = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    top_k=None,\n",
    "    truncation=True,      # 텍스트 자르기 활성화 ✂️\n",
    "    max_length=512,       # 최대 512 토큰으로 제한\n",
    "    padding=True          # 패딩 추가\n",
    ")\n",
    "\n",
    "# 텍스트 길이 확인\n",
    "texts = df['full_text'].tolist()\n",
    "print(f\"감정분석 대상 텍스트 수: {len(texts)}\")\n",
    "\n",
    "# 텍스트 길이 통계 확인 (처음 100개 샘플)\n",
    "print(\"텍스트 길이 분석 중...\")\n",
    "sample_lengths = []\n",
    "for i, text in enumerate(texts[:100]):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    sample_lengths.append(len(tokens))\n",
    "\n",
    "print(f\"텍스트 길이 통계 (토큰 수, 샘플 100개):\")\n",
    "print(f\"  평균: {sum(sample_lengths)/len(sample_lengths):.1f}\")\n",
    "print(f\"  최대: {max(sample_lengths)}\")\n",
    "print(f\"  최소: {min(sample_lengths)}\")\n",
    "print(f\"  512 토큰 초과하는 텍스트: {sum(1 for x in sample_lengths if x > 512)}개\")\n",
    "\n",
    "# 감정분석 실행\n",
    "print(\"\\n🔄 감정분석 시작... (시간이 다소 걸릴 수 있습니다)\")\n",
    "results = finbert(texts, batch_size=4)  # 배치 크기를 줄여서 메모리 절약\n",
    "print(\"✅ 감정분석 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 감정분석 결과 처리 ===\n",
      "감정분석 결과:\n",
      "- Positive: 2384개\n",
      "- Neutral: 4731개\n",
      "- Negative: 1652개\n"
     ]
    }
   ],
   "source": [
    "# 4단계: 감정분석 결과 처리 및 최종 데이터프레임 구성\n",
    "print(\"\\n=== 감정분석 결과 처리 ===\")\n",
    "\n",
    "# 감정분석 점수 추출\n",
    "pos_scores, neu_scores, neg_scores = [], [], []\n",
    "sentiment_labels = []\n",
    "\n",
    "for res in results:\n",
    "    # 각 감정의 점수를 딕셔너리로 정리\n",
    "    d = {r[\"label\"].lower(): r[\"score\"] for r in res}\n",
    "    pos_score = d.get(\"positive\", 0.0)\n",
    "    neu_score = d.get(\"neutral\", 0.0)\n",
    "    neg_score = d.get(\"negative\", 0.0)\n",
    "    \n",
    "    pos_scores.append(pos_score)\n",
    "    neu_scores.append(neu_score)\n",
    "    neg_scores.append(neg_score)\n",
    "    \n",
    "    # 가장 높은 점수의 감정을 주 감정으로 결정\n",
    "    max_score = max(pos_score, neu_score, neg_score)\n",
    "    if max_score == pos_score:\n",
    "        sentiment_labels.append(\"positive\")\n",
    "    elif max_score == neg_score:\n",
    "        sentiment_labels.append(\"negative\")\n",
    "    else:\n",
    "        sentiment_labels.append(\"neutral\")\n",
    "\n",
    "# 결과를 데이터프레임에 추가\n",
    "df['pos'] = pos_scores\n",
    "df['neu'] = neu_scores\n",
    "df['neg'] = neg_scores\n",
    "df['sentiment'] = sentiment_labels\n",
    "\n",
    "print(f\"감정분석 결과:\")\n",
    "print(f\"- Positive: {sentiment_labels.count('positive')}개\")\n",
    "print(f\"- Neutral: {sentiment_labels.count('neutral')}개\") \n",
    "print(f\"- Negative: {sentiment_labels.count('negative')}개\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 최종 결과 저장 ===\n",
      "✅ 최종 전처리된 데이터 저장 완료: AAPL_finnhub_processed_final.csv\n",
      "📊 최종 데이터 행 수: 8767\n",
      "📋 최종 컬럼: ['Date', 'full_text', 'sentiment', 'neg', 'neu', 'pos']\n",
      "\n",
      "감정분석 결과 분포:\n",
      "sentiment\n",
      "neutral     4731\n",
      "positive    2384\n",
      "negative    1652\n",
      "Name: count, dtype: int64\n",
      "\n",
      "처음 3개 샘플:\n",
      "                  Date sentiment       pos       neu       neg\n",
      "0  2025-06-14 09:55:00  negative  0.000066  0.000041  0.999893\n",
      "1  2025-06-14 01:30:33   neutral  0.027770  0.944107  0.028124\n",
      "2  2025-06-14 01:00:00  negative  0.000960  0.456238  0.542802\n"
     ]
    }
   ],
   "source": [
    "# 5단계: 최종 결과 저장\n",
    "print(\"\\n=== 최종 결과 저장 ===\")\n",
    "\n",
    "# 요구사항에 맞는 컬럼만 선택하여 최종 데이터프레임 생성\n",
    "final_df = pd.DataFrame({\n",
    "    'Date': df['pubDate'].dt.strftime('%Y-%m-%d %H:%M:%S'),  # 날짜 포맷 지정\n",
    "    'full_text': df['full_text'],\n",
    "    'sentiment': df['sentiment'],  # positive/negative/neutral 중 하나\n",
    "    'neg': df['neg'],\n",
    "    'neu': df['neu'], \n",
    "    'pos': df['pos']\n",
    "})\n",
    "\n",
    "# 최종 결과 저장\n",
    "output_filename = \"AAPL_finnhub_processed_final.csv\"\n",
    "final_df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ 최종 전처리된 데이터 저장 완료: {output_filename}\")\n",
    "print(f\"📊 최종 데이터 행 수: {len(final_df)}\")\n",
    "print(f\"📋 최종 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"\\n감정분석 결과 분포:\")\n",
    "print(final_df['sentiment'].value_counts())\n",
    "\n",
    "# 샘플 데이터 확인\n",
    "print(f\"\\n처음 3개 샘플:\")\n",
    "print(final_df[['Date', 'sentiment', 'pos', 'neu', 'neg']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
