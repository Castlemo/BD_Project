{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 4. VADER를 이용한 X(Twitter) 트윗 감정분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-1. 필수 라이브러리 Import\n",
    "감정분석과 데이터 처리에 필요한 라이브러리들을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분석 및 조작을 위한 pandas 라이브러리\n",
    "import pandas as pd\n",
    "\n",
    "# VADER 감정분석 모델을 위한 NLTK 라이브러리\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# 정규표현식을 사용한 텍스트 전처리를 위한 re 라이브러리\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-2. VADER 감정분석 사전 다운로드\n",
    "VADER 감정분석에 필요한 어휘 사전을 다운로드합니다. (최초 1회만 실행)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\82102\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VADER 감정분석에 필요한 어휘 사전을 다운로드\n",
    "# vader_lexicon: 감정 점수가 매핑된 단어 사전 데이터\n",
    "# 최초 한 번만 실행하면 로컬에 저장됨\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-3. 다중 사용자 트윗 데이터 병합\n",
    "X_data 디렉토리의 모든 `user_*.csv` 파일들을 하나로 병합하여 통합 데이터셋을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 모든 CSV 파일: ['user_@Ajay_Bagga_tweets.csv', 'user_@BillAckman_tweets.csv', 'user_@CathieDWood_tweets.csv', 'user_@elonmusk_tweets.csv', 'user_@JDVance_tweets.csv', 'user_@LizAnnSonders_tweets.csv', 'user_@marcorubio_tweets.csv', 'user_@michaelbatnick_tweets.csv', 'user_@RayDalio_tweets.csv', 'user_@SecScottBessent_tweets.csv', 'user_@sundarpichai_tweets.csv', 'user_@tim_cook_tweets.csv', 'user_@WhiteHouse_tweets.csv']\n",
      "총 13개의 파일을 병합합니다.\n",
      "user_@Ajay_Bagga_tweets.csv 읽기 완료 - 530개 행\n",
      "user_@BillAckman_tweets.csv 읽기 완료 - 800개 행\n",
      "user_@CathieDWood_tweets.csv 읽기 완료 - 670개 행\n",
      "user_@elonmusk_tweets.csv 읽기 완료 - 764개 행\n",
      "user_@JDVance_tweets.csv 읽기 완료 - 701개 행\n",
      "user_@LizAnnSonders_tweets.csv 읽기 완료 - 673개 행\n",
      "user_@marcorubio_tweets.csv 읽기 완료 - 806개 행\n",
      "user_@michaelbatnick_tweets.csv 읽기 완료 - 760개 행\n",
      "user_@RayDalio_tweets.csv 읽기 완료 - 725개 행\n",
      "user_@SecScottBessent_tweets.csv 읽기 완료 - 255개 행\n",
      "user_@sundarpichai_tweets.csv 읽기 완료 - 585개 행\n",
      "user_@tim_cook_tweets.csv 읽기 완료 - 838개 행\n",
      "user_@WhiteHouse_tweets.csv 읽기 완료 - 651개 행\n",
      "🎉 총 8758개 행이 병합되었습니다!\n",
      "📊 포함된 사용자 수: 13\n"
     ]
    }
   ],
   "source": [
    "# 파일 시스템 작업에 필요한 추가 라이브러리 import\n",
    "import glob  # 파일 패턴 매칭을 위한 라이브러리\n",
    "import os    # 운영체제 인터페이스를 위한 라이브러리\n",
    "from datetime import datetime  # 날짜/시간 처리를 위한 라이브러리\n",
    "\n",
    "# glob 패턴을 사용하여 현재 디렉토리에서 \"user_\"로 시작하고 \".csv\"로 끝나는 모든 파일 찾기\n",
    "csv_files = glob.glob(\"user_*.csv\")\n",
    "print(f\"발견된 모든 CSV 파일: {csv_files}\")\n",
    "print(f\"총 {len(csv_files)}개의 파일을 병합합니다.\")\n",
    "\n",
    "# 각 CSV 파일의 데이터프레임을 저장할 리스트 초기화\n",
    "dfs = []\n",
    "\n",
    "# 발견된 모든 CSV 파일을 순회하며 데이터 로드\n",
    "for file in csv_files:\n",
    "    # 파일이 실제로 존재하는지 확인\n",
    "    if os.path.exists(file):\n",
    "        # CSV 파일을 pandas 데이터프레임으로 읽기\n",
    "        temp_df = pd.read_csv(file)\n",
    "        \n",
    "        # 파일명에서 사용자명 추출 (예: \"user_@elonmusk_tweets.csv\" → \"@elonmusk\")\n",
    "        username = file.replace(\"user_\", \"\").replace(\"_tweets.csv\", \"\")\n",
    "        \n",
    "        # 각 행에 해당 사용자명을 컬럼으로 추가\n",
    "        temp_df['username'] = username\n",
    "        \n",
    "        # 리스트에 데이터프레임 추가\n",
    "        dfs.append(temp_df)\n",
    "        print(f\"{file} 읽기 완료 - {len(temp_df)}개 행\")\n",
    "\n",
    "# 모든 데이터프레임을 하나로 병합\n",
    "if dfs:\n",
    "    # concat 함수를 사용하여 세로로 병합하고 인덱스 리셋\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"🎉 총 {len(df)}개 행이 병합되었습니다!\")\n",
    "    print(f\"📊 포함된 사용자 수: {len(df['username'].unique())}\")\n",
    "else:\n",
    "    print(\"읽을 수 있는 파일이 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-4. URL 제거 함수 정의\n",
    "트윗 텍스트에서 URL을 제거하는 전처리 함수를 정의합니다. URL은 감정분석에 불필요한 노이즈입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 URL을 제거하는 함수\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원본 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        str: URL이 제거된 텍스트\n",
    "    \"\"\"\n",
    "    # 정규표현식을 사용하여 HTTP/HTTPS URL 패턴을 찾아서 공백으로 대체\n",
    "    # r'https?://\\S+' 패턴 설명:\n",
    "    # - https? : http 또는 https (? 는 s가 있거나 없거나)\n",
    "    # - :// : 프로토콜 구분자\n",
    "    # - \\S+ : 공백이 아닌 문자가 1개 이상 연속 (URL의 나머지 부분)\n",
    "    cleaned_text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # 앞뒤 공백 제거하여 반환\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-5. 트윗 텍스트에서 URL 제거 적용\n",
    "정의한 URL 제거 함수를 모든 트윗 텍스트에 적용하여 데이터를 정제합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas의 apply() 함수를 사용하여 모든 트윗 텍스트에 URL 제거 함수 적용\n",
    "# apply()는 시리즈의 각 요소에 함수를 적용하여 새로운 시리즈를 반환\n",
    "df['full_text'] = df['full_text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-6. 빈 문자열을 NaN으로 변환\n",
    "URL 제거 후 빈 문자열이 된 텍스트들을 pandas의 NaN(결측값)으로 변환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_36128\\1257741985.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['full_text'].replace('', pd.NA, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 빈 문자열('')을 pandas의 NA(Not Available) 값으로 변환\n",
    "# replace() 함수: 첫 번째 인자를 두 번째 인자로 대체\n",
    "# inplace=True: 원본 데이터프레임을 직접 수정 (새로운 객체 생성 안 함)\n",
    "# pd.NA: pandas 2.0+에서 권장하는 결측값 표현\n",
    "df['full_text'].replace('', pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-7. 결측값(NaN) 제거\n",
    "감정분석이 불가능한 빈 텍스트나 결측값을 가진 행들을 데이터셋에서 제거합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna() 함수를 사용하여 결측값을 가진 행들을 제거\n",
    "# subset=['full_text']: full_text 컬럼에서 NaN 값을 가진 행들만 제거\n",
    "# inplace=True: 원본 데이터프레임을 직접 수정\n",
    "# 감정분석을 위해서는 텍스트 내용이 필수이므로 빈 텍스트는 제거 필요\n",
    "df.dropna(subset=['full_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-8. 날짜 형식 변환 및 데이터 정렬\n",
    "트위터 API 형식의 날짜를 표준 날짜 형식으로 변환하고, 최신 순으로 정렬합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜 형식 변환 중...\n",
      "날짜 형식 변환 및 정렬 완료!\n",
      "날짜 범위: 2020-01-17 01:05:39 ~ 2025-06-16 04:36:37\n",
      "\n",
      "결과 확인:\n",
      "           created_at                                          full_text  \\\n",
      "0 2025-06-16 04:36:37  1. Iran produces around 3.3mn barrels per day ...   \n",
      "1 2025-06-16 03:40:49             Today at Apple. #F1TheMovie #Severance   \n",
      "2 2025-06-16 03:04:38  Two countries, separated by 700 kms from each ...   \n",
      "3 2025-06-16 03:01:08  BREAKING: Iranian opposition Telegram channels...   \n",
      "4 2025-06-16 02:50:54  26 now.   Note the swing due east at the edge ...   \n",
      "\n",
      "      username  \n",
      "0  @Ajay_Bagga  \n",
      "1    @tim_cook  \n",
      "2  @Ajay_Bagga  \n",
      "3  @BillAckman  \n",
      "4  @BillAckman  \n"
     ]
    }
   ],
   "source": [
    "def convert_date_format(date_str):\n",
    "\n",
    "    try:\n",
    "        # 트위터 API 날짜 형식: \"Mon Jun 16 02:50:54 +0000 2025\"\n",
    "        # pd.to_datetime으로 파싱하고 strftime으로 원하는 형식으로 변환\n",
    "        # format 매개변수 설명:\n",
    "        # %a: 축약된 요일명 (Mon, Tue, ...)\n",
    "        # %b: 축약된 월명 (Jan, Feb, ...)  \n",
    "        # %d: 일 (01-31)\n",
    "        # %H:%M:%S: 시:분:초\n",
    "        # %z: 타임존 오프셋 (+0000)\n",
    "        # %Y: 4자리 연도\n",
    "        dt = pd.to_datetime(date_str, format='%a %b %d %H:%M:%S %z %Y')\n",
    "        \n",
    "        # 타임존 제거하고 \"YYYY-MM-DD HH:MM:SS\" 형식으로 변환\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        # 파싱 실패 시 원본 그대로 반환\n",
    "        return date_str\n",
    "\n",
    "# 날짜 형식 변환 프로세스 시작\n",
    "print(\"날짜 형식 변환 중...\")\n",
    "\n",
    "# 모든 트윗의 created_at 컬럼에 날짜 변환 함수 적용\n",
    "df['created_at'] = df['created_at'].apply(convert_date_format)\n",
    "\n",
    "# 문자열을 pandas datetime 객체로 변환 (정렬 및 시간 연산을 위해)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# 최신 트윗이 맨 위에 오도록 내림차순 정렬\n",
    "# ascending=False: 내림차순 (큰 값부터 작은 값 순)\n",
    "# reset_index(drop=True): 정렬 후 인덱스를 0부터 다시 시작\n",
    "df = df.sort_values(by='created_at', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"날짜 형식 변환 및 정렬 완료!\")\n",
    "print(f\"날짜 범위: {df['created_at'].min()} ~ {df['created_at'].max()}\")\n",
    "print(\"\\n결과 확인:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-9. VADER 감정분석기 초기화\n",
    "VADER(Valence Aware Dictionary and sEntiment Reasoner) 감정분석 모델을 초기화합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER 감정분석기 인스턴스 생성\n",
    "# SentimentIntensityAnalyzer: VADER 알고리즘을 구현한 클래스\n",
    "# - 소셜 미디어 텍스트에 특화된 감정분석 도구\n",
    "# - 이모티콘, 대문자, 구두점, 단어 조합 등을 고려하여 감정 점수 계산\n",
    "# - positive, negative, neutral, compound 점수를 반환\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-10. 감정분석 함수 정의\n",
    "텍스트를 분석하여 positive/negative/neutral로 분류하고 각 감정 점수를 반환하는 함수를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    # VADER 감정분석기로 텍스트 분석\n",
    "    # polarity_scores() 반환값:\n",
    "    # - 'neg': negative 감정 점수 (0~1)\n",
    "    # - 'neu': neutral 감정 점수 (0~1)  \n",
    "    # - 'pos': positive 감정 점수 (0~1)\n",
    "    # - 'compound': 복합 점수 (-1~1, 전체적인 감정 강도)\n",
    "    scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # compound 점수를 기준으로 감정 분류\n",
    "    compound = scores['compound']\n",
    "    \n",
    "    # VADER 권장 임계값을 사용한 감정 분류\n",
    "    if compound >= 0.05:\n",
    "        # compound >= 0.05: 긍정적 감정\n",
    "        sentiment = 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        # compound <= -0.05: 부정적 감정\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        # -0.05 < compound < 0.05: 중립적 감정\n",
    "        sentiment = 'neutral'\n",
    "    \n",
    "    # pandas Series로 반환 (DataFrame의 새 컬럼들로 할당하기 위함)\n",
    "    return pd.Series([sentiment, scores['neg'], scores['neu'], scores['pos']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-11. 모든 트윗에 감정분석 실행\n",
    "정의한 감정분석 함수를 모든 트윗 텍스트에 적용하여 감정 컬럼들을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas의 apply() 함수를 사용하여 모든 트윗에 감정분석 함수 적용\n",
    "# analyze_sentiment() 함수가 pd.Series를 반환하므로 \n",
    "# 여러 컬럼에 동시에 할당 가능\n",
    "# - sentiment: 감정 분류 라벨 ('positive', 'negative', 'neutral')\n",
    "# - neg: 부정 감정 점수 (0~1)\n",
    "# - neu: 중립 감정 점수 (0~1)\n",
    "# - pos: 긍정 감정 점수 (0~1)\n",
    "df[['sentiment', 'neg', 'neu', 'pos']] = df['full_text'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-12. 최종 데이터 컬럼 선택\n",
    "분석에 필요한 핵심 컬럼들만 선택하여 최종 데이터셋을 구성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 분석 결과에 필요한 컬럼들만 선택\n",
    "# - created_at: 트윗 작성 날짜/시간\n",
    "# - full_text: 전처리된 트윗 텍스트 (URL 제거됨)\n",
    "# - username: 트윗 작성자 (@사용자명)\n",
    "# - sentiment: 감정 분류 결과 (positive/negative/neutral)\n",
    "# - neg: 부정 감정 점수\n",
    "# - neu: 중립 감정 점수  \n",
    "# - pos: 긍정 감정 점수\n",
    "df = df[['created_at', 'full_text', 'username', 'sentiment', 'neg', 'neu', 'pos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4-13. 최종 결과 저장 및 요약\n",
    "감정분석이 완료된 트윗 데이터를 CSV 파일로 저장하고 처리 결과를 요약합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 merged_tweets_with_sentiment.csv에 저장되었습니다.\n",
      "총 8594개의 트윗이 처리되었습니다.\n",
      "포함된 사용자: ['@Ajay_Bagga', '@BillAckman', '@CathieDWood', '@JDVance', '@LizAnnSonders', '@RayDalio', '@SecScottBessent', '@WhiteHouse', '@elonmusk', '@marcorubio', '@michaelbatnick', '@sundarpichai', '@tim_cook']\n",
      "날짜 범위: 2020-01-17 01:05:39 ~ 2025-06-16 04:36:37\n"
     ]
    }
   ],
   "source": [
    "# 결과 파일 저장\n",
    "output_filename = \"merged_tweets_with_sentiment.csv\"\n",
    "\n",
    "# 감정분석이 완료된 데이터프레임을 CSV 파일로 저장\n",
    "# index=False: 행 인덱스를 파일에 포함하지 않음\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "# 처리 결과 요약 정보 출력\n",
    "print(f\"✅ 결과가 {output_filename}에 저장되었습니다.\")\n",
    "print(f\"📊 총 {len(df)}개의 트윗이 처리되었습니다.\")\n",
    "\n",
    "# 감정분석에 포함된 사용자 목록 (알파벳 순 정렬)\n",
    "print(f\"👥 포함된 사용자: {sorted(df['username'].unique())}\")\n",
    "\n",
    "# 트윗 데이터의 시간 범위\n",
    "print(f\"📅 날짜 범위: {df['created_at'].min()} ~ {df['created_at'].max()}\")\n",
    "\n",
    "# 감정 분류별 트윗 수 통계\n",
    "print(f\"\\n📈 감정 분류별 통계:\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  - {sentiment}: {count}개 ({percentage:.1f}%)\")\n",
    "\n",
    "# 사용자별 트윗 수 통계 (상위 5명)\n",
    "print(f\"\\n🏆 사용자별 트윗 수 (상위 5명):\")\n",
    "user_counts = df['username'].value_counts().head(5)\n",
    "for username, count in user_counts.items():\n",
    "    print(f\"  - {username}: {count}개\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
