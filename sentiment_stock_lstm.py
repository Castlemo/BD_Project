import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ML/DL ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# ì‹œê°í™”
import matplotlib.pyplot as plt
import seaborn as sns

class SentimentStockPredictor:
    """
    ë‰´ìŠ¤ ê°ì •ë¶„ì„(FinBERT) + íŠ¸ìœ„í„° ê°ì •ë¶„ì„(VADER) + ì£¼ê°€ ë°ì´í„°ë¥¼ 
    í†µí•©í•˜ì—¬ LSTMìœ¼ë¡œ ì£¼ê°€ ë°©í–¥ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” í´ëž˜ìŠ¤
    """
    
    def __init__(self, stock_file, news_file, twitter_file):
        """
        Parameters:
        stock_file (str): ì£¼ê°€ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ
        news_file (str): ë‰´ìŠ¤ ê°ì •ë¶„ì„ CSV íŒŒì¼ ê²½ë¡œ  
        twitter_file (str): íŠ¸ìœ„í„° ê°ì •ë¶„ì„ CSV íŒŒì¼ ê²½ë¡œ
        """
        self.stock_file = stock_file
        self.news_file = news_file
        self.twitter_file = twitter_file
        
        self.combined_data = None
        self.model = None
        self.scaler = None
        self.sequence_length = 24  # 24ì‹œê°„ ì‹œí€€ìŠ¤
        
        print("ðŸš€ ê°ì •ë¶„ì„ ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def load_and_sync_data(self):
        """3ê°œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë™ê¸°í™”"""
        
        print("\nðŸ“Š ë°ì´í„° ë¡œë”© ë° ë™ê¸°í™” ì‹œìž‘...")
        
        # 1. ì£¼ê°€ ë°ì´í„° ë¡œë“œ
        print("ðŸ“ˆ ì£¼ê°€ ë°ì´í„° ë¡œë”©...")
        try:
            stock_df = pd.read_csv(self.stock_file)
            stock_df['Datetime'] = pd.to_datetime(stock_df['Datetime'])
            stock_df = stock_df.sort_values('Datetime').reset_index(drop=True)
            print(f"âœ… ì£¼ê°€ ë°ì´í„°: {len(stock_df):,}ê°œ í–‰, ê¸°ê°„: {stock_df['Datetime'].min()} ~ {stock_df['Datetime'].max()}")
        except Exception as e:
            print(f"âŒ ì£¼ê°€ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
        
        # 2. ë‰´ìŠ¤ ê°ì •ë¶„ì„ ë°ì´í„° ë¡œë“œ
        print("ðŸ“° ë‰´ìŠ¤ ê°ì •ë¶„ì„ ë°ì´í„° ë¡œë”©...")
        try:
            news_df = pd.read_csv(self.news_file)
            news_df['Date'] = pd.to_datetime(news_df['Date'])
            news_df = news_df.sort_values('Date').reset_index(drop=True)
            print(f"âœ… ë‰´ìŠ¤ ë°ì´í„°: {len(news_df):,}ê°œ í–‰, ê¸°ê°„: {news_df['Date'].min()} ~ {news_df['Date'].max()}")
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
        
        # 3. íŠ¸ìœ„í„° ê°ì •ë¶„ì„ ë°ì´í„° ë¡œë“œ
        print("ðŸ¦ íŠ¸ìœ„í„° ê°ì •ë¶„ì„ ë°ì´í„° ë¡œë”©...")
        try:
            twitter_df = pd.read_csv(self.twitter_file)
            twitter_df['created_at'] = pd.to_datetime(twitter_df['created_at'])
            twitter_df = twitter_df.sort_values('created_at').reset_index(drop=True)
            print(f"âœ… íŠ¸ìœ„í„° ë°ì´í„°: {len(twitter_df):,}ê°œ í–‰, ê¸°ê°„: {twitter_df['created_at'].min()} ~ {twitter_df['created_at'].max()}")
        except Exception as e:
            print(f"âŒ íŠ¸ìœ„í„° ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
        
        # 4. ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë™ê¸°í™”
        print("\nðŸ”„ ì‹œê°„ ê¸°ì¤€ ë°ì´í„° ë™ê¸°í™”...")
        
        # ì£¼ê°€ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì§‘ê³„
        stock_df['hour'] = stock_df['Datetime'].dt.floor('H')
        
        # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì§‘ê³„
        news_df['hour'] = news_df['Date'].dt.floor('H')
        news_hourly = news_df.groupby('hour').agg({
            'pos': 'mean',
            'neu': 'mean', 
            'neg': 'mean',
            'sentiment': lambda x: x.mode().iloc[0] if not x.empty else 'neutral'
        }).reset_index()
        news_hourly.columns = ['hour', 'news_pos', 'news_neu', 'news_neg', 'news_sentiment']
        
        # íŠ¸ìœ„í„° ë°ì´í„°ë¥¼ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì§‘ê³„
        twitter_df['hour'] = twitter_df['created_at'].dt.floor('H')
        twitter_hourly = twitter_df.groupby('hour').agg({
            'pos': 'mean',
            'neu': 'mean',
            'neg': 'mean', 
            'sentiment': lambda x: x.mode().iloc[0] if not x.empty else 'neutral'
        }).reset_index()
        twitter_hourly.columns = ['hour', 'twitter_pos', 'twitter_neu', 'twitter_neg', 'twitter_sentiment']
        
        # ì£¼ê°€ ë°ì´í„°ë„ ì‹œê°„ë³„ë¡œ ì •ë¦¬ (ì¤‘ë³µ ì œê±°)
        stock_hourly = stock_df.drop_duplicates(subset=['hour'], keep='last').reset_index(drop=True)
        
        # 3ê°œ ë°ì´í„° ë³‘í•©
        print("ðŸ”— ë°ì´í„° ë³‘í•© ì¤‘...")
        combined = stock_hourly.merge(news_hourly, on='hour', how='left')
        combined = combined.merge(twitter_hourly, on='hour', how='left')
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (forward fill)
        sentiment_cols = ['news_pos', 'news_neu', 'news_neg', 'twitter_pos', 'twitter_neu', 'twitter_neg']
        combined[sentiment_cols] = combined[sentiment_cols].fillna(method='ffill')
        combined[sentiment_cols] = combined[sentiment_cols].fillna(0.5)  # ì´ˆê¸°ê°’
        
        # ê°ì • ë¼ë²¨ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        combined['news_sentiment'] = combined['news_sentiment'].fillna('neutral')
        combined['twitter_sentiment'] = combined['twitter_sentiment'].fillna('neutral')
        
        print(f"âœ… ë°ì´í„° ë³‘í•© ì™„ë£Œ: {len(combined):,}ê°œ í–‰")
        print(f"ðŸ“… í†µí•© ë°ì´í„° ê¸°ê°„: {combined['hour'].min()} ~ {combined['hour'].max()}")
        
        self.combined_data = combined
        return combined
    
    def create_combined_features(self):
        """í†µí•© íŠ¹ì„± ìƒì„±"""
        
        if self.combined_data is None:
            print("âŒ ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return
        
        print("\nðŸ”§ í†µí•© íŠ¹ì„± ìƒì„± ì¤‘...")
        
        df = self.combined_data.copy()
        
        # 1. ê°ì • ì ìˆ˜ í†µí•© (ê°€ì¤‘ í‰ê· : FinBERT 60%, VADER 40%)
        df['combined_pos'] = 0.6 * df['news_pos'] + 0.4 * df['twitter_pos']
        df['combined_neu'] = 0.6 * df['news_neu'] + 0.4 * df['twitter_neu']
        df['combined_neg'] = 0.6 * df['news_neg'] + 0.4 * df['twitter_neg']
        
        # 2. ê°ì • ê°•ë„ ë° ê·¹ì„±
        df['news_sentiment_intensity'] = df[['news_pos', 'news_neu', 'news_neg']].max(axis=1) - df[['news_pos', 'news_neu', 'news_neg']].min(axis=1)
        df['twitter_sentiment_intensity'] = df[['twitter_pos', 'twitter_neu', 'twitter_neg']].max(axis=1) - df[['twitter_pos', 'twitter_neu', 'twitter_neg']].min(axis=1)
        df['combined_sentiment_intensity'] = df[['combined_pos', 'combined_neu', 'combined_neg']].max(axis=1) - df[['combined_pos', 'combined_neu', 'combined_neg']].min(axis=1)
        
        # 3. ê°ì • ì ìˆ˜ ë³€í™”ìœ¨ (ì‹œê°„ë³„)
        for col in ['news_pos', 'news_neu', 'news_neg', 'twitter_pos', 'twitter_neu', 'twitter_neg']:
            df[f'{col}_change'] = df[col].pct_change()
        
        # 4. ê°ì • ì ìˆ˜ ì´ë™í‰ê·  (3ì‹œê°„, 6ì‹œê°„)
        for window in [3, 6]:
            for col in ['combined_pos', 'combined_neu', 'combined_neg']:
                df[f'{col}_ma{window}'] = df[col].rolling(window).mean()
        
        # 5. ì£¼ê°€ íŠ¹ì„±
        df['price_change'] = df['Close'].pct_change()  # ìˆ˜ìµë¥ 
        df['price_volatility'] = df['price_change'].rolling(6).std()  # ë³€ë™ì„±
        df['volume_ma'] = df['Volume'].rolling(6).mean()  # ê±°ëž˜ëŸ‰ ì´ë™í‰ê· 
        
        # 6. ê¸°ìˆ ì  ì§€í‘œ
        df['sma_5'] = df['Close'].rolling(5).mean()
        df['sma_10'] = df['Close'].rolling(10).mean()
        df['rsi'] = self._calculate_rsi(df['Close'])
        
        # 7. ì‹œê°„ íŠ¹ì„±
        df['hour_of_day'] = df['hour'].dt.hour
        df['day_of_week'] = df['hour'].dt.dayofweek
        df['is_trading_hours'] = ((df['hour_of_day'] >= 9) & (df['hour_of_day'] <= 16)).astype(int)
        
        # 8. íƒ€ê²Ÿ ë¼ë²¨ ìƒì„± (ë‹¤ìŒ ì‹œê°„ì˜ ì£¼ê°€ ë°©í–¥ì„±)
        df['next_price_change'] = df['price_change'].shift(-1)  # ë‹¤ìŒ ì‹œê°„ ìˆ˜ìµë¥ 
        
        # ìž„ê³„ê°’ ì„¤ì •: Â±0.5%
        def classify_direction(change):
            if pd.isna(change):
                return 'neutral'
            elif change > 0.005:  # +0.5%
                return 'positive'
            elif change < -0.005:  # -0.5%
                return 'negative'
            else:
                return 'neutral'
        
        df['target'] = df['next_price_change'].apply(classify_direction)
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        df = df.dropna().reset_index(drop=True)
        
        print(f"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(df):,}ê°œ í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
        print(f"ðŸ“Š íƒ€ê²Ÿ ë¶„í¬:")
        print(df['target'].value_counts())
        
        self.combined_data = df
        return df
    
    def _calculate_rsi(self, prices, window=14):
        """RSI ê³„ì‚°"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def prepare_lstm_data(self):
        """LSTM ìž…ë ¥ìš© ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„"""
        
        if self.combined_data is None:
            print("âŒ ë¨¼ì € íŠ¹ì„±ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
            return
        
        print(f"\nðŸ”„ LSTM ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„ ì¤‘... (ì‹œí€€ìŠ¤ ê¸¸ì´: {self.sequence_length})")
        
        df = self.combined_data.copy()
        
        # íŠ¹ì„± ì„ íƒ (ê°ì • + ì£¼ê°€ + ê¸°ìˆ ì  ì§€í‘œ)
        feature_cols = [
            # ê°ì • ì ìˆ˜ (ì›ë³¸)
            'news_pos', 'news_neu', 'news_neg',
            'twitter_pos', 'twitter_neu', 'twitter_neg',
            # í†µí•© ê°ì • ì ìˆ˜
            'combined_pos', 'combined_neu', 'combined_neg',
            # ê°ì • ê°•ë„
            'news_sentiment_intensity', 'twitter_sentiment_intensity', 'combined_sentiment_intensity',
            # ì£¼ê°€ íŠ¹ì„±
            'Close', 'Volume', 'price_change', 'price_volatility',
            # ê¸°ìˆ ì  ì§€í‘œ
            'rsi', 'sma_5', 'sma_10',
            # ì‹œê°„ íŠ¹ì„±
            'hour_of_day', 'day_of_week', 'is_trading_hours'
        ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_cols = [col for col in feature_cols if col in df.columns]
        print(f"ðŸ“‹ ì‚¬ìš©í•  íŠ¹ì„±: {len(available_cols)}ê°œ")
        print(f"  {available_cols}")
        
        # ë°ì´í„° ì¤€ë¹„
        X = df[available_cols].values
        y = df['target'].values
        
        # ë ˆì´ë¸” ì¸ì½”ë”©
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        self.label_encoder = le
        
        print(f"ðŸ“Š ë ˆì´ë¸” ë§¤í•‘: {dict(zip(le.classes_, le.transform(le.classes_)))}")
        
        # ë°ì´í„° ì •ê·œí™”
        self.scaler = MinMaxScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        X_sequences, y_sequences = [], []
        
        for i in range(len(X_scaled) - self.sequence_length):
            X_sequences.append(X_scaled[i:(i + self.sequence_length)])
            y_sequences.append(y_encoded[i + self.sequence_length])
        
        X_sequences = np.array(X_sequences)
        y_sequences = np.array(y_sequences)
        
        print(f"âœ… ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ:")
        print(f"  X shape: {X_sequences.shape}")
        print(f"  y shape: {y_sequences.shape}")
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X_sequences, y_sequences, test_size=0.2, random_state=42, stratify=y_sequences
        )
        
        print(f"ðŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"  í›ˆë ¨ ì„¸íŠ¸: {X_train.shape[0]:,}ê°œ")
        print(f"  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape[0]:,}ê°œ")
        
        self.X_train, self.X_test = X_train, X_test
        self.y_train, self.y_test = y_train, y_test
        self.n_features = X_sequences.shape[2]
        self.n_classes = len(np.unique(y_sequences))
        
        return X_train, X_test, y_train, y_test
    
    def build_lstm_model(self):
        """LSTM ëª¨ë¸ êµ¬ì¶•"""
        
        print(f"\nðŸ—ï¸ LSTM ëª¨ë¸ êµ¬ì¶• ì¤‘...")
        print(f"  ìž…ë ¥ í˜•íƒœ: ({self.sequence_length}, {self.n_features})")
        print(f"  ì¶œë ¥ í´ëž˜ìŠ¤: {self.n_classes}ê°œ")
        
        model = Sequential([
            # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´
            LSTM(128, return_sequences=True, input_shape=(self.sequence_length, self.n_features)),
            Dropout(0.2),
            BatchNormalization(),
            
            # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´  
            LSTM(64, return_sequences=True),
            Dropout(0.2),
            BatchNormalization(),
            
            # ì„¸ ë²ˆì§¸ LSTM ë ˆì´ì–´
            LSTM(32, return_sequences=False),
            Dropout(0.2),
            
            # ì™„ì „ì—°ê²° ë ˆì´ì–´
            Dense(50, activation='relu'),
            Dropout(0.2),
            Dense(25, activation='relu'),
            
            # ì¶œë ¥ ë ˆì´ì–´ (3í´ëž˜ìŠ¤ ë¶„ë¥˜)
            Dense(self.n_classes, activation='softmax')
        ])
        
        # ëª¨ë¸ ì»´íŒŒì¼
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',  # ì •ìˆ˜ ë¼ë²¨ìš©
            metrics=['accuracy']
        )
        
        print("âœ… ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!")
        model.summary()
        
        self.model = model
        return model
    
    def train_model(self, epochs=50, batch_size=32):
        """ëª¨ë¸ í›ˆë ¨"""
        
        if self.model is None:
            print("âŒ ë¨¼ì € ëª¨ë¸ì„ êµ¬ì¶•í•´ì£¼ì„¸ìš”.")
            return
        
        print(f"\nðŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œìž‘... (ì—í¬í¬: {epochs}, ë°°ì¹˜: {batch_size})")
        
        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)
        ]
        
        # í›ˆë ¨ ì‹¤í–‰
        history = self.model.fit(
            self.X_train, self.y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(self.X_test, self.y_test),
            callbacks=callbacks,
            verbose=1
        )
        
        print("âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        
        # í›ˆë ¨ ê³¡ì„  ì‹œê°í™”
        self._plot_training_history(history)
        
        return history
    
    def evaluate_model(self):
        """ëª¨ë¸ í‰ê°€"""
        
        if self.model is None:
            print("âŒ í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\nðŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        # ì˜ˆì¸¡
        y_pred_proba = self.model.predict(self.X_test)
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # ì •í™•ë„
        accuracy = accuracy_score(self.y_test, y_pred)
        print(f"ðŸŽ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        target_names = self.label_encoder.classes_
        print(f"\nðŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        print(classification_report(self.y_test, y_pred, target_names=target_names))
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(self.y_test, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=target_names, yticklabels=target_names)
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.show()
        
        return accuracy, y_pred, y_pred_proba
    
    def _plot_training_history(self, history):
        """í›ˆë ¨ ê³¡ì„  ì‹œê°í™”"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss ê³¡ì„ 
        ax1.plot(history.history['loss'], label='Train Loss')
        ax1.plot(history.history['val_loss'], label='Validation Loss')
        ax1.set_title('Model Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        
        # Accuracy ê³¡ì„ 
        ax2.plot(history.history['accuracy'], label='Train Accuracy')
        ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')
        ax2.set_title('Model Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        
        plt.tight_layout()
        plt.show()
    
    def predict_next_direction(self, n_predictions=24):
        """í–¥í›„ Nì‹œê°„ì˜ ì£¼ê°€ ë°©í–¥ì„± ì˜ˆì¸¡"""
        
        if self.model is None:
            print("âŒ í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nðŸ”® í–¥í›„ {n_predictions}ì‹œê°„ ì£¼ê°€ ë°©í–¥ì„± ì˜ˆì¸¡...")
        
        # ìµœê·¼ ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
        recent_data = self.combined_data.tail(self.sequence_length)
        
        feature_cols = [
            'news_pos', 'news_neu', 'news_neg',
            'twitter_pos', 'twitter_neu', 'twitter_neg', 
            'combined_pos', 'combined_neu', 'combined_neg',
            'news_sentiment_intensity', 'twitter_sentiment_intensity', 'combined_sentiment_intensity',
            'Close', 'Volume', 'price_change', 'price_volatility',
            'rsi', 'sma_5', 'sma_10',
            'hour_of_day', 'day_of_week', 'is_trading_hours'
        ]
        
        available_cols = [col for col in feature_cols if col in recent_data.columns]
        X_recent = recent_data[available_cols].values
        X_recent_scaled = self.scaler.transform(X_recent)
        
        predictions = []
        current_sequence = X_recent_scaled.copy()
        
        for i in range(n_predictions):
            # ì˜ˆì¸¡
            X_input = current_sequence.reshape(1, self.sequence_length, self.n_features)
            pred_proba = self.model.predict(X_input, verbose=0)
            pred_class = np.argmax(pred_proba, axis=1)[0]
            pred_label = self.label_encoder.inverse_transform([pred_class])[0]
            
            predictions.append({
                'hour': i + 1,
                'prediction': pred_label,
                'confidence': np.max(pred_proba),
                'probabilities': {
                    label: prob for label, prob in zip(self.label_encoder.classes_, pred_proba[0])
                }
            })
            
            # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ (ë‹¨ìˆœí™”: ë§ˆì§€ë§‰ ê°’ ë³µì‚¬)
            current_sequence = np.roll(current_sequence, -1, axis=0)
            current_sequence[-1] = current_sequence[-2]  # ë‹¨ìˆœ ë³µì‚¬
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nðŸŽ¯ ì˜ˆì¸¡ ê²°ê³¼:")
        for pred in predictions:
            print(f"  ì‹œê°„ +{pred['hour']:2d}: {pred['prediction']:8s} (ì‹ ë¢°ë„: {pred['confidence']:.3f})")
        
        # ì˜ˆì¸¡ ë¶„í¬
        pred_counts = {}
        for pred in predictions:
            label = pred['prediction']
            pred_counts[label] = pred_counts.get(label, 0) + 1
        
        print(f"\nðŸ“Š {n_predictions}ì‹œê°„ ì˜ˆì¸¡ ë¶„í¬:")
        for label, count in pred_counts.items():
            print(f"  {label}: {count}ì‹œê°„ ({count/n_predictions*100:.1f}%)")
        
        return predictions
    
    def save_model(self, model_path="sentiment_stock_lstm_model.h5"):
        """ëª¨ë¸ ì €ìž¥"""
        if self.model is not None:
            self.model.save(model_path)
            print(f"ðŸ’¾ ëª¨ë¸ì´ '{model_path}'ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âŒ ì €ìž¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def run_full_pipeline(self, epochs=50, batch_size=32):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        print("ðŸš€ ê°ì •ë¶„ì„ ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì‹¤í–‰!")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ë™ê¸°í™”
        if self.load_and_sync_data() is None:
            return
        
        # 2. íŠ¹ì„± ìƒì„±
        self.create_combined_features()
        
        # 3. LSTM ë°ì´í„° ì¤€ë¹„
        self.prepare_lstm_data()
        
        # 4. ëª¨ë¸ êµ¬ì¶•
        self.build_lstm_model()
        
        # 5. ëª¨ë¸ í›ˆë ¨
        self.train_model(epochs=epochs, batch_size=batch_size)
        
        # 6. ëª¨ë¸ í‰ê°€
        accuracy, _, _ = self.evaluate_model()
        
        # 7. ë¯¸ëž˜ ì˜ˆì¸¡
        self.predict_next_direction(n_predictions=24)
        
        # 8. ëª¨ë¸ ì €ìž¥
        self.save_model()
        
        print(f"\nðŸŽ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ìµœì¢… ì •í™•ë„: {accuracy:.4f}")
        
        return accuracy

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    
    print("ðŸŽ¯ ê°ì •ë¶„ì„ ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)
    stock_file = "stock/AAPL_1hour_data_365days.csv"  # ì£¼ê°€ ë°ì´í„°
    news_file = "finnhub/AAPL_finnhub_processed_final.csv"  # ë‰´ìŠ¤ ê°ì •ë¶„ì„
    twitter_file = "X_data/merged_tweets_with_sentiment.csv"  # íŠ¸ìœ„í„° ê°ì •ë¶„ì„
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    predictor = SentimentStockPredictor(
        stock_file=stock_file,
        news_file=news_file, 
        twitter_file=twitter_file
    )
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    try:
        final_accuracy = predictor.run_full_pipeline(epochs=30, batch_size=32)
        print(f"\nâœ… ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ! ìµœì¢… ì„±ëŠ¥: {final_accuracy:.4f}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("íŒŒì¼ ê²½ë¡œì™€ ë°ì´í„° í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")